<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Regression | Linear Algebra for Data Science</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Regression | Linear Algebra for Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/mfdscover.png" />
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Regression | Linear Algebra for Data Science" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="twitter:image" content="/images/mfdscover.png" />

<meta name="author" content="Tom Tegtmeyer" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="orthogonality.html"/>
<link rel="next" href="svd-and-pca.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Linear Algebra for Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html"><i class="fa fa-check"></i><b>1</b> Matrices and Systems of Equations</a>
<ul>
<li class="chapter" data-level="1.1" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#systems-of-equations"><i class="fa fa-check"></i><b>1.1</b> Systems of Equations</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#the-geometry"><i class="fa fa-check"></i>The geometry</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#the-three-cases"><i class="fa fa-check"></i>The three cases</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#GE"><i class="fa fa-check"></i><b>1.2</b> Method of Solution: Gaussian Elimination</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#parameters"><i class="fa fa-check"></i>Parameters</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#more-equations-more-variables"><i class="fa fa-check"></i>More equations, more variables</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#overdetermined-and-underdetermined-systems"><i class="fa fa-check"></i>Overdetermined and underdetermined systems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#matrices"><i class="fa fa-check"></i><b>1.3</b> Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#elementary-row-operations"><i class="fa fa-check"></i>Elementary row operations</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#row-echelon-form"><i class="fa fa-check"></i>Row echelon form</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#reduced-row-echelon-form"><i class="fa fa-check"></i>Reduced row echelon form</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#TM"><i class="fa fa-check"></i>Matrices with Technology</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#BMO"><i class="fa fa-check"></i><b>1.4</b> Basic Matrix Operations</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#row-and-column-vectors"><i class="fa fa-check"></i>Row and column vectors</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#matrix-addition"><i class="fa fa-check"></i>Matrix addition</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#scalar-multiplication"><i class="fa fa-check"></i>Scalar Multiplication</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#matrix-multiplication"><i class="fa fa-check"></i>Matrix multiplication</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#multiplying-two-matrices"><i class="fa fa-check"></i>Multiplying two matrices</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#identity-matrices"><i class="fa fa-check"></i>Identity Matrices</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#technology"><i class="fa fa-check"></i>Technology</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#the-transpose-of-a-matrix"><i class="fa fa-check"></i>The transpose of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#Inverses"><i class="fa fa-check"></i><b>1.5</b> Matrix Inverses</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#finding-inverses"><i class="fa fa-check"></i>Finding inverses</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#a-formula-for-2-x-2-matrices"><i class="fa fa-check"></i>A formula for 2 x 2 matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#dets"><i class="fa fa-check"></i><b>1.6</b> Determinants</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#triangular-and-diagonal-matrices"><i class="fa fa-check"></i>Triangular and diagonal matrices</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#upper-and-lower-triangular-matrices"><i class="fa fa-check"></i>Upper and lower triangular matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#systems-of-linear-equations-as-matrix-equations"><i class="fa fa-check"></i><b>1.7</b> Systems of Linear Equations as Matrix Equations</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#how-does-row-reduction-work"><i class="fa fa-check"></i>How does row reduction work?</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#Colley"><i class="fa fa-check"></i><b>1.8</b> Application: The Colley Matrix Method</a>
<ul>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#ratings-and-rankings"><i class="fa fa-check"></i>Ratings and Rankings</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#the-bcs-and-wes-colley"><i class="fa fa-check"></i>The BCS and Wes Colley</a></li>
<li class="chapter" data-level="" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#the-colley-matrix"><i class="fa fa-check"></i>The Colley Matrix</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="matrices-and-systems-of-equations.html"><a href="matrices-and-systems-of-equations.html#exercises"><i class="fa fa-check"></i><b>1.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="vector-spaces.html"><a href="vector-spaces.html"><i class="fa fa-check"></i><b>2</b> Vector Spaces</a>
<ul>
<li class="chapter" data-level="2.1" data-path="vector-spaces.html"><a href="vector-spaces.html#artoo"><i class="fa fa-check"></i><b>2.1</b> The Vector Space <span class="math inline">\(\mathbb{R}^n\)</span></a>
<ul>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#vector-addition-and-scalar-multiplication"><i class="fa fa-check"></i>Vector addition and scalar multiplication</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#linear-dependence-and-independence"><i class="fa fa-check"></i>Linear dependence and independence</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#linear-combinations"><i class="fa fa-check"></i>Linear Combinations</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#mathbbr3-and-mathbbrn"><i class="fa fa-check"></i><span class="math inline">\(\mathbb{R}^3\)</span> and <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="vector-spaces.html"><a href="vector-spaces.html#subspaces"><i class="fa fa-check"></i><b>2.2</b> Subspaces</a></li>
<li class="chapter" data-level="2.3" data-path="vector-spaces.html"><a href="vector-spaces.html#linear-dependence-and-independence-1"><i class="fa fa-check"></i><b>2.3</b> Linear Dependence and Independence</a>
<ul>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#linear-dependence-revisited"><i class="fa fa-check"></i>Linear dependence revisited</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="vector-spaces.html"><a href="vector-spaces.html#basis-and-dimension"><i class="fa fa-check"></i><b>2.4</b> Basis and Dimension</a>
<ul>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#the-standard-basis-for-mathbbrn"><i class="fa fa-check"></i>The standard basis for <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#the-dimension-of-a-subspace"><i class="fa fa-check"></i>The dimension of a subspace</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#the-dimension-of-the-nullspace"><i class="fa fa-check"></i>The dimension of the Nullspace</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#the-column-space-of-a-matrix"><i class="fa fa-check"></i>The column space of a matrix</a></li>
<li class="chapter" data-level="" data-path="vector-spaces.html"><a href="vector-spaces.html#the-row-space"><i class="fa fa-check"></i>The row space</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="vector-spaces.html"><a href="vector-spaces.html#exercises-1"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>3</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="3.1" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#eigenvalues-and-eigenvectors-1"><i class="fa fa-check"></i><b>3.1</b> Eigenvalues and Eigenvectors</a>
<ul>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#eigenvalues-and-eigenvectors-in-r"><i class="fa fa-check"></i>Eigenvalues and Eigenvectors in R</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#higher-dimensional-matrices"><i class="fa fa-check"></i>Higher dimensional matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#special-cases-and-complications"><i class="fa fa-check"></i><b>3.2</b> Special Cases and Complications</a>
<ul>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#diagonal-and-triangular-matrices"><i class="fa fa-check"></i>Diagonal and triangular matrices</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#the-trace"><i class="fa fa-check"></i>The trace</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#complications"><i class="fa fa-check"></i>Complications</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#eigenvalues-of-a-transpose"><i class="fa fa-check"></i>Eigenvalues of a transpose</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#matrix-powers"><i class="fa fa-check"></i>Matrix powers</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#application-the-leslie-matrix"><i class="fa fa-check"></i><b>3.3</b> Application: The Leslie Matrix</a></li>
<li class="chapter" data-level="3.4" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#graphs-and-adjacency-matrices"><i class="fa fa-check"></i><b>3.4</b> Graphs and Adjacency Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#adjacency-matrices"><i class="fa fa-check"></i>Adjacency matrices</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#directed-adjacency-matrices"><i class="fa fa-check"></i>Directed adjacency matrices</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#the-directed-graph-of-a-tournament"><i class="fa fa-check"></i>The directed graph of a tournament</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#random-surfers-and-stochastic-matrices"><i class="fa fa-check"></i><b>3.5</b> Random surfers and Stochastic Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#stochastic-matrices"><i class="fa fa-check"></i>Stochastic matrices</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#markov-chains"><i class="fa fa-check"></i>Markov Chains</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#why-is-1-always-an-eigenvalue"><i class="fa fa-check"></i>Why is 1 always an eigenvalue?</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#the-markov-and-oracle-ranking-methods"><i class="fa fa-check"></i><b>3.6</b> The Markov and Oracle Ranking Methods</a>
<ul>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#the-markov-method"><i class="fa fa-check"></i>The Markov method</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#the-oracle-method"><i class="fa fa-check"></i>The Oracle method</a></li>
<li class="chapter" data-level="" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#modifications"><i class="fa fa-check"></i>Modifications</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="eigenvalues-and-eigenvectors.html"><a href="eigenvalues-and-eigenvectors.html#exercises-2"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="orthogonality.html"><a href="orthogonality.html"><i class="fa fa-check"></i><b>4</b> Orthogonality</a>
<ul>
<li class="chapter" data-level="4.1" data-path="orthogonality.html"><a href="orthogonality.html#ILO"><i class="fa fa-check"></i><b>4.1</b> Inner Product, Length, and Orthogonality</a>
<ul>
<li class="chapter" data-level="" data-path="orthogonality.html"><a href="orthogonality.html#distance-in-mathbbrn"><i class="fa fa-check"></i>Distance in <span class="math inline">\(\mathbb{R}^n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-subspaces"><i class="fa fa-check"></i><b>4.2</b> Orthogonal Subspaces</a>
<ul>
<li class="chapter" data-level="" data-path="orthogonality.html"><a href="orthogonality.html#the-row-space-and-nullspace-of-a-matrix"><i class="fa fa-check"></i>The row space and nullspace of a matrix</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-and-orthonormal-sets"><i class="fa fa-check"></i><b>4.3</b> Orthogonal and Orthonormal Sets</a>
<ul>
<li class="chapter" data-level="" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-projections"><i class="fa fa-check"></i>Orthogonal projections</a></li>
<li class="chapter" data-level="" data-path="orthogonality.html"><a href="orthogonality.html#orthogonal-matrices"><i class="fa fa-check"></i>Orthogonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="orthogonality.html"><a href="orthogonality.html#OProj"><i class="fa fa-check"></i><b>4.4</b> Orthogonal Projections</a></li>
<li class="chapter" data-level="4.5" data-path="orthogonality.html"><a href="orthogonality.html#orthogonalization"><i class="fa fa-check"></i><b>4.5</b> Orthogonalization</a>
<ul>
<li class="chapter" data-level="" data-path="orthogonality.html"><a href="orthogonality.html#qr-factorization"><i class="fa fa-check"></i>QR-factorization</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="orthogonality.html"><a href="orthogonality.html#exercises-3"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>5</b> Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression.html"><a href="regression.html#LSQP"><i class="fa fa-check"></i><b>5.1</b> Least Squares Problems</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#least-squares-error"><i class="fa fa-check"></i>Least squares error</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#least-squares-and-the-qr-factorization"><i class="fa fa-check"></i>Least squares and the QR-factorization</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="regression.html"><a href="regression.html#application-the-massey-method"><i class="fa fa-check"></i><b>5.2</b> Application: The Massey Method</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#adjustments"><i class="fa fa-check"></i>Adjustments</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#offensive-and-defensive-ratings"><i class="fa fa-check"></i>Offensive and defensive ratings</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="regression.html"><a href="regression.html#LSRSec"><i class="fa fa-check"></i><b>5.3</b> Least Squares Regression</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#multilinear-regression"><i class="fa fa-check"></i>Multilinear regression</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="regression.html"><a href="regression.html#CorSec"><i class="fa fa-check"></i><b>5.4</b> Correlation</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#some-notation-and-a-formula"><i class="fa fa-check"></i>Some notation and a formula</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#correlation-in-r"><i class="fa fa-check"></i>Correlation in R</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="regression.html"><a href="regression.html#formulas-for-least-squares-regression"><i class="fa fa-check"></i><b>5.5</b> Formulas for Least Squares Regression</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#the-coefficient-of-determination"><i class="fa fa-check"></i>The coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="regression.html"><a href="regression.html#uncertainty-in-least-squares"><i class="fa fa-check"></i><b>5.6</b> Uncertainty in Least Squares</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#confidence-and-prediction-intervals-for-responses"><i class="fa fa-check"></i>Confidence and prediction intervals for responses</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#checking-assumptions"><i class="fa fa-check"></i>Checking assumptions</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="regression.html"><a href="regression.html#multilinear-regression-1"><i class="fa fa-check"></i><b>5.7</b> Multilinear Regression</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#r-and-multilinear-regression"><i class="fa fa-check"></i>R and multilinear regression</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#indicator-variables"><i class="fa fa-check"></i>Indicator variables</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#fitting-polynomials"><i class="fa fa-check"></i>Fitting polynomials</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#interactions"><i class="fa fa-check"></i>Interactions</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="regression.html"><a href="regression.html#model-selection"><i class="fa fa-check"></i><b>5.8</b> Model Selection</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#best-subsets-regression"><i class="fa fa-check"></i>Best subsets regression</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#stepwise-regression"><i class="fa fa-check"></i>Stepwise regression</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#some-warnings"><i class="fa fa-check"></i>Some warnings</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="regression.html"><a href="regression.html#Logistic"><i class="fa fa-check"></i><b>5.9</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#logistic-regression-with-multiple-independent-variables"><i class="fa fa-check"></i>Logistic regression with multiple independent variables</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#likelihood-and-deviance"><i class="fa fa-check"></i>Likelihood and deviance</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="regression.html"><a href="regression.html#GDA"><i class="fa fa-check"></i><b>5.10</b> Gradient Descent</a>
<ul>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#functions-of-several-variables"><i class="fa fa-check"></i>Functions of several variables</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#optimizing-parameters"><i class="fa fa-check"></i>Optimizing parameters</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="regression.html"><a href="regression.html#exercises-4"><i class="fa fa-check"></i><b>5.11</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="svd-and-pca.html"><a href="svd-and-pca.html"><i class="fa fa-check"></i><b>6</b> SVD and PCA</a>
<ul>
<li class="chapter" data-level="6.1" data-path="svd-and-pca.html"><a href="svd-and-pca.html#DSM"><i class="fa fa-check"></i><b>6.1</b> Diagonalizable and Symmetric Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#diagonalizable-matrices"><i class="fa fa-check"></i>Diagonalizable matrices</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#symmetric-matrices"><i class="fa fa-check"></i>Symmetric matrices</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#eigenvalues-and-eigenvectors-of-symmetric-matrices"><i class="fa fa-check"></i>Eigenvalues and eigenvectors of symmetric matrices</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#orthogonal-diagonalization"><i class="fa fa-check"></i>Orthogonal diagonalization</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="svd-and-pca.html"><a href="svd-and-pca.html#quadratic-forms-and-constrained-optimization"><i class="fa fa-check"></i><b>6.2</b> Quadratic Forms and Constrained Optimization</a>
<ul>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#change-of-variables"><i class="fa fa-check"></i>Change of variables</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#classifying-quadratic-forms"><i class="fa fa-check"></i>Classifying quadratic forms</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#constrained-optimization"><i class="fa fa-check"></i>Constrained optimization</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="svd-and-pca.html"><a href="svd-and-pca.html#SVD"><i class="fa fa-check"></i><b>6.3</b> The Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#singular-values"><i class="fa fa-check"></i>Singular values</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#svd-with-r"><i class="fa fa-check"></i>SVD with R</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="svd-and-pca.html"><a href="svd-and-pca.html#applications-of-the-svd"><i class="fa fa-check"></i><b>6.4</b> Applications of the SVD</a>
<ul>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#movie-reviews-and-latent-factors"><i class="fa fa-check"></i>Movie reviews and latent factors</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="svd-and-pca.html"><a href="svd-and-pca.html#principal-component-analysis"><i class="fa fa-check"></i><b>6.5</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#pca-in-r"><i class="fa fa-check"></i>PCA in R</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#biplots"><i class="fa fa-check"></i>Biplots</a></li>
<li class="chapter" data-level="" data-path="svd-and-pca.html"><a href="svd-and-pca.html#scaling"><i class="fa fa-check"></i>Scaling</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="svd-and-pca.html"><a href="svd-and-pca.html#exercises-5"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="additional-topics.html"><a href="additional-topics.html"><i class="fa fa-check"></i><b>7</b> Additional Topics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="additional-topics.html"><a href="additional-topics.html#k-means-clustering"><i class="fa fa-check"></i><b>7.1</b> <span class="math inline">\(k\)</span>-means Clustering</a>
<ul>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#k-means-clustering-1"><i class="fa fa-check"></i><span class="math inline">\(k\)</span>-means clustering</a></li>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#cluster-optimization"><i class="fa fa-check"></i>Cluster optimization</a></li>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#clustering-for-classification"><i class="fa fa-check"></i>Clustering for classification</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="additional-topics.html"><a href="additional-topics.html#collaborative-filtering"><i class="fa fa-check"></i><b>7.2</b> Collaborative Filtering</a>
<ul>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#similarity-measures"><i class="fa fa-check"></i>Similarity measures</a></li>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#collaborative-filtering-with-recommenderlab"><i class="fa fa-check"></i>Collaborative filtering with recommenderlab</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="additional-topics.html"><a href="additional-topics.html#decision-tree-classification"><i class="fa fa-check"></i><b>7.3</b> Decision Tree Classification</a>
<ul>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#decision-trees-with-r"><i class="fa fa-check"></i>Decision trees with R</a></li>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#random-forests"><i class="fa fa-check"></i>Random Forests</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="additional-topics.html"><a href="additional-topics.html#support-vector-machines"><i class="fa fa-check"></i><b>7.4</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="" data-path="additional-topics.html"><a href="additional-topics.html#support-vector-machines-in-r"><i class="fa fa-check"></i>Support vector machines in R</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="additional-topics.html"><a href="additional-topics.html#exercises-6"><i class="fa fa-check"></i><b>7.5</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="MVCA.html"><a href="MVCA.html"><i class="fa fa-check"></i><b>A</b> An Introduction to Multivariable Calculus</a>
<ul>
<li class="chapter" data-level="A.1" data-path="MVCA.html"><a href="MVCA.html#functions-of-several-variables-1"><i class="fa fa-check"></i><b>A.1</b> Functions of several variables</a>
<ul>
<li class="chapter" data-level="" data-path="MVCA.html"><a href="MVCA.html#minima-and-maxima"><i class="fa fa-check"></i>Minima and maxima</a></li>
<li class="chapter" data-level="" data-path="MVCA.html"><a href="MVCA.html#limits-and-continuity"><i class="fa fa-check"></i>Limits and continuity</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="MVCA.html"><a href="MVCA.html#partial-derivatives"><i class="fa fa-check"></i><b>A.2</b> Partial Derivatives</a>
<ul>
<li class="chapter" data-level="" data-path="MVCA.html"><a href="MVCA.html#higher-order-partial-derivatives"><i class="fa fa-check"></i>Higher order partial derivatives</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="MVCA.html"><a href="MVCA.html#directional-derivatives"><i class="fa fa-check"></i><b>A.3</b> Directional Derivatives</a>
<ul>
<li class="chapter" data-level="" data-path="MVCA.html"><a href="MVCA.html#the-gradient-vector"><i class="fa fa-check"></i>The gradient vector</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="MVCA.html"><a href="MVCA.html#optimization"><i class="fa fa-check"></i><b>A.4</b> Optimization</a>
<ul>
<li class="chapter" data-level="" data-path="MVCA.html"><a href="MVCA.html#classifying-critical-points"><i class="fa fa-check"></i>Classifying critical points</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="MVCA.html"><a href="MVCA.html#exercises-7"><i class="fa fa-check"></i><b>A.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="the-igraph-and-ggally-packages.html"><a href="the-igraph-and-ggally-packages.html"><i class="fa fa-check"></i><b>B</b> The iGraph and GGally Packages</a>
<ul>
<li class="chapter" data-level="" data-path="the-igraph-and-ggally-packages.html"><a href="the-igraph-and-ggally-packages.html#ggally"><i class="fa fa-check"></i>GGally</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i><b>C</b> Packages</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Algebra for Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Regression<a href="regression.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The focus in this chapter is regression: fitting linear and similar models to data. There is a beautiful connection between this problem and the matrix algebra material that we have studied so far.</p>
<div id="LSQP" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Least Squares Problems<a href="regression.html#LSQP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some linear equations have no solution. For instance, if we try to solve <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>, where</p>
<p><span class="math display">\[\mathbf{A}=\begin{bmatrix}4 &amp; 0\\0 &amp; 2\\ 1&amp; 1\end{bmatrix}, \text{ and } \mathbf{b}=\begin{bmatrix}2\\0\\11\end{bmatrix},\]</span></p>
<p>we run into a problem when we put the augmented matrix in reduced row echelon form:</p>
<p><span class="math display">\[\left[\begin{array}{cc|c}4 &amp; 0 &amp; 2\\0 &amp; 2 &amp; 0\\1 &amp; 1 &amp; 11\end{array}\right]\leadsto \left[\begin{array}{cc|c} 1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0\\0 &amp; 0 &amp; 1\end{array}\right].\]</span></p>
<p>Remember that the last row says <span class="math inline">\(0=1\)</span>, so there is no solution. In terms of linear algebra, what is going on here? The matrix product</p>
<p><span class="math display">\[\mathbf{A}\mathbf{x}=\begin{bmatrix}4 &amp; 0\\0 &amp; 2\\ 1&amp; 1\end{bmatrix}\begin{bmatrix}x_1\\x_2\end{bmatrix}=\begin{bmatrix}4x_1+0x_2\\0x_1+2x_2\\1x_1+1x_2\end{bmatrix}=x_1\begin{bmatrix}4\\0\\1\end{bmatrix}+x_2\begin{bmatrix}0\\2\\1\end{bmatrix}\]</span></p>
<p>is really a linear combination of the columns of <span class="math inline">\(\mathbf{A}.\)</span> There is no solution because</p>
<p><span class="math display">\[\mathbf{b}=\begin{bmatrix}2\\0\\11\end{bmatrix}\]</span></p>
<p>cannot be written as a linear combination of the columns of <span class="math inline">\(\mathbf{A}.\)</span> That is, <span class="math inline">\(\mathbf{b}\)</span> is not in the column space of <span class="math inline">\(\mathbf{A}.\)</span></p>
<p>In a <em>least squares problem</em>, we project <span class="math inline">\(\mathbf{b}\)</span> into the column space of <span class="math inline">\(\mathbf{A}\)</span> (see Fig. <a href="regression.html#fig:oproj2">5.1</a>)</p>
<p><span class="math display">\[\hat{\mathbf{b}}=\text{proj}_{\text{Col}(\mathbf{A})}\mathbf{b},\]</span></p>
<p>and solve <span class="math inline">\(\mathbf{A}\mathbf{x}=\hat{\mathbf{b}}\)</span>, which we know will have a solution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:oproj2"></span>
<img src="images/lsqvec.png" alt="A projection onto the column space" width="75%" />
<p class="caption">
Figure 5.1: A projection onto the column space
</p>
</div>
<p>Why do we call it “least squares?” Remember from Theorem <a href="orthogonality.html#thm:lsp">4.12</a> that the projection <span class="math inline">\(\hat{\mathbf{b}}\)</span> satisfies</p>
<p><span class="math display">\[|\mathbf{b}-\hat{\mathbf{b}}|&lt;|\mathbf{b}-\mathbf{v}|\]</span></p>
<p>for any vector <span class="math inline">\(\mathbf{v}\)</span> in <span class="math inline">\(\text{Col}(\mathbf{A})\)</span> different from <span class="math inline">\(\hat{\mathbf{b}}\)</span>. If <span class="math inline">\(\mathbf{b}=(b_1,b_2,b_3)\)</span> and <span class="math inline">\(\mathbf{v}=(v_1,v_2,v_3),\)</span> then</p>
<p><span class="math display">\[|\mathbf{b}-\mathbf{v}|^2=(b_1-v_1)^2+(b_2-v_2)^2+(b_3-v_3)^2.\]</span></p>
<p>The projection <span class="math inline">\(\hat{\mathbf{b}}\)</span> minimizes this sum of squares over all possible vectors in <span class="math inline">\(\text{Col}(\mathbf{A})\)</span>.</p>
<p>How do we find the least squares solution? We want to find a vector <span class="math inline">\(\hat{\mathbf{x}}\)</span> that satisfies</p>
<p><span class="math display">\[\mathbf{A}\hat{\mathbf{x}}=\hat{\mathbf{b}}.\]</span></p>
<p>The key is that <span class="math inline">\(\mathbf{b}-\hat{\mathbf{b}}\)</span> is orthogonal to the column space of <span class="math inline">\(\mathbf{A}.\)</span> If <span class="math inline">\(\mathbf{a}_j\)</span> is any column of <span class="math inline">\(\mathbf{A},\)</span> then <span class="math inline">\(\mathbf{a}_j\cdot(\mathbf{b}-\hat{\mathbf{b}})=0\)</span>, or</p>
<p><span class="math display">\[\mathbf{a}_j^T (\mathbf{b}-\hat{\mathbf{b}})=\mathbf{a}_j^T (\mathbf{b}-\mathbf{A}\hat{\mathbf{x}})=0.\]</span></p>
<p>Since the <span class="math inline">\(\mathbf{a}_j^T\)</span> vectors form the rows of <span class="math inline">\(\mathbf{A}^T\)</span> we actually have</p>
<p><span class="math display">\[\mathbf{A}^T(\mathbf{b}-\mathbf{A}\hat{\mathbf{x}})=\mathbf{0}.\]</span></p>
<p>When we apply the distributive law to</p>
<p><span class="math display">\[\mathbf{A}^T(\mathbf{b}-\mathbf{A}\hat{\mathbf{x}})=\mathbf{0},\]</span></p>
<p>we get</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{b}-\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}}=\mathbf{0},\]</span></p>
<p>or</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}}=\mathbf{A}^T\mathbf{b}.\]</span></p>
<p>This leads us to what are called the <em>normal equations</em>.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:NormalEq" class="theorem"><strong>Theorem 5.1  </strong></span>The set of least-squares solutions of <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span> coincides with the nonempty set of solutions of the normal equations</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}}=\mathbf{A}^T\mathbf{b}.\]</span></p>
</div>
</div>
<p>By the nature of the problem, there will always be a least-squares solution for the equation <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>. Under additional conditions, the solution will be unique. We’ll see those conditions in a bit.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:ls1" class="example"><strong>Example 5.1  </strong></span>Let’s return to our example</p>
<p><span class="math display">\[\begin{bmatrix}4 &amp; 0\\0 &amp; 2\\ 1&amp; 1\end{bmatrix}\mathbf{x}=\begin{bmatrix}2\\0\\11\end{bmatrix}\]</span></p>
<p>and find a least squares solution. We calculate</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{A}=\begin{bmatrix}4 &amp; 0 &amp; 1\\0 &amp; 2 &amp; 1\end{bmatrix}\begin{bmatrix}4 &amp; 0\\0 &amp; 2\\ 1&amp; 1\end{bmatrix}=\begin{bmatrix}17 &amp; 1\\1 &amp; 5\end{bmatrix}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{b}=\begin{bmatrix}4 &amp; 0 &amp; 1\\0 &amp; 2 &amp; 1\end{bmatrix}\begin{bmatrix}2\\0\\11\end{bmatrix}=\begin{bmatrix}19\\11\end{bmatrix}.\]</span></p>
<p>To find our least squares solution, we solve
<span class="math inline">\(\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}}=\mathbf{A}^T\mathbf{b}\)</span>:</p>
<p><span class="math display">\[\begin{bmatrix}17 &amp; 1\\1 &amp; 5\end{bmatrix}\hat{\mathbf{x}}=\begin{bmatrix}19\\11\end{bmatrix}.\]</span></p>
<p>Using the inverse of <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span>, we get our solution:</p>
<p><span class="math display">\[\hat{\mathbf{x}}=\frac{1}{17\cdot 5-1\cdot 1}\begin{bmatrix}5 &amp; -1\\-1 &amp; 17\end{bmatrix}\begin{bmatrix}19\\11\end{bmatrix}=\frac{1}{84}\begin{bmatrix}84\\168\end{bmatrix}=\begin{bmatrix}1\\2\end{bmatrix}.\]</span></p>
</div>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:ims" class="example"><strong>Example 5.2  </strong></span>Find the least squares solution to</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\1 &amp; 1 &amp; 0 &amp; 0\\1 &amp; 0 &amp; 1 &amp; 0\\1 &amp; 0 &amp; 1 &amp; 0\\1 &amp; 0 &amp; 0 &amp; 1\\1 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}\mathbf{x}=\begin{bmatrix}-3\\-1\\0\\2\\5\\1\end{bmatrix}.\]</span></p>
<p>Note that the first column is the sum of the other three, so the columns are linearly dependent.</p>
<p>If we try to find a regular (non-least squares) solution to the equation via row reduction, we get</p>
<p><span class="math display">\[\left[\begin{array}{cccc|r} 1 &amp; 1 &amp; 0 &amp; 0 &amp; -3\\1 &amp; 1 &amp; 0 &amp; 0 &amp; -1\\1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 2\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 5\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1\end{array}\right]\leadsto \left[\begin{array}{cccr|r} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\0 &amp; 1 &amp; 0 &amp; -1 &amp; 0\\0 &amp; 0 &amp; 1 &amp; -1 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right],\]</span></p>
<p>so there is no solution. For the normal equations, we find</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{A}=\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\end{bmatrix}\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\1 &amp; 1 &amp; 0 &amp; 0\\1 &amp; 0 &amp; 1 &amp; 0\\1 &amp; 0 &amp; 1 &amp; 0\\1 &amp; 0 &amp; 0 &amp; 1\\1 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}=\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2\\2 &amp; 2 &amp; 0 &amp; 0\\2 &amp; 0 &amp; 2 &amp; 0\\2 &amp; 0 &amp; 0 &amp; 2\end{bmatrix},\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{b}=\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\end{bmatrix}\begin{bmatrix}-3\\-1\\0\\2\\5\\1\end{bmatrix}=\begin{bmatrix}4\\-4\\2\\6\end{bmatrix}.\]</span></p>
<p>It turns out that <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span> is not invertible, so we use row reduction to solve:</p>
<p><span class="math display">\[\left[\begin{array}{cccc|r} 6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\2 &amp; 2 &amp; 0 &amp; 0&amp; -4\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\2 &amp; 0 &amp; 0 &amp; 2&amp; 6\end{array}\right] \leadsto \left[\begin{array}{cccr|r}1 &amp; 0 &amp; 0 &amp; 1 &amp; 3\\0 &amp; 1 &amp; 0 &amp; -1 &amp; -5\\0 &amp; 0 &amp; 1 &amp; -1 &amp; -2\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right].\]</span></p>
<p>With <span class="math inline">\(x_4=t\)</span> as a free variable, we have <span class="math inline">\(x_1=3-t,x_2=-5+t\)</span>, and <span class="math inline">\(x_3=-2+t\)</span>. We can write our least squares solution as</p>
<p><span class="math display">\[\hat{\mathbf{x}}=\begin{bmatrix}3\\-5\\-2\\0\end{bmatrix}+t\begin{bmatrix}-1\\1\\1\\1\end{bmatrix}.\]</span></p>
<p>Note that the vector multiplied by <span class="math inline">\(t\)</span> is in the null space of <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span> and the null space of <span class="math inline">\(\mathbf{A},\)</span> too. They actually share the same null space.</p>
</div>
</div>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:unlabeled-div-92" class="theorem"><strong>Theorem 5.2  </strong></span>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(m\times n\)</span> matrix. Then <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span> have the same nullspace.</p>
</div>
</div>
<div class="proof">
<p><span id="unlabeled-div-93" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\mathbf{x}\in \text{Null}(\mathbf{A})\)</span>. Then</p>
<p><span class="math display">\[ (\mathbf{A}^T\mathbf{A})\mathbf{x}=\mathbf{A}^T(\mathbf{A}\mathbf{x})=\mathbf{A}^T\mathbf{0}=\mathbf{0}.\]</span></p>
<p>Any vector in the nullspace of <span class="math inline">\(\mathbf{A}\)</span> is also in the nullspace of <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span>. Suppose <span class="math inline">\(\mathbf{x}\in\text{Null}(\mathbf{A}^T\mathbf{A}).\)</span> Then</p>
<p><span class="math display">\[\mathbf{A}^T(\mathbf{A}\mathbf{x})=(\mathbf{A}^T\mathbf{A})\mathbf{x}=\mathbf{0}.\]</span></p>
<p>That means that <span class="math inline">\(\mathbf{A}\mathbf{x}\in \text{Null}(\mathbf{A}^T).\)</span> From Theorem <a href="orthogonality.html#thm:MatSubspaces">4.6</a> (see, we’re using it), we know that <span class="math inline">\(\text{Null}(\mathbf{A}^T)=\text{Col}(\mathbf{A})^{\perp}.\)</span> The vector <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> is simultaneously in <span class="math inline">\(\text{Col}(\mathbf{A})^{\perp}\)</span> and <span class="math inline">\(\text{Col}(\mathbf{A})\)</span>. The only vector common to a subspace and its orthogonal complement is the zero vector, so <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{0}\)</span> and <span class="math inline">\(\mathbf{x}\in\text{Null}(\mathbf{A}).\)</span> Any vector in <span class="math inline">\(\text{Null}(\mathbf{A}^T\mathbf{A})\)</span> is also in <span class="math inline">\(\text{Null}(\mathbf{A}).\)</span> They are the same subspace.</p>
</div>
<p>Why did Example <a href="regression.html#exm:ims">5.2</a> have infinitely many solutions? Since the columns of <span class="math inline">\(\mathbf{A}\)</span> were not linearly independent, there were actually infinitely many ways to write the projection <span class="math inline">\(\hat{\mathbf{b}}\)</span> in terms of them. This does not happen when the columns are linearly independent.</p>
<div class="theorembox">
<div class="theorem">
<p><span id="thm:unlabeled-div-94" class="theorem"><strong>Theorem 5.3  </strong></span>Let <span class="math inline">\(\mathbf{A}\)</span> be an <span class="math inline">\(m\times n\)</span> matrix. The following are equivalent:</p>
<ol style="list-style-type: decimal">
<li><p>The equation <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span> has a unique least squares solution for each <span class="math inline">\(\mathbf{b}\)</span> in <span class="math inline">\(\mathbb{R}^m\)</span>.</p></li>
<li><p>The columns of <span class="math inline">\(\mathbf{A}\)</span> are linearly independent.</p></li>
<li><p>The matrix <span class="math inline">\(\mathbf{A}^T\mathbf{A}\)</span> is invertible.</p></li>
</ol>
<p>When these statements are true, the least squares solution is given by</p>
<p><span class="math display">\[\hat{\mathbf{x}}=(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}.\]</span></p>
</div>
</div>
<p>The last formula</p>
<p><span class="math display">\[\hat{\mathbf{x}}=(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}\]</span></p>
<p>looks awkward, and we might be tempted to use the formula for the inverse of a product</p>
<p><span class="math display">\[(\mathbf{A}\mathbf{B})^{-1}=\mathbf{B}^{-1}\mathbf{A}^{-1},\]</span>
but there are two issues:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{A}^T\)</span> are not necessarily invertible and,</p></li>
<li><p>They don’t even need to be square.</p></li>
</ol>
<p>In fact, if <span class="math inline">\(\mathbf{A}\)</span> were invertible, then we wouldn’t need a least squares solution, since <span class="math inline">\(\mathbf{x}=\mathbf{A}^{-1}\mathbf{b}\)</span> is a regular solution.</p>
<div id="least-squares-error" class="section level3 unnumbered hasAnchor">
<h3>Least squares error<a href="regression.html#least-squares-error" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How close does the least squares solution come to being an actual solution? The distance between <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\hat{\mathbf{b}}\)</span>, which is <span class="math inline">\(|\mathbf{b}-\hat{\mathbf{b}}|=|\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}|\)</span>, tells us this.</p>
<p>In Example <a href="regression.html#exm:ls1">5.1</a>,</p>
<p><span class="math display">\[\mathbf{b}=\begin{bmatrix}2\\0\\11\end{bmatrix},\]</span></p>
<p>while</p>
<p><span class="math display">\[\hat{\mathbf{b}}=\mathbf{A}\hat{\mathbf{x}}=\begin{bmatrix}4&amp;0\\0 &amp; 2\\1 &amp; 1\end{bmatrix}\begin{bmatrix}1\\2\end{bmatrix}=\begin{bmatrix}4\\4\\3\end{bmatrix}.\]</span></p>
<p>The least squares error is</p>
<p><span class="math display">\[\sqrt{(2-4)^2+(0-4)^2+(11-3)^2}=\sqrt{84}\approx 9.2.\]</span></p>
</div>
<div id="least-squares-and-the-qr-factorization" class="section level3 unnumbered hasAnchor">
<h3>Least squares and the QR-factorization<a href="regression.html#least-squares-and-the-qr-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the columns of <span class="math inline">\(\mathbf{A}\)</span> are linearly independent, then we can write <span class="math inline">\(\mathbf{A}=\mathbf{Q}\mathbf{R}\)</span>, where <span class="math inline">\(\mathbf{Q}\)</span> has orthonormal columns and <span class="math inline">\(\mathbf{R}\)</span> is invertible. In this case, the least squares solution is given by</p>
<p><span class="math display">\[\hat{\mathbf{x}}=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{b}.\]</span></p>
<p>Why does this work? If <span class="math inline">\(\hat{\mathbf{x}}=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{b}\)</span>, then</p>
<p><span class="math display">\[\mathbf{A}\hat{\mathbf{x}}=\mathbf{Q}\mathbf{R}\hat{\mathbf{x}}=\mathbf{Q}\mathbf{R}\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{b}=\mathbf{Q}\mathbf{Q}^T\mathbf{b},\]</span></p>
<p>which we learned in Theorem <a href="orthogonality.html#thm:Oproj">4.13</a> is the orthogonal projection of <span class="math inline">\(\mathbf{b}\)</span> onto the column space of <span class="math inline">\(\mathbf{Q}\)</span>, which is the same as the column space of <span class="math inline">\(\mathbf{A}\)</span>. This projection is <span class="math inline">\(\hat{\mathbf{b}}\)</span>.</p>
</div>
</div>
<div id="application-the-massey-method" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Application: The Massey Method<a href="regression.html#application-the-massey-method" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, all of our ranking methods have ignored the points scored by teams or players. The <em>Massey Method</em> is a least squares method that is actually designed to predict the point spread in a competition. Ken Massey developed his method as part of an honors project in college <span class="citation">[<a href="#ref-Massey">15</a>]</span>. His site is also an excellent source of sports schedule data.</p>
<p>The development and notation in this section come from Amy Langville and Carl Meyer <span class="citation">[<a href="#ref-No1">11</a>]</span>. The idea behind the ratings is that if teams <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> play in game number <span class="math inline">\(k\)</span>, the point spread <span class="math inline">\(y_k\)</span> will just be the difference of the two teams ratings:</p>
<p><span class="math display">\[r_i-r_j=y_k.\]</span></p>
<p>Note that this method can easily handle ties, since we can just set the margin of victory to be 0. If there are <span class="math inline">\(n\)</span> teams in the league, and a total of <span class="math inline">\(m\)</span> games, we will get an <span class="math inline">\(m\times n\)</span> system of equations. Since the number of games will quickly exceed the number of teams, the system of equations will be overdetermined, but we can still find a least squares solution.</p>
<p>Recall our toy example.</p>
<table>
<colgroup>
<col width="17%" />
<col width="19%" />
<col width="16%" />
<col width="14%" />
<col width="16%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="right"><strong>Aardvarks</strong></th>
<th align="right"><strong>Beagles</strong></th>
<th align="right"><strong>Crocs</strong></th>
<th align="right"><strong>Donkeys</strong></th>
<th align="right"><strong>Egrets</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Aardvarks</strong></td>
<td align="right">x</td>
<td align="right">17-31</td>
<td align="right">6-42</td>
<td align="right">x</td>
<td align="right">17-7</td>
</tr>
<tr class="even">
<td><strong>Beagles</strong></td>
<td align="right"></td>
<td align="right">x</td>
<td align="right">3-59</td>
<td align="right">10-35</td>
<td align="right">x</td>
</tr>
<tr class="odd">
<td><strong>Crocs</strong></td>
<td align="right"></td>
<td align="right"></td>
<td align="right">x</td>
<td align="right">21-20</td>
<td align="right">49-0</td>
</tr>
<tr class="even">
<td><strong>Donkeys</strong></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right">x</td>
<td align="right">28-21</td>
</tr>
<tr class="odd">
<td><strong>Egrets</strong></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right">x</td>
</tr>
</tbody>
</table>
<p>The first few lines of the system of equations will look like this</p>
<p><span class="math display">\[ \left\{\begin{array}{rrr}
    r_1-r_2&amp;=&amp;-14\\
    r_1-r_3&amp;=&amp;-36\\
    r_1-r_5&amp;=&amp;10
\end{array} \right. .
\]</span></p>
<p>In matrix form, the full system looks like this.</p>
<p><span class="math display">\[\left[\begin{array}{rrrrr}1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\1 &amp; 0 &amp; 0 &amp; 0 &amp; -1\\0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\0 &amp; 1 &amp; 0 &amp; -1 &amp; 0\\0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -1\end{array}\right]\begin{bmatrix}r_1\\r_2\\r_3\\r_4\\r_5\end{bmatrix}=\left[\begin{array}{r}-14 \\-36\\10\\-56\\-25\\1\\49\\7\end{array}\right]\]</span></p>
<p>or</p>
<p><span class="math display">\[\mathbf{X}\mathbf{r}=\mathbf{y}.\]</span></p>
<p>This system is overdetermined, so we instead solve the normal equations</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}\mathbf{r}=\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>That system is</p>
<p><span class="math display">\[\left[\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\-1 &amp; 0 &amp; -1 &amp; -1 &amp; 3\end{array}\right]\begin{bmatrix}r_1\\r_2\\r_3\\r_4\\r_5\end{bmatrix}=\left[\begin{array}{r}-40\\-67\\142\\31\\-66\end{array}\right].\]</span></p>
<p>First note that <span class="math inline">\(\mathbf{M}=\mathbf{X}^T\mathbf{X}\)</span> looks very similar to the Colley matrix. The only difference is that the diagonal entries are the number of games that each team has played. The <span class="math inline">\(ij\)</span>-entry is still the number of times teams <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> have played, multiplied by <span class="math inline">\(-1\)</span>.</p>
<p>The <span class="math inline">\(\mathbf{p}=\mathbf{X}^T\mathbf{y}\)</span> vector represents the scoring difference for each team. For instance, Team 1 (The Aardvarks) scored 40 fewer points than their opponents, while the Crocs (Team 3) outscored their opponents by a whopping 142 points, while the Aardvarks were outscored by 40 points.</p>
<p>There is one problem with this system. The columns (and rows) all add up to 0, so they are dependent and there will not be a unique solution. Recall that if the columns of the original matrix were dependent, then there is not a unique solution to the least squares problem. In our original matrix, the rows all added up to 0, because each 1 was paired with a -1.</p>
<p>Massey’s workaround is to replace one of the rows (we will chose the last one) of <span class="math inline">\(\mathbf{M}\)</span> with a row of ones to create <span class="math inline">\(\overline{\mathbf{M}}\)</span>. He also replaces the last entry in <span class="math inline">\(\mathbf{p}\)</span> with a 0 to create <span class="math inline">\(\bar{\mathbf{p}}\)</span>. This essentially adds a new constraint on the system: all of the ratings will add up to 0. Here is the revised system:</p>
<p><span class="math display">\[\left[\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\end{array}\right]\begin{bmatrix}r_1\\r_2\\r_3\\r_4\\r_5\end{bmatrix}=\left[\begin{array}{r}-40\\-67\\142\\31\\0\end{array}\right].\]</span></p>
<p>or</p>
<p><span class="math display">\[\overline{\mathbf{M}}\mathbf{r}=\bar{\mathbf{p}}.\]</span></p>
<p>When we solve the revised system, we get our ratings vector:
<span class="math display">\[\hat{\mathbf{r}}=\begin{bmatrix}-12.73\\-13.47\\28.4\\10.93\\-13.13\end{bmatrix}.\]</span></p>
<p>Our standings in order of Massey ratings is as follows.</p>
<table>
<thead>
<tr class="header">
<th align="left">Team</th>
<th align="left">W-L</th>
<th align="left">PCT</th>
<th align="right">Rating</th>
<th align="right">PF</th>
<th align="right">PA</th>
<th align="right">PD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Crocs</td>
<td align="left">4-0</td>
<td align="left">1.000</td>
<td align="right">28.40</td>
<td align="right">171</td>
<td align="right">29</td>
<td align="right">142</td>
</tr>
<tr class="even">
<td align="left">Donkeys</td>
<td align="left">2-1</td>
<td align="left">0.667</td>
<td align="right">10.93</td>
<td align="right">83</td>
<td align="right">52</td>
<td align="right">31</td>
</tr>
<tr class="odd">
<td align="left">Aardvarks</td>
<td align="left">1-2</td>
<td align="left">0.333</td>
<td align="right">-12.73</td>
<td align="right">40</td>
<td align="right">80</td>
<td align="right">-40</td>
</tr>
<tr class="even">
<td align="left">Egrets</td>
<td align="left">0-3</td>
<td align="left">0.000</td>
<td align="right">-13.13</td>
<td align="right">28</td>
<td align="right">94</td>
<td align="right">-66</td>
</tr>
<tr class="odd">
<td align="left">Beagles</td>
<td align="left">1-2</td>
<td align="left">0.333</td>
<td align="right">-13.47</td>
<td align="right">44</td>
<td align="right">111</td>
<td align="right">-67</td>
</tr>
</tbody>
</table>
<p>Note that the Beagles drop to the bottom of the ratings, mostly due to their blowout losses to the Crocs and Donkeys.</p>
<p>Again, the difference in ratings can be used to predict point spreads. If the Aardvarks play the Donkeys, the predicted point spread is <span class="math inline">\((-12.73)-10.93=-23.66\)</span>. The Donkeys are 23.66-point favorites.</p>
<div class="propbox">
<p><strong>The Massey Method</strong></p>
<ol style="list-style-type: decimal">
<li><p>Create the matrix <span class="math inline">\(\mathbf{M}\)</span>, whose diagonal entries <span class="math inline">\(\mathbf{M}_{ii}\)</span> are the number of games played by Team <span class="math inline">\(i\)</span>, and whose off-diagonal entries <span class="math inline">\(\mathbf{M}_{ij}\)</span> are the number of games between Teams <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> multiplied by <span class="math inline">\(-1\)</span>.</p></li>
<li><p>Create the column vector <span class="math inline">\(\mathbf{p}\)</span>, whose <span class="math inline">\(i\)</span>th entry is the point difference for Team <span class="math inline">\(i\)</span>.</p></li>
<li><p>Create the matrix <span class="math inline">\(\overline{\mathbf{M}}\)</span> by replacing the last row of <span class="math inline">\(\mathbf{M}\)</span> with a row of ones.</p></li>
<li><p>Create the vector <span class="math inline">\(\bar{\mathbf{p}}\)</span> by replacing the last entry of <span class="math inline">\(\mathbf{p}\)</span> with a 0.</p></li>
<li><p>Solve <span class="math inline">\(\overline{\mathbf{M}}\mathbf{r}=\bar{\mathbf{p}}\)</span> for the ratings vector <span class="math inline">\(\mathbf{r}=\hat{\mathbf{r}}\)</span>.</p></li>
</ol>
</div>
<div id="adjustments" class="section level3 unnumbered hasAnchor">
<h3>Adjustments<a href="regression.html#adjustments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What if you don’t want to take points into consideration? You can change the margin of victory to 1 in every game. In this case, the new <span class="math inline">\(\mathbf{X}^T\mathbf{y}\)</span> vector will just be the number of wins minus the number of losses for each team. For our running example, the system becomes</p>
<p><span class="math display">\[\left[\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\end{array}\right]\begin{bmatrix}r_1\\r_2\\r_3\\r_4\\r_5\end{bmatrix}=\left[\begin{array}{r}-1\\-1\\4\\1\\0\end{array}\right].\]</span></p>
<p>The new ratings vector is <span class="math inline">\((-0.333,-0.067,0.8,0.333,-0.733)\)</span>. The Beagles aren’t penalized by their blowout losses. (We could also impose a cap on the margin of victory.)</p>
</div>
<div id="offensive-and-defensive-ratings" class="section level3 unnumbered hasAnchor">
<h3>Offensive and defensive ratings<a href="regression.html#offensive-and-defensive-ratings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Massey’s method also allows you to split a team’s rating into offensive and defensive ratings. Let <span class="math inline">\(\mathbf{f}\)</span> and <span class="math inline">\(\mathbf{a}\)</span> be the vectors of points for and against, respectively, for each team. Then the point difference vector <span class="math inline">\(\mathbf{p}=\mathbf{f}-\mathbf{a}\)</span>. Write <span class="math inline">\(\mathbf{M}=\mathbf{T}-\mathbf{P},\)</span> where <span class="math inline">\(\mathbf{T}\)</span> is the diagonal part of <span class="math inline">\(\mathbf{M}\)</span>, representing the number of games each team has played, and <span class="math inline">\(\mathbf{P}\)</span> shows the number of matchups between each pair of teams.</p>
<p>We now introduce the offensive <span class="math inline">\(\mathbf{o}\)</span> and defensive <span class="math inline">\(\mathbf{d}\)</span> vectors, with <span class="math inline">\(\mathbf{r}=\mathbf{o}+\mathbf{d}\)</span> as follows.</p>
<p><span class="math display">\[\begin{align*}
    \mathbf{M}\mathbf{r}&amp;=\mathbf{p}\\
    (\mathbf{T}-\mathbf{P})\mathbf{r}&amp;=\mathbf{p}\\
    (\mathbf{T}-\mathbf{P})(\mathbf{o}+\mathbf{d})&amp;=\mathbf{p}\\
    \mathbf{T}\mathbf{o}-\mathbf{P}\mathbf{o}+\mathbf{T}\mathbf{d}-\mathbf{P}\mathbf{d}&amp;=\mathbf{p}\\
    \mathbf{T}\mathbf{o}-\mathbf{P}\mathbf{o}+\mathbf{T}\mathbf{d}-\mathbf{P}\mathbf{d}&amp;=\mathbf{f}-\mathbf{a}.
\end{align*}\]</span></p>
<p>We split this last equation into two separate ones:</p>
<p><span class="math display">\[\mathbf{T}\mathbf{o}-\mathbf{P}\mathbf{d}=\mathbf{f}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{T}\mathbf{d}-\mathbf{P}\mathbf{o}=-\mathbf{a}.\]</span></p>
<p>The first equation says that the number of points a team scores depends on their offensive rating and their opponents’ defensive ratings. We can interpret the second in a similar manner.</p>
<p>Having already solved for <span class="math inline">\(\mathbf{r},\)</span> we can use the first equation to solve for <span class="math inline">\(\mathbf{d},\)</span> using <span class="math inline">\(\mathbf{o}=\mathbf{r}-\mathbf{d}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
    \mathbf{T}\mathbf{o}-\mathbf{P}\mathbf{d}&amp;=\mathbf{f}\\
  \mathbf{T}(\mathbf{r}-\mathbf{d})-\mathbf{P}\mathbf{d}&amp;=\mathbf{f}\\
(\mathbf{T}+\mathbf{P})\mathbf{d}&amp;=\mathbf{T}\mathbf{r}-\mathbf{f}.
\end{align*}\]</span></p>
<p>We can solve this for <span class="math inline">\(\mathbf{d},\)</span> and once we know <span class="math inline">\(\mathbf{d},\)</span> we can find <span class="math inline">\(\textbf{o}.\)</span> Here are the results.</p>
<table>
<thead>
<tr class="header">
<th align="left">Team</th>
<th align="right">Rating</th>
<th align="right">Offense</th>
<th align="right">Defense</th>
<th align="left">Rank</th>
<th align="left">OffRank</th>
<th align="left">DefRank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Crocs</td>
<td align="right">28.40</td>
<td align="right">28.82</td>
<td align="right">-0.43</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">Donkeys</td>
<td align="right">10.93</td>
<td align="right">14.34</td>
<td align="right">-3.41</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">Aardvarks</td>
<td align="right">-12.73</td>
<td align="right">0.01</td>
<td align="right">-12.74</td>
<td align="left">3</td>
<td align="left">5</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">Egrets</td>
<td align="right">-13.13</td>
<td align="right">3.81</td>
<td align="right">-16.94</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">4</td>
</tr>
<tr class="odd">
<td align="left">Beagles</td>
<td align="right">-13.47</td>
<td align="right">9.14</td>
<td align="right">-22.61</td>
<td align="left">5</td>
<td align="left">3</td>
<td align="left">5</td>
</tr>
</tbody>
</table>
<p>The Massey method has some issues. First, if the underlying graph is disconnected, then the Massey matrix <span class="math inline">\(\bar{\textbf{M}}\)</span> will still be singular. For example, in the following matrix, the first three teams have no connections with the last three. After adjustment, the Massey matrix is still singular. (Check to see that <span class="math inline">\(\det \bar{\mathbf{M}}=0\)</span>.)</p>
<p><span class="math display">\[\bar{\mathbf{M}}=\left[\begin{array}{rrrrrr}2 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\-1 &amp; 2 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\-1 &amp; -1 &amp; 2 &amp; 0 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 0 &amp; 2 &amp; -1 &amp; -1\\0 &amp; 0 &amp; 0 &amp; -1 &amp; 2 &amp; -1\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\end{array}\right]
\]</span></p>
<p>It is also possible that the matrix <span class="math inline">\(\textbf{T}+\textbf{P}\)</span> is singular, leading to non-unique offensive and defensive ratings. It will take some further manipulation to come up with reasonable adjustments in both of these cases.</p>
</div>
</div>
<div id="LSRSec" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Least Squares Regression<a href="regression.html#LSRSec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You may have seen least squares regression in statistics or some other class. The idea is that, given a collection of pairs of points <span class="math inline">\((x_1,y_1),(x_2,y_2),\dots,(x_n,y_n),\)</span> we’d like to find a function of the form <span class="math inline">\(y=b_0+b_1x\)</span> that fits the data the best.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lfit1"></span>
<img src="_main_files/figure-html/lfit1-1.png" alt="A scatterplot" width="75%" />
<p class="caption">
Figure 5.2: A scatterplot
</p>
</div>
<p>The problem is, when you have three or more points, they probably won’t lie on the same line, as in the scatterplot in Figure <a href="regression.html#fig:lfit1">5.2</a>. If they were on a straight line, then we could get <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> by solving the system of equations</p>
<p><span class="math display">\[\begin{align*}
    b_0+b_1x_1&amp;=y_1\\
    b_0+b_1x_2&amp;=y_2\\
    \vdots &amp;\\
    b_0+b_1x_n&amp;=y_n,
\end{align*}\]</span></p>
<p>which we can write in matrix form as</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; x_1\\1 &amp; x_2\\\vdots &amp; \vdots\\1 &amp; x_n\end{bmatrix}\begin{bmatrix}b_0\\b_1\end{bmatrix}=\begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\]</span></p>
<p>or <span class="math inline">\(\mathbf{X}\mathbf{b}=\mathbf{y}\)</span>. The matrix <span class="math inline">\(\mathbf{X}\)</span> is called the <em>design matrix</em>.</p>
<p>If you do have more than two pairs of points, the system is overdetermined, and there will only be a solution if the points all lie on a straight line. If not, we can, find a least squares approximation. That least squares solution will be <span class="math inline">\(\hat{\mathbf{b}}\)</span>, the solution to the normal equations</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}\mathbf{b}=\mathbf{X}^T\mathbf{y}.\]</span></p>
<p>It will be unique if the columns of <span class="math inline">\(X\)</span> are linearly independent. Since there are only two columns in this case, and the first column is all ones, this boils down to having at least two different <span class="math inline">\(x\)</span> values in the data.</p>
<p>The points on the scatterplot in Figure (<a href="regression.html#fig:lfit1">5.2</a>) are <span class="math inline">\((4,1),(8,6),(10,5),\)</span> and <span class="math inline">\((7,7)\)</span>. That makes our system</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; 4\\1 &amp; 8\\1 &amp; 10\\1 &amp; 7\end{bmatrix}\mathbf{b}=\begin{bmatrix}1\\6\\5\\7\end{bmatrix}.\]</span></p>
<p>We have</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}=\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1\\4 &amp; 8 &amp; 10 &amp; 7\end{bmatrix}\begin{bmatrix}1 &amp; 4\\1 &amp; 8\\1 &amp; 10\\1 &amp; 7\end{bmatrix}=\begin{bmatrix}4 &amp; 29\\29 &amp; 229\end{bmatrix}.\]</span></p>
<p>Also,</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{y}=\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1\\4 &amp; 8 &amp; 10 &amp; 7\end{bmatrix}\begin{bmatrix}1\\6\\5\\7\end{bmatrix}=\begin{bmatrix}19\\151\end{bmatrix},\]</span></p>
<p>so our least squares solution is</p>
<p><span class="math display">\[\hat{\mathbf{b}}=\frac{1}{4\cdot229-29\cdot29}\begin{bmatrix}229 &amp; -29\\-29 &amp; 4\end{bmatrix}\begin{bmatrix}19\\151\end{bmatrix}=\frac{1}{75}\begin{bmatrix}-28\\53\end{bmatrix}.\]</span></p>
<p>Our least squares coefficients are <span class="math inline">\(\hat{b}_0=-\frac{28}{75}\approx -0.373\)</span> and <span class="math inline">\(\hat{b}_1=\frac{53}{75}\approx0.707\)</span>. When we put the line <span class="math inline">\(y=-\frac{28}{75}+\frac{53}{75}x\)</span> on the scatterplot, we see the least squares regression line (Fig. <a href="regression.html#fig:lfit2">5.3</a>). It’s not a great fit, mostly because of the point <span class="math inline">\((4,1).\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lfit2"></span>
<img src="_main_files/figure-html/lfit2-1.png" alt="A fitted least squares regression line" width="75%" />
<p class="caption">
Figure 5.3: A fitted least squares regression line
</p>
</div>
<p>If you’ve seen regression before, you might wonder how this method compares to that method. Note that</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}=\begin{bmatrix}1 &amp; 1 &amp; \cdots &amp; 1\\x_1 &amp; x_2 &amp; \cdots &amp; x_n\end{bmatrix}\begin{bmatrix}1 &amp; x_1\\1 &amp; x_2\\\vdots &amp; \vdots\\1 &amp; x_n\end{bmatrix}=\begin{bmatrix} n &amp; \sum x_i\\ \sum x_i &amp; \sum x_i^2\end{bmatrix}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{y}=\begin{bmatrix}1 &amp; 1 &amp; \cdots &amp; 1\\x_1 &amp; x_2 &amp; \cdots &amp; x_n\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots \\y_n\end{bmatrix}=\begin{bmatrix} \sum y_i\\ \sum x_iy_i\end{bmatrix}.\]</span></p>
<p>All of these terms show up in the formulas for the coefficients that you may have seen. This method produces the same results.</p>
<div id="multilinear-regression" class="section level3 unnumbered hasAnchor">
<h3>Multilinear regression<a href="regression.html#multilinear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can even add independent variables to our least squares linear regression models, without changing the complexity of the mathematical theory.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:multilin" class="example"><strong>Example 5.3  </strong></span>Consider the following data in <span class="math inline">\((x,y,z)\)</span>-form.</p>
<p><span class="math display">\[(1,4,5),(2,3,1),(6,5,2),(2,1,1),(4,3,3)\]</span></p>
<p>Suppose we want to fit a linear model of the form <span class="math inline">\(z=b_0+b_1x+b_2y\)</span> to the data. The graph of this function is a plane, and if all of our sample points lie on that plane, then there will be a solution to the following system of equations.</p>
<p><span class="math display">\[\begin{equation*}
\left\{ \begin{array}{rcl}
b_0+1b_1+4b_2&amp;=5\\
b_0+2b_1+3b_2&amp;=1\\
b_0+6b_1+5b_2&amp;=2\\
b_0+2b_1+1b_2&amp;=1\\
b_0+4b_1+3b_2&amp;=3\\
\end{array}
\right.
\end{equation*}\]</span></p>
<p>We can write this in matrix form as</p>
<p><span class="math display">\[\begin{bmatrix} 1 &amp; 1 &amp; 4\\1 &amp; 2 &amp; 3\\1 &amp; 6 &amp; 5\\1 &amp; 2 &amp; 1\\1 &amp; 4 &amp; 3\end{bmatrix}\begin{bmatrix} b_0\\b_1\\b_2\end{bmatrix}=\begin{bmatrix}5\\1\\2\\1\\3\end{bmatrix}\]</span></p>
<p>or <span class="math inline">\(\mathbf{X}\mathbf{b}=\mathbf{z}.\)</span> Our matrix <span class="math inline">\(\mathbf{X}\)</span> has a first column of ones. The second column consists of the <span class="math inline">\(x\)</span> values, and the third consists of the <span class="math inline">\(y\)</span> values. The <span class="math inline">\(\mathbf{z}\)</span> vector contains the <span class="math inline">\(z\)</span> values. If the points don’t lie on a plane we can still find a least squares solution using the normal equations. The least squares solution will be unique if the columns of <span class="math inline">\(\mathbf{X}\)</span> are linearly independent. Dependence would only occur if <span class="math inline">\(y\)</span> were a linear function of <span class="math inline">\(x\)</span> or if <span class="math inline">\(x\)</span> were a linear function of <span class="math inline">\(y\)</span>. We get</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}=\begin{bmatrix}5 &amp; 15 &amp; 16\\15 &amp; 61 &amp; 54\\16 &amp; 54 &amp; 60\end{bmatrix}\]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{z}=\begin{bmatrix}12\\33\\43\end{bmatrix}.\]</span></p>
<p>Our least squares solution is</p>
<p><span class="math display">\[\hat{\mathbf{b}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{z}=\begin{bmatrix}301/262\\-135/262\\229/262\end{bmatrix}\approx\begin{bmatrix}1.149\\-0.515\\0.874\end{bmatrix}.\]</span></p>
<p>The equation is</p>
<p><span class="math display">\[z=1.149-0.515x+0.874y.\]</span></p>
</div>
</div>
</div>
</div>
<div id="CorSec" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Correlation<a href="regression.html#CorSec" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It would be nice to have a number to describe how “linear” the relationship between two variables is. We have a few of those. The first we will investigate is Pearson’s correlation coefficient.</p>
<p>Here is some motivation for the topic. In Figure <a href="regression.html#fig:posslope">5.4</a>, we have data where <span class="math inline">\(y\)</span> is a linear function of <span class="math inline">\(x\)</span> with positive slope. On the right, we have “centered” the data by subtracting the <span class="math inline">\(x\)</span> mean from the <span class="math inline">\(x\)</span> values and the <span class="math inline">\(y\)</span> mean from the <span class="math inline">\(y\)</span> values. Let’s call the centered values <span class="math inline">\(\hat{x}\)</span> and <span class="math inline">\(\hat{y}\)</span>. When we do this, we get points that are on a line with the same slope, and the intercept is 0.</p>
<div class="figure"><span style="display:block;" id="fig:posslope"></span>
<img src="_main_files/figure-html/posslope-1.png" alt="Centering data" width="50%" /><img src="_main_files/figure-html/posslope-2.png" alt="Centering data" width="50%" />
<p class="caption">
Figure 5.4: Centering data
</p>
</div>
<p>If we put the centered values in vectors, <span class="math inline">\(\hat{\mathbf{x}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span>, we have <span class="math inline">\(\hat{\mathbf{y}}=m\hat{\mathbf{x}},\)</span> where <span class="math inline">\(m\)</span> is the positive slope. We can’t see the <span class="math inline">\(\hat{\mathbf{x}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> vectors because they live in <span class="math inline">\(\mathbb{R}^5\)</span>, but the vectors are pointing in the same direction, so the angle between them, <span class="math inline">\(\theta,\)</span> is <span class="math inline">\(0^{\circ}.\)</span></p>
<div class="figure"><span style="display:block;" id="fig:negslope"></span>
<img src="_main_files/figure-html/negslope-1.png" alt="Negative slope" width="50%" /><img src="_main_files/figure-html/negslope-2.png" alt="Negative slope" width="50%" />
<p class="caption">
Figure 5.5: Negative slope
</p>
</div>
<p>In Figure <a href="regression.html#fig:negslope">5.5</a>, the points lie on a straight line with negative slope. When we center the values, the angle between the centered vectors is <span class="math inline">\(180^{\circ}.\)</span></p>
<p>Recall that the angle between two vectors satisfies Equation <a href="orthogonality.html#eq:lawofcosines">(4.1)</a></p>
<p><span class="math display">\[\cos\theta=\frac{\mathbf{u}\cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}.\]</span></p>
<p>For the points on the line with positive slope, we have <span class="math inline">\(\cos\theta=1,\)</span> and for the points on the line with negative slope, we have <span class="math inline">\(\cos\theta=-1.\)</span> If points are not on a straight line, the angle between the centered vectors will be between <span class="math inline">\(0^{\circ}\)</span> and <span class="math inline">\(180^{\circ},\)</span> and the cosine of that angle will be between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1.\)</span> We’re going to call this number <span class="math inline">\(r.\)</span></p>
<div id="some-notation-and-a-formula" class="section level3 unnumbered hasAnchor">
<h3>Some notation and a formula<a href="regression.html#some-notation-and-a-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If our points are <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n),\)</span> then the dot product of the centered vectors is</p>
<p><span class="math display">\[\hat{\mathbf{x}}\cdot\hat{\mathbf{y}}=(x_1-\bar{x})(y_1-\bar{y})+\cdots+(x_n-\bar{x})(y_n-\bar{y})=\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}).\]</span></p>
<p>The “sigma notation” means we are adding all terms of the form <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})\)</span> with subscripts <span class="math inline">\(i\)</span> running from 1 to <span class="math inline">\(n.\)</span></p>
<p>The magnitudes of <span class="math inline">\(\hat{\mathbf{x}}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\)</span> can be written as</p>
<p><span class="math display">\[\sqrt{(x_1-\bar{x})^2+\cdots+(x_n-\bar{x})^2}=\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\]</span></p>
<p>and</p>
<p><span class="math display">\[\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}.\]</span></p>
<p>This gives our formula for Pearson’s correlation coefficient.</p>
<p><span class="math display" id="eq:pearson">\[\begin{equation}
r=\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n(y_i-\bar{y})^2}}
\tag{5.1}
\end{equation}\]</span></p>
<p>We can get these terms using matrices as follows. Let</p>
<p><span class="math display">\[
\mathbf{B}=\begin{bmatrix}\hat{\mathbf{x}} &amp; \hat{\mathbf{y}}\end{bmatrix}=\begin{bmatrix} x_1-\bar{x} &amp; y_1-\bar{y}\\ \vdots &amp; \vdots \\ x_n-\bar{x} &amp; y_n-\bar{y}\end{bmatrix}.\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{align*}
\mathbf{B}^T \mathbf{B}&amp;=\begin{bmatrix} x_1-\bar{x} &amp; \cdots &amp; x_n-\bar{x}\\  x_1-\bar{y} &amp; \cdots &amp; y_n-\bar{y}\end{bmatrix} \begin{bmatrix} x_1-\bar{x} &amp; y_1-\bar{y}\\ \vdots &amp; \vdots \\ x_n-\bar{x} &amp; y_n-\bar{y}\end{bmatrix}\\
&amp;=\begin{bmatrix} \sum_{i=1}^n(x_i-\bar{x})^2 &amp; \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})\\ \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) &amp; \sum_{i=1}^n(y_i-\bar{y})^2\end{bmatrix}.
\end{align*}\]</span></p>
<p>Note: the matrix <span class="math inline">\(\mathbf{S}=\frac{1}{n-1}\mathbf{B}^T \mathbf{B}\)</span> is called the <em>covariance matrix</em> (or <em>sample covariance matrix</em>). The diagonal entries of <span class="math inline">\(\mathbf{S}\)</span> give the sample variance for the corresponding values, while the off-diagonal entries give the sample covariance, which is like an unscaled version of the correlation coefficient. We will see this matrix again when we study principal component analysis.</p>
<p>Before we do some calculations, here are some properties of <span class="math inline">\(r.\)</span></p>
<div class="propbox">
<p><strong>Properties of Pearson’s Correlation Coefficient</strong> <span class="math inline">\(r\)</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(-1\leq r\leq 1\)</span></p></li>
<li><p>If <span class="math inline">\(r=1,\)</span> there is perfect positive correlation: all of the <span class="math inline">\((x,y)\)</span> pairs lie on a straight line with positive slope.</p></li>
<li><p>If <span class="math inline">\(r=-1,\)</span> there is perfect negative correlation.</p></li>
<li><p>The closer <span class="math inline">\(r\)</span> is to <span class="math inline">\(1\)</span> or <span class="math inline">\(-1,\)</span> the stronger the correlation.</p></li>
<li><p>The closer <span class="math inline">\(r\)</span> is to <span class="math inline">\(0,\)</span> the weaker the <em>linear</em> correlation. There may be some other type of correlation. It is always a good idea to look at a scatterplot to see if there is some sort of non-linear trend in the data.</p></li>
</ol>
</div>
<p>Correlation computations are almost always performed using calculators or software packages, but sometimes it is illustrative to do calculations by hand. In that case, there are some formulas that make it a little easier to do hand calculations. Here they are</p>
<p><span class="math display" id="eq:Sxx">\[\begin{equation}
S_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2=\left(\sum_{i=1}^n x_i^2\right)-n\bar{x}^2
\tag{5.2}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:Syy">\[\begin{equation}
S_{yy}=\sum_{i=1}^n(y_i-\bar{y})^2=\left(\sum_{i=1}^n y_i^2\right)-n\bar{y}^2
\tag{5.3}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:Sxy">\[\begin{equation}
S_{xy}=\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})=\left(\sum_{i=1}^n x_iy_i\right)-n\bar{x}\bar{y}
\tag{5.4}
\end{equation}\]</span></p>
<p>These allow us to write the correlation coefficient as</p>
<p><span class="math display" id="eq:PCC">\[\begin{equation}
r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}.
\tag{5.5}
\end{equation}\]</span></p>
<p>The terms <span class="math inline">\(\sum_{i=1}^nx_i^2\)</span> and <span class="math inline">\(\sum_{i=1}^ny_i^2\)</span> mean we square the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values and add them up. The <span class="math inline">\(\sum_{i=1}^n x_iy_i\)</span> means we multiply each <span class="math inline">\((x_i,y_i)\)</span> pair and add them up. These terms show up frequently in statistical settings.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-95" class="example"><strong>Example 5.4  </strong></span>Here is a simple example with five nice pairs of points. The sums appear in bold at the bottom.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(y_i\)</span></th>
<th align="left"><span class="math inline">\(x_i^2\)</span></th>
<th align="left"><span class="math inline">\(y_i^2\)</span></th>
<th align="left"><span class="math inline">\(x_i y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">4</td>
<td align="left">2</td>
<td align="left">16</td>
<td align="left">4</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">-1</td>
<td align="left">3</td>
<td align="left">1</td>
<td align="left">9</td>
<td align="left">-3</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">2</td>
<td align="left">25</td>
<td align="left">4</td>
<td align="left">10</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">-3</td>
<td align="left">4</td>
<td align="left">9</td>
<td align="left">-6</td>
</tr>
<tr class="odd">
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left"><strong>10</strong></td>
<td align="left"><strong>5</strong></td>
<td align="left"><strong>46</strong></td>
<td align="left"><strong>27</strong></td>
<td align="left"><strong>9 </strong></td>
</tr>
</tbody>
</table>
<p>We have <span class="math inline">\(\bar{x}=10/5=2,\bar{y}=5/5=1,S_{xx}=46-5*2^2=26, S_{yy}=27-5*1^2=22,\)</span> and <span class="math inline">\(S_{xy}=9-5*2*1=-1,\)</span> so <span class="math inline">\(r=\dfrac{-1}{\sqrt{26*22}}=-0.042.\)</span> This is very weak negative linear correlation, as you can see from the scatterplot (Fig. <a href="regression.html#fig:weak">5.6</a>).</p>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:weak"></span>
<img src="_main_files/figure-html/weak-1.png" alt="Weak correlation" width="75%" />
<p class="caption">
Figure 5.6: Weak correlation
</p>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:requalneg1" class="example"><strong>Example 5.5  </strong></span></p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(y_i\)</span></th>
<th align="left"><span class="math inline">\(x_i^2\)</span></th>
<th align="left"><span class="math inline">\(y_i^2\)</span></th>
<th align="left"><span class="math inline">\(x_i y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">9</td>
<td align="left">1</td>
<td align="left">81</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">16</td>
<td align="left">9</td>
<td align="left">12</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">7</td>
<td align="left">4</td>
<td align="left">49</td>
<td align="left">14</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">1</td>
<td align="left">25</td>
<td align="left">1</td>
<td align="left">5</td>
</tr>
<tr class="odd">
<td align="left"><strong>12</strong></td>
<td align="left"><strong>20</strong></td>
<td align="left"><strong>46</strong></td>
<td align="left"><strong>140</strong></td>
<td align="left"><strong>40</strong></td>
</tr>
</tbody>
</table>
<p>With <span class="math inline">\(n=4\)</span> this time, <span class="math inline">\(\bar{x}=12/4=3\)</span> and <span class="math inline">\(\bar{y}=20/4=5.\)</span> That gives us <span class="math inline">\(S_{xx}=46-4\cdot3^2=10,S_{yy}=140-4\cdot 5^2=40,\)</span> and <span class="math inline">\(S_{xy}=40-4\cdot 3\cdot 5=-20.\)</span> So,</p>
<p><span class="math display">\[r=\frac{-20}{\sqrt{10\cdot40}}=-1.\]</span></p>
<p>Our points lie on a straight line with negative slope (Fig. <a href="regression.html#fig:ns">5.7</a>).</p>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ns"></span>
<img src="_main_files/figure-html/ns-1.png" alt="Perfect negative correlation" width="75%" />
<p class="caption">
Figure 5.7: Perfect negative correlation
</p>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-96" class="example"><strong>Example 5.6  </strong></span>Here is one last simple example.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(y_i\)</span></th>
<th align="left"><span class="math inline">\(x_i^2\)</span></th>
<th align="left"><span class="math inline">\(y_i^2\)</span></th>
<th align="left"><span class="math inline">\(x_i y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">-2</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">16</td>
<td align="left">-8</td>
</tr>
<tr class="even">
<td align="left">-1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">-1</td>
</tr>
<tr class="odd">
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">16</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left"><strong>0</strong></td>
<td align="left"><strong>10</strong></td>
<td align="left"><strong>10</strong></td>
<td align="left"><strong>34</strong></td>
<td align="left"><strong>0 </strong></td>
</tr>
</tbody>
</table>
<p>We have <span class="math inline">\(\bar{x}=0,\bar{y}=10/5=2,\)</span> and <span class="math inline">\(S_{xy}=0-5\cdot0\cdot2\)</span>, so <span class="math inline">\(r=0.\)</span> There is no linear correlation between the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. We should always look at the scatterplot, though. Figure <a href="regression.html#fig:nonlinear">5.8</a> tells us what we should have noticed from the table of values. The points all lie on the parabola <span class="math inline">\(y=x^2.\)</span></p>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nonlinear"></span>
<img src="_main_files/figure-html/nonlinear-1.png" alt="Always look at the scatterplot" width="75%" />
<p class="caption">
Figure 5.8: Always look at the scatterplot
</p>
</div>
<div style="page-break-after: always;"></div>
</div>
<div id="correlation-in-r" class="section level3 unnumbered hasAnchor">
<h3>Correlation in R<a href="regression.html#correlation-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>cor</code> function in R will calculate Pearson’s and other correlation coefficients. Pearson’s is the default.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="regression.html#cb84-1" tabindex="-1"></a><span class="fu">cor</span>(mtcars<span class="sc">$</span>wt,mtcars<span class="sc">$</span>mpg)</span></code></pre></div>
<pre><code>## [1] -0.8676594</code></pre>
<p>The <code>with</code> function makes it easier to work with data frames.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="regression.html#cb86-1" tabindex="-1"></a><span class="fu">with</span>(mtcars, <span class="fu">cor</span>(wt,mpg))</span></code></pre></div>
<pre><code>## [1] -0.8676594</code></pre>
<p>Figure <a href="regression.html#fig:with">5.9</a> shows the scatterplot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:with"></span>
<img src="_main_files/figure-html/with-1.png" alt="Miles per gallon and weight have a strong negative correlation" width="75%" />
<p class="caption">
Figure 5.9: Miles per gallon and weight have a strong negative correlation
</p>
</div>
<p>If you use the <code>cor</code> function on a numerical data frame, you get a table of pairwise correlations. Note that the correlation of a variable with itself is always 1.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="regression.html#cb88-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(mtcars), <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##        mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
## mpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55
## cyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53
## disp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39
## hp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75
## drat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09
## wt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43
## qsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66
## vs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57
## am    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06
## gear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27
## carb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00</code></pre>
<p>To isolate a single variable’s correlations, you can use code like the following.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="regression.html#cb90-1" tabindex="-1"></a><span class="fu">cor</span>(mtcars<span class="sc">$</span>mpg,mtcars)</span></code></pre></div>
</div>
</div>
<div id="formulas-for-least-squares-regression" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Formulas for Least Squares Regression<a href="regression.html#formulas-for-least-squares-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we are going to approach least squares regression from a slightly different perspective from that in Section <a href="regression.html#LSRSec">5.3</a>.</p>
<p>If we determine that two variables are linearly correlated, we would like to find an equation <span class="math inline">\(\hat{y}=\hat{b}_0+\hat{b}_1x\)</span> that best fits the data. (The value <span class="math inline">\(\hat{y}\)</span> is the predicted or average value of <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span> value.)</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:residuals"></span>
<img src="images/residuals.png" alt="Minimizing the residuals" width="70%" />
<p class="caption">
Figure 5.10: Minimizing the residuals
</p>
</div>
<p>The goal is to somehow minimize the <em>residuals</em> <span class="math inline">\(e_i=y_i-\hat{y}_i.\)</span> (See Fig. <a href="regression.html#fig:residuals">5.10</a>.) In any “best” fit, some points will be above the line, and some will be below the line. Because of this, some residuals will be positive, and some will be negative. The least-squares solution minimizes the sum of the <em>squares</em> of the residuals so there’s no cancellation.</p>
<div class="propbox">
<p><strong>The Least Squares Problem</strong></p>
<p>Find coefficients <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> that minimize</p>
<p><span class="math display">\[S=\sum_{i=1}^n (y_i-\hat{y}_i)^2,\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i=\hat{b}_0+\hat{b}_1 x_i.\)</span></p>
</div>
<p>Note: this is a multivariable calculus problem. For a short introduction to, or review of, the material, consult Appendix A. The sum we are trying to minimize</p>
<p><span class="math display">\[S=\sum_{i=1}^n (y_i-\hat{y}_i)^2=\sum_{i=1}^n(y_i-\hat{b}_0-\hat{b}_1x_i)^2\]</span></p>
<p>is a function of two variables, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>. We can minimize it by finding the critical point, i.e., the solution to</p>
<p><span class="math display">\[\frac{\partial S}{\partial \hat{b}_0}=0\text{ and }\frac{\partial S}{\partial \hat{b}_1}=0.\]</span></p>
<p>Remember that the derivative of a sum is the sum of the derivatives, and that holds for partial differentiation, too. The value <span class="math inline">\(S\)</span> is just a sum of a bunch of squared terms. With</p>
<p><span class="math display">\[S=\sum_{i=1}^n(y_i-\hat{b}_0-\hat{b}_1x_i)^2,\]</span></p>
<p><span class="math display" id="eq:b0partial">\[\begin{equation}
\frac{\partial S}{\partial \hat{b}_0}=\sum_{i=1}^n 2(y_i-\hat{b}_0-\hat{b}_1x_i)(-1)=0,
\tag{5.6}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:b1partial">\[\begin{equation}
\frac{\partial S}{\partial \hat{b}_1}=\sum_{i=1}^n 2(y_i-\hat{b}_0-\hat{b}_1x_i)(-x_i)=0.
\tag{5.7}
\end{equation}\]</span></p>
<p>We can divide by <span class="math inline">\(-2\)</span> and rearrange things. From <a href="regression.html#eq:b0partial">(5.6)</a>, We have</p>
<p><span class="math display">\[\sum_{i=1}^n (y_i-\hat{b}_0-\hat{b}_1x_i)=0\Rightarrow \sum_{i=1}^ny_i=\sum_{i=1}^n \hat{b}_0+\sum_{i=1}^n\hat{b}_1x_i\]</span></p>
<p>We can rewrite this as</p>
<p><span class="math display">\[n\bar{y}=n\hat{b}_0+\hat{b}_1n\bar{x}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:intercept">\[\begin{equation}
\boxed{\hat{b}_0=\bar{y}-\hat{b}_1\bar{x}.}
\tag{5.8}
\end{equation}\]</span></p>
<p>Now we need to find <span class="math inline">\(\hat{b}_1\)</span>.</p>
<p>Setting <span class="math inline">\(\partial S/\partial \hat{b}_1\)</span> <a href="regression.html#eq:b1partial">(5.7)</a> equal to zero and dividing by <span class="math inline">\(-2\)</span> gives us</p>
<p><span class="math display">\[\sum_{i=1}^n (y_i-\hat{b}_0-\hat{b}_1x_i)(x_i)=0\Rightarrow \sum_{i=1}^n x_iy_i=\sum_{i=1}^n \hat{b}_0x_i+\sum_{i=1}^n\hat{b}_1x_i^2,\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{b}_1\sum_{i=1}^n x_i^2=\left(\sum_{i=1}^nx_iy_i\right)-\hat{b}_0 n\bar{x}=\left(\sum_{i=1}^nx_iy_i\right)-(\bar{y}-\hat{b}_1\bar{x})n\bar{x}.\]</span></p>
<p>Collecting the <span class="math inline">\(\hat{b}_1\)</span> terms gives us</p>
<p><span class="math display">\[\hat{b}_1\left(\left(\sum_{i=1}^nx_i^2\right)-n\bar{x}^2\right)=\left(\sum_{i=1}^nx_iy_i\right)-n\bar{x}\bar{y}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:slope">\[\begin{equation}
\boxed{\hat{b}_1=\frac{\left(\sum_{i=1}^nx_iy_i\right)-n\bar{x}\bar{y}}{\left(\sum_{i=1}^nx_i^2\right)-n\bar{x}^2}=\frac{S_{xy}}{S_{xx}},}
\tag{5.9}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(S_{xy}\)</span> and <span class="math inline">\(S_{xx}\)</span> are defined in Equations <a href="regression.html#eq:Sxy">(5.4)</a> and <a href="regression.html#eq:Sxx">(5.2)</a>, respectively. Let’s summarize our results.</p>
<div class="propbox">
<p><strong>The Least Squares Equation</strong></p>
<p>The linear least-squares equation is <span class="math inline">\(\hat{y}=\hat{b}_0+\hat{b}_1 x,\)</span> where <span class="math inline">\(\hat{b}_1=\dfrac{S_{xy}}{S_{xx}},\)</span> and <span class="math inline">\(\hat{b}_0=\bar{y}-\hat{b}_1 \bar{x}.\)</span></p>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-97" class="example"><strong>Example 5.7  </strong></span>First, let’s check to make sure that our formula works for truly linear data. Here is the data from Example <a href="regression.html#exm:requalneg1">5.5</a>.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(y_i\)</span></th>
<th align="left"><span class="math inline">\(x_i^2\)</span></th>
<th align="left"><span class="math inline">\(y_i^2\)</span></th>
<th align="left"><span class="math inline">\(x_i y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">9</td>
<td align="left">1</td>
<td align="left">81</td>
<td align="left">9</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">3</td>
<td align="left">16</td>
<td align="left">9</td>
<td align="left">12</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">7</td>
<td align="left">4</td>
<td align="left">49</td>
<td align="left">14</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">1</td>
<td align="left">25</td>
<td align="left">1</td>
<td align="left">5</td>
</tr>
<tr class="odd">
<td align="left"><strong>12</strong></td>
<td align="left"><strong>20</strong></td>
<td align="left"><strong>46</strong></td>
<td align="left"><strong>140</strong></td>
<td align="left"><strong>40</strong></td>
</tr>
</tbody>
</table>
<p>Again, <span class="math inline">\(\bar{x}=3, \bar{y}=5, S_{xy}=-20,\)</span> and <span class="math inline">\(S_{xx}=10.\)</span> That gives us <span class="math inline">\(\hat{b}_1=-20/10=-2\)</span> and <span class="math inline">\(\hat{b}_0=5-(-2)(3)=11.\)</span> Our equation is <span class="math inline">\(\hat{y}=11-2x.\)</span> A quick check reveals this to be the correct line.</p>
</div>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:gravity" class="example"><strong>Example 5.8  </strong></span>The gravity points (GP)<span class="citation">[<a href="#ref-GP">16</a>]</span> and alcohol content by volume (ABV) for five different one-gallon batches of homebrewed India Pale Ales (IPA) appear below. With GP as the independent variable, find the least-squares regression equation.</p>
<table>
<thead>
<tr class="header">
<th align="left">GP (<span class="math inline">\(x_i\)</span>)</th>
<th align="left">ABV (<span class="math inline">\(y_i\)</span>)</th>
<th align="left"><span class="math inline">\(x_i^2\)</span></th>
<th align="left"><span class="math inline">\(x_i y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">68</td>
<td align="left">7.42</td>
<td align="left">4624</td>
<td align="left">504.56</td>
</tr>
<tr class="even">
<td align="left">64</td>
<td align="left">6.03</td>
<td align="left">4096</td>
<td align="left">385.92</td>
</tr>
<tr class="odd">
<td align="left">57</td>
<td align="left">5.58</td>
<td align="left">3249</td>
<td align="left">318.06</td>
</tr>
<tr class="even">
<td align="left">72</td>
<td align="left">7.22</td>
<td align="left">5184</td>
<td align="left">519.84</td>
</tr>
<tr class="odd">
<td align="left">54</td>
<td align="left">5.34</td>
<td align="left">2916</td>
<td align="left">288.36</td>
</tr>
<tr class="even">
<td align="left"><strong>315</strong></td>
<td align="left"><strong>31.59</strong></td>
<td align="left"><strong>20069</strong></td>
<td align="left"><strong>2016.74</strong></td>
</tr>
</tbody>
</table>
<p>We don’t need the <span class="math inline">\(y_i^2\)</span> column for the regression coefficients, so it’s not in the table. We get <span class="math inline">\(\bar{x}=63,\bar{y}=6.318,S_{xy}=2016.74-5*63*6.318=26.57,\)</span> and <span class="math inline">\(S_{xx}=20069-5*63^2=224.\)</span> Thus, <span class="math inline">\(\hat{b}_1=26.57/224=0.119,\)</span> and <span class="math inline">\(\hat{b}_0=6.318-(26.57/224)*63=-1.155.\)</span> Our equation is</p>
<p><span class="math display">\[\hat{y}=-1.155+0.119x\]</span></p>
<p>or</p>
<p><span class="math display">\[\text{ABV}=-1.155+0.119\text{GP}.\]</span></p>
<p>We can interpret both the slope and the intercept. The slope tells us the predicted change in the <span class="math inline">\(y\)</span> value for each unit change in <span class="math inline">\(x.\)</span> Here, the slope (<span class="math inline">\(\hat{b}_1\)</span>) is 0.119, so for each additional gravity point, the ABV is predicted to increase by <span class="math inline">\(0.119\)</span> percentage points.</p>
<p>The <span class="math inline">\(y\)</span>-intercept (<span class="math inline">\(\hat{b}_0\)</span>) tells us the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0.\)</span> In this context, the negative <span class="math inline">\(y\)</span>-intercept does not make sense. This formula will not work for low GP values. Generally speaking, these regression equations will not work well for observations outside the scope of the original data. (Here from 54 to 72 gravity points.)</p>
<p>We can use our regression equation to make predictions. The equation predicts a beer with 68 gravity points will have an ABV<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> of <span class="math inline">\(\hat{y}=-1.155+0.119*68=6.94\%.\)</span> The actual ABV of our 68-GP beer was 7.42%. The residual for this observation is <span class="math inline">\(e=y-\hat{y}=7.42-6.94=0.48.\)</span> The model underestimated the actual ABV.</p>
</div>
</div>
<p>Here is how we can find the coefficients in R. We use the <code>lm</code> function. The dependent variable (ABV) comes first. The <code>summary</code> function applied to the named regression output gives us a lot of information.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="regression.html#cb91-1" tabindex="-1"></a>GP <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">68</span>,<span class="dv">64</span>,<span class="dv">57</span>,<span class="dv">72</span>,<span class="dv">54</span>)</span>
<span id="cb91-2"><a href="regression.html#cb91-2" tabindex="-1"></a>ABV<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fl">7.42</span>,<span class="fl">6.03</span>,<span class="fl">5.58</span>,<span class="fl">7.22</span>,<span class="fl">5.34</span>)</span>
<span id="cb91-3"><a href="regression.html#cb91-3" tabindex="-1"></a>abvfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(ABV<span class="sc">~</span>GP)</span>
<span id="cb91-4"><a href="regression.html#cb91-4" tabindex="-1"></a><span class="fu">summary</span>(abvfit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ABV ~ GP)
## 
## Residuals:
##        1        2        3        4        5 
##  0.50892 -0.40662 -0.02630 -0.16554  0.08954 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -1.15481    1.65838  -0.696   0.5363  
## GP           0.11862    0.02618   4.531   0.0201 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3918 on 3 degrees of freedom
## Multiple R-squared:  0.8725, Adjusted R-squared:   0.83 
## F-statistic: 20.53 on 1 and 3 DF,  p-value: 0.02011</code></pre>
<p>The coefficients appear in the <code>Estimates</code> column from the <code>Coefficients</code> table. The <code>(Intercept)</code> value is, unsurprisingly, <span class="math inline">\(\hat{b}_0\)</span>, while the slope, <span class="math inline">\(\hat{b}_1\)</span>, appears next to <code>GP</code>. When we have additional independent variables, they will each get their own coefficient.</p>
<p>Note that the output also calculates the individual residuals. We will explain most of the additional information later.</p>
<p>It is easy to add a regression line to a scatterplot in base R if we’ve named the <code>lm</code> output. The <code>lwd = 2</code> argument results in a slightly thicker line (Fig. <a href="regression.html#fig:pabv">5.11</a>).</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="regression.html#cb93-1" tabindex="-1"></a><span class="fu">plot</span>(ABV<span class="sc">~</span>GP, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb93-2"><a href="regression.html#cb93-2" tabindex="-1"></a><span class="fu">abline</span>(abvfit, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pabv"></span>
<img src="_main_files/figure-html/pabv-1.png" alt="Percent ABV as a function of Gravity Points" width="75%" />
<p class="caption">
Figure 5.11: Percent ABV as a function of Gravity Points
</p>
</div>
<p>Here we apply the <code>lm</code> function on the <code>mtcars</code> data frame, with weight as the independent variable and fuel economy as the dependent variable.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="regression.html#cb94-1" tabindex="-1"></a>mtfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>wt, <span class="at">data =</span> mtcars)</span>
<span id="cb94-2"><a href="regression.html#cb94-2" tabindex="-1"></a><span class="fu">summary</span>(mtfit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>The formula is <span class="math inline">\(\text{mpg}=37.2851-5.3445\text{wt}.\)</span> The weight is given in 1000 pounds, so the formula says that for each additional 1000 pounds in weight, the predicted mpg <em>decreases</em> by 5.3445.</p>
<p>We can also get the scatterplot (Fig. <a href="regression.html#fig:mpgscatter">5.12</a>).</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="regression.html#cb96-1" tabindex="-1"></a><span class="fu">plot</span>(mpg<span class="sc">~</span>wt, <span class="at">data =</span> mtcars, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb96-2"><a href="regression.html#cb96-2" tabindex="-1"></a><span class="fu">abline</span>(mtfit, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mpgscatter"></span>
<img src="_main_files/figure-html/mpgscatter-1.png" alt="Miles per gallon as a function of weight in 1000 pounds" width="75%" />
<p class="caption">
Figure 5.12: Miles per gallon as a function of weight in 1000 pounds
</p>
</div>
<div id="the-coefficient-of-determination" class="section level3 unnumbered hasAnchor">
<h3>The coefficient of determination<a href="regression.html#the-coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The deviation of the <span class="math inline">\(y\)</span> values from the mean can be broken down into two pieces: the part explained by the model (<span class="math inline">\(\hat{y}_i-\bar{y}\)</span>), and the part that is not explained (<span class="math inline">\(y_i-\hat{y}_i=e_i\)</span>) (Fig. <a href="regression.html#fig:explained">5.13</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:explained"></span>
<img src="images/variation.png" alt="The parts of the variation in $y$" width="75%" />
<p class="caption">
Figure 5.13: The parts of the variation in <span class="math inline">\(y\)</span>
</p>
</div>
<p>For each observation <span class="math inline">\((x_i,y_i),\)</span></p>
<p><span class="math display">\[y_i-\bar{y}=(y_i-\hat{y}_i)+(\hat{y}_i-\bar{y}).\]</span></p>
<p>What’s not obvious, but is true, is that if we square each term and add them up, we still have equality.</p>
<p><span class="math display" id="eq:AnovaIdentity">\[\begin{equation}
\sum_{i=1}^n (y_i-\bar{y})^2=\sum_{i=1}^n(y_i-\hat{y}_i)^2+\sum_{i=1}^n (\hat{y}_i-\bar{y})^2.
\tag{5.10}
\end{equation}\]</span></p>
<p>This can be written as <span class="math inline">\(SST=SSE+SSR,\)</span> where <span class="math inline">\(SST\)</span> is the total sum of squares, <span class="math inline">\(SSE\)</span> is the sum of squares of the error (the residuals), and <span class="math inline">\(SSR\)</span> is the sum of squares from the regression. Rearranging the equation gives us</p>
<p><span class="math display">\[SSR=SST-SSE,\]</span></p>
<p>and dividing both sides by <span class="math inline">\(SST\)</span> gives us</p>
<p><span class="math display">\[R^2\overset{\mathrm{def}}{=}\frac{SSR}{SST}=1-\frac{SSE}{SST}.\]</span></p>
<p>The <em>coefficient of determination</em>, <span class="math inline">\(R^2,\)</span> is the fraction of the total squared variation that comes from the regression. (I.e. the fraction that is explained.)</p>
<p>We don’t need to mess around with <span class="math inline">\(SST\)</span> and the others because we have the following formula for <span class="math inline">\(R^2.\)</span></p>
<p><span class="math display">\[R^2=r^2\]</span></p>
<p>In simple linear regression , the coefficient of determination is simply Pearson’s correlation coefficient squared.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-98" class="example"><strong>Example 5.9  </strong></span>We saw in Section <a href="regression.html#CorSec">5.4</a> that the correlation between <code>mpg</code> and <code>wt</code> from the <code>mtcars</code> data frame is -0.8676594. That means that <span class="math inline">\(r^2=0.753\)</span> or around 75% of the variance in mpg can be explained by the regression equation involving weight.</p>
</div>
</div>
<p>The coefficient of determination shows up in the R summary as <code>Multiple R-Squared</code>. (We will see <code>Adjusted R-Squared</code> in the future.)</p>
<pre><code>## 
## Call:
## lm(formula = ABV ~ GP)
## 
## Residuals:
##        1        2        3        4        5 
##  0.50892 -0.40662 -0.02630 -0.16554  0.08954 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -1.15481    1.65838  -0.696   0.5363  
## GP           0.11862    0.02618   4.531   0.0201 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3918 on 3 degrees of freedom
## Multiple R-squared:  0.8725, Adjusted R-squared:   0.83 
## F-statistic: 20.53 on 1 and 3 DF,  p-value: 0.02011</code></pre>
<p>In the GP/ABV example, the coefficient of determination is 0.8725, so around 87.25% of the variance in ABV is explained by the equation.</p>
</div>
</div>
<div id="uncertainty-in-least-squares" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Uncertainty in Least Squares<a href="regression.html#uncertainty-in-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In both the matrix approach and the calculus approach to least squares, we are minimizing the same thing: the sum of squares of the residuals.</p>
<p><span class="math display">\[\begin{equation*}
\sum_{i=1}^n(y_i-\hat{y}_i)^2=\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2
\end{equation*}\]</span></p>
<p>A more statistical/probabilistic approach involves what is called a <em>linear model</em>. In this linear model, the underlying assumption is that the relationship between the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values is indeed linear, and any departure from linearity is due to randomness. The model is, for <span class="math inline">\(i=1\dots n\)</span>,</p>
<p><span class="math display">\[y_i=b_0+b_1x_i+\varepsilon_i,\]</span></p>
<p>where <span class="math inline">\((x_i,y_i)\)</span> is the <span class="math inline">\(i\)</span>th pair, and <span class="math inline">\(\varepsilon_i\)</span> is the error term.</p>
<p>In order to estimate <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> using the observed <span class="math inline">\((x_i,y_i)\)</span> pairs, we make some assumptions on the error terms <span class="citation">[<a href="#ref-Navidi">1</a>]</span>.</p>
<div class="propbox">
<p><strong>Assumptions on the error terms</strong></p>
<p>In the simplest situation, the following assumptions are satisfied.</p>
<ol style="list-style-type: decimal">
<li><p>The errors <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are random and independent. In particular, the magnitude of any error <span class="math inline">\(\varepsilon_i\)</span> does not influence the value of the next error.</p></li>
<li><p>The errors <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> all have mean 0.</p></li>
<li><p>The errors <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> all have the same variance, which we denote by <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p>The errors <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> are normally distributed.</p></li>
</ol>
</div>
<p>With the assumptions in place, we can find what are called <em>maximum likelihood estimators</em>, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>, of the true intercept and slope, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, respectively, based on the observed data. It turns out that the normality assumption on the errors leads to the same estimates <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> as the linear algebra and calculus approaches!</p>
<p>One benefit of the linear model approach is that it allows us to estimate the uncertainty in the coefficients and in predictions based on the estimates. Let’s revisit the output from our attempt to write <code>mpg</code> as a function of <code>wt</code> using the <code>mtcars</code> data frame.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="regression.html#cb98-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>wt, <span class="at">data =</span> mtcars)</span>
<span id="cb98-2"><a href="regression.html#cb98-2" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 37.285126   1.877627 19.857575 8.241799e-19
## wt          -5.344472   0.559101 -9.559044 1.293959e-10</code></pre>
<p>The <em>standard error</em> (<code>Std. Error</code>) of the coefficients is related to how much uncertainty there is in them. Consult a statistics textbook <span class="citation">[<a href="#ref-Navidi">1</a>,<a href="#ref-OA">9</a>]</span> for a detailed explanation of how to calculate them.</p>
<p>When you divide the coefficient estimates by their standard errors, the resulting value follows what is known as <em>Student’s t distribution</em> with <span class="math inline">\(n-2\)</span> degrees of freedom, where <span class="math inline">\(n\)</span> is the number of pairs in the data. In the <code>mtcars</code> data frame, there are 32 cars, so there are 30 degrees of freedom. The value in the <code>t value</code> column is this <span class="math inline">\(t\)</span>-distribution number.</p>
<p>One question to ask is, “Are the slope and intercept significantly different from 0?” The values in the <code>Pr(&gt;|t|)</code> column tell us the probability of seeing values at least as extreme as they are if they were truly zero. These probabilities are called <em>P-values</em>. In both cases, it is extremely unlikely that they are zero. If the slope were not significantly different from 0, then the independent variable wouldn’t have any significant effect on the dependent variable. We commonly consider <span class="math inline">\(P\)</span>-values below 0.05 to be significant.</p>
<p>You can also use the standard error values to find confidence interval estimates for the true values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. The R function <code>confint</code> applied to the <code>lm</code> output will produce these intervals.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="regression.html#cb100-1" tabindex="-1"></a><span class="fu">confint</span>(fit, <span class="at">level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 33.450500 41.119753
## wt          -6.486308 -4.202635</code></pre>
<p>The numbers in the <code>2.5 %</code> column are the lower bounds for 95% confidence intervals for the intercept and slope (<code>wt</code>). The numbers in the <code>97.5 %</code> column are the upper bounds. Note that 95% (<code>level = 0.95</code>) is the default setting and doesn’t need to be there if you are happy with that level.</p>
<div id="confidence-and-prediction-intervals-for-responses" class="section level3 unnumbered hasAnchor">
<h3>Confidence and prediction intervals for responses<a href="regression.html#confidence-and-prediction-intervals-for-responses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the linear model approach, the value</p>
<p><span class="math display">\[\hat{y}=\hat{b}_0+\hat{b}_1x\]</span></p>
<p>can be thought of as the <em>estimated mean output for a given input</em> <span class="math inline">\(x\)</span>. We can construct a confidence interval for the true mean response.</p>
<p>We can also produce what is called a <em>prediction interval</em>: given an input <span class="math inline">\(x\)</span>, what is the likely range of <span class="math inline">\(y\)</span> values? The <code>predict</code> function applied to the output of the <code>lm</code> function can calculate both of these intervals. To use it, we need to create a data frame with new values. (The <code>interval</code> setting will accept abbreviations of “confidence” and “prediction”.)</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="regression.html#cb102-1" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">wt =</span> <span class="dv">2</span>), <span class="at">interval =</span> <span class="st">&quot;conf&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 26.59618 24.82389 28.36848</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="regression.html#cb104-1" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">wt =</span> <span class="dv">2</span>), <span class="at">interval =</span> <span class="st">&quot;predict&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 26.59618 20.12811 33.06425</code></pre>
<p>These are 95% confidence and prediction intervals, respectively, for a 2000-lb vehicle (<code>wt = 2</code>). The prediction interval is much wider, since it’s giving a range of all likely values and not just the mean. The value under <code>fit</code> is what you get when you plug <code>wt = 2</code> into the regression formula.</p>
<p>It is fairly common to see 95% confidence intervals for the mean response included in regression plots. Here is how to get it using the <code>ggplot2</code> package (<a href="regression.html#fig:gglm">5.14</a>).</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="regression.html#cb106-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb106-2"><a href="regression.html#cb106-2" tabindex="-1"></a>mtplot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(mtcars, <span class="fu">aes</span>(wt,mpg)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span>  </span>
<span id="cb106-3"><a href="regression.html#cb106-3" tabindex="-1"></a>          <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&#39;lm&#39;</span>)</span>
<span id="cb106-4"><a href="regression.html#cb106-4" tabindex="-1"></a>mtplot</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gglm"></span>
<img src="_main_files/figure-html/gglm-1.png" alt="A regression plot with confidence interval" width="75%" />
<p class="caption">
Figure 5.14: A regression plot with confidence interval
</p>
</div>
<p>It takes a little more work to include a prediction interval. Figure <a href="regression.html#fig:beerpi">5.15</a> shows the 95% confidence interval (gray shaded region) and 95% prediction interval (dashed) from the GP/ABV data in Example <a href="regression.html#exm:gravity">5.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:beerpi"></span>
<img src="images/beerpi.png" alt="Confidence and prediction intervals" width="75%" />
<p class="caption">
Figure 5.15: Confidence and prediction intervals
</p>
</div>
</div>
<div id="checking-assumptions" class="section level3 unnumbered hasAnchor">
<h3>Checking assumptions<a href="regression.html#checking-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <em>residual plot</em> is a plot of residuals in a linear fit versus either the <span class="math inline">\(x\)</span> values or the predicted <span class="math inline">\(y\)</span> values. It can be used to see if the equal variance assumption is met. It can also display any nonlinear trend in the data.</p>
<p>In Figure <a href="regression.html#fig:resnl">5.16</a>, the linear fit on the left looks pretty good, but the residual plot on the right reveals a distinct nonlinear trend.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resnl"></span>
<img src="images/rp1.png" alt="A nonlinear trend" width="40%" /><img src="images/rp1a.png" alt="A nonlinear trend" width="40%" />
<p class="caption">
Figure 5.16: A nonlinear trend
</p>
</div>
<p>In Figure <a href="regression.html#fig:uneqvar">5.17</a>, the residual plot on the right shows that the residuals tend to spread out as <span class="math inline">\(y\)</span> increases. This indicates that the equal variance assumption is not true.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uneqvar"></span>
<img src="images/rp2.png" alt="Increasing residuals" width="40%" /><img src="images/rp2a.png" alt="Increasing residuals" width="40%" />
<p class="caption">
Figure 5.17: Increasing residuals
</p>
</div>
<p>In Figure <a href="regression.html#fig:assumptionsmet">5.18</a>, there is no obvious nonlinear trend and they seem to be fairly uniform across the board.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:assumptionsmet"></span>
<img src="images/rp3.png" alt="A good looking residual plot" width="40%" /><img src="images/rp3a.png" alt="A good looking residual plot" width="40%" />
<p class="caption">
Figure 5.18: A good looking residual plot
</p>
</div>
<p>If you apply the <code>plot</code> function to the output from <code>lm</code>, you get a sequence of diagnostic plots. The first is the residual plot.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="regression.html#cb107-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb107-2"><a href="regression.html#cb107-2" tabindex="-1"></a><span class="fu">plot</span>(fit)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:diag"></span>
<img src="_main_files/figure-html/diag-1.png" alt="Diagnostic plots" width="672" />
<p class="caption">
Figure 5.19: Diagnostic plots
</p>
</div>
<p>See <a href="https://www.statology.org/diagnostic-plots-in-r/">www.statology.org</a> <span class="citation">[<a href="#ref-statology">17</a>]</span> for a detailed discussion of all four plots.</p>
</div>
</div>
<div id="multilinear-regression-1" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Multilinear Regression<a href="regression.html#multilinear-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the real world, many quantities depend on more than one independent variable. For instance, the fuel efficiency of a car might depend on its weight and on its horsepower. If the relationship is linear, we can use multilinear regression to investigate it.</p>
<p>We saw a short example in Section <a href="regression.html#LSRSec">5.3</a>. Now we will develop the idea in a little more depth. Suppose that a quantity <span class="math inline">\(z\)</span> is a linear function of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, i.e.</p>
<p><span class="math display">\[z=b_0+b_1x+b_2y.\]</span></p>
<p>Further suppose that we have a bunch of <span class="math inline">\((x,y,z)\)</span> data:</p>
<p><span class="math display">\[(x_1,y_1,z_1),(x_2,y_2,z_2),\dots,(x_n,y_n,z_n).\]</span></p>
<p>We’d like to use this data to find the coefficients that best fit the data. We already have the tools to do this in the least squares sense. Our data gives us <span class="math inline">\(n\)</span> equations</p>
<p><span class="math display">\[\begin{align*}
    b_0+b_1x_1+b_2y_1&amp;=z_1\\
    b_0+b_1x_2+b_2y_2&amp;=z_2\\
    &amp;\vdots\\
    b_0+b_1x_n+b_2y_n&amp;=z_n.
\end{align*}\]</span></p>
<p>We can write this system in matrix form</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; x_1 &amp; y_1\\1 &amp; x_2 &amp; y_2\\ \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_n &amp; y_n\end{bmatrix}\begin{bmatrix}b_0\\b_1\\b_2\end{bmatrix}=\begin{bmatrix}z_1\\z_2\\ \vdots \\ z_n\end{bmatrix}\]</span></p>
<p>or</p>
<p><span class="math display">\[\mathbf{X}\mathbf{b}=\mathbf{z}.\]</span></p>
<p>Usually the system won’t have a solution, but we can find a least squares approximation using the normal equations:</p>
<p><span class="math display">\[\mathbf{X}^T\mathbf{X}\mathbf{b}=\mathbf{X}^T\mathbf{z},\]</span></p>
<p>which will have a solution <span class="math inline">\(\hat{\mathbf{b}}\)</span>. It will be unique if the columns of <span class="math inline">\(\mathbf{X}\)</span> are independent.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-99" class="example"><strong>Example 5.10  </strong></span>Let’s find the least squares equation <span class="math inline">\(\hat{z}=\hat{b}_0+\hat{b}_1x+\hat{b}_2 y\)</span> that best fits the points in <span class="math inline">\((x,y,z)\)</span> form: <span class="math inline">\((1,1,1),(2,0,2),(-1,-1,1),\)</span> and <span class="math inline">\((1,1,3).\)</span> The matrix equation is</p>
<p><span class="math display">\[\begin{bmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 2 &amp; 0 \\
1 &amp; -1 &amp; -1 \\
1 &amp; 1 &amp; 1
\end{bmatrix}\mathbf{b}=
\begin{bmatrix}
1\\2\\1\\3
\end{bmatrix}.\]</span></p>
<p>The first column of <span class="math inline">\(\mathbf{X}\)</span> is all 1’s, the second column is the <span class="math inline">\(x\)</span> values, and the third is the <span class="math inline">\(y\)</span> values. The normal equation(s) are</p>
<p><span class="math display">\[\begin{bmatrix}
4 &amp; 3 &amp; 1\\3 &amp; 7 &amp; 3\\1 &amp; 3 &amp; 3
\end{bmatrix}\mathbf{b}=\begin{bmatrix}
7\\7\\3
\end{bmatrix}
.\]</span></p>
<p>The least squares solution is <span class="math inline">\(\hat{\mathbf{b}}=(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{z})=\begin{bmatrix}
3/2\\1/4\\1/4
\end{bmatrix}\)</span>, so the equation is</p>
<p><span class="math display">\[\hat{z}=\frac{3}{2}+\frac{1}{4}x+\frac{1}{4}y.\]</span></p>
<p>The beauty of the linear algebra approach is that we can use as many independent variables as we want and the theory stays the same. The matrices involved get bigger, and it’s harder to do by hand, but that’s what computers are for.</p>
</div>
</div>
<div id="r-and-multilinear-regression" class="section level3 unnumbered hasAnchor">
<h3>R and multilinear regression<a href="regression.html#r-and-multilinear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-100" class="example"><strong>Example 5.11  </strong></span>Let’s use R to find a model for <code>mpg</code> as a linear function of <code>wt</code> and <code>hp</code> from the <code>mtcars</code> data frame. We again use the <code>lm</code> function, with a plus sign separating the independent variables.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="regression.html#cb108-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>wt<span class="sc">+</span>hp, <span class="at">data =</span> mtcars)</span>
<span id="cb108-2"><a href="regression.html#cb108-2" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + hp, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.941 -1.600 -0.182  1.050  5.854 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***
## wt          -3.87783    0.63273  -6.129 1.12e-06 ***
## hp          -0.03177    0.00903  -3.519  0.00145 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.593 on 29 degrees of freedom
## Multiple R-squared:  0.8268, Adjusted R-squared:  0.8148 
## F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12</code></pre>
<p>The coefficients appear in the Estimate column. Our model is</p>
<p><span class="math display">\[\texttt{mpg}=37.227-3.878\texttt{wt}-0.032\texttt{hp}.\]</span></p>
<p>We can use the model to estimate the <code>mpg</code> for various <code>wt</code>-<code>hp</code> pairs. For instance, a 2500-lb car with a 175-hp engine would on average get</p>
<p><span class="math display">\[\texttt{mpg}=37.227-3.878\cdot 2.5-0.032\cdot 175=21.932 \text{ miles per gallon}.\]</span></p>
<p>We can also use the predict function.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="regression.html#cb110-1" tabindex="-1"></a><span class="fu">predict</span>(fit2, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">wt =</span> <span class="fl">2.5</span>, <span class="at">hp =</span> <span class="dv">175</span>))</span></code></pre></div>
<pre><code>##        1 
## 21.97243</code></pre>
<p>The discrepancies between the two answers are due to the rounding of the coefficients. What about the rest of the output? The intercept, the <code>wt</code> coefficient, and the <code>hp</code> coefficient are all significantly different from 0. Note that we lose an additional degree of freedom when we add a variable.</p>
<p>The model from last section, that only had weight as a predictor, had a coefficient of determination of 0.7528. Now it’s 0.8268. <em>It will always increase when additional variables are added.</em> The adjusted <span class="math inline">\(R^2\)</span> of 0.8148 is the appropriate value for comparing multiple regression models. (We’ll talk about Adjusted <span class="math inline">\(R^2\)</span> in the next section.)</p>
<p>In simple linear regression, the <span class="math inline">\(P\)</span>-value associated with the <span class="math inline">\(F\)</span> statistic is the same as the <span class="math inline">\(P\)</span>-value for the slope. That changes in multilinear regression. We’re not going to get into the details of the <span class="math inline">\(F\)</span> statistic here, but what we’re looking for is a <span class="math inline">\(P\)</span>-value of less than <span class="math inline">\(0.05\)</span>. Otherwise, there’s basically no linear relationship.</p>
</div>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-101" class="example"><strong>Example 5.12  </strong></span>We can add all of the other variables into the mix by using a period after the tilde.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="regression.html#cb112-1" tabindex="-1"></a>fitall <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>., <span class="at">data =</span> mtcars)</span>
<span id="cb112-2"><a href="regression.html#cb112-2" tabindex="-1"></a><span class="fu">summary</span>(fitall)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ ., data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4506 -1.6044 -0.1196  1.2193  4.6271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 12.30337   18.71788   0.657   0.5181  
## cyl         -0.11144    1.04502  -0.107   0.9161  
## disp         0.01334    0.01786   0.747   0.4635  
## hp          -0.02148    0.02177  -0.987   0.3350  
## drat         0.78711    1.63537   0.481   0.6353  
## wt          -3.71530    1.89441  -1.961   0.0633 .
## qsec         0.82104    0.73084   1.123   0.2739  
## vs           0.31776    2.10451   0.151   0.8814  
## am           2.52023    2.05665   1.225   0.2340  
## gear         0.65541    1.49326   0.439   0.6652  
## carb        -0.19942    0.82875  -0.241   0.8122  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.65 on 21 degrees of freedom
## Multiple R-squared:  0.869,  Adjusted R-squared:  0.8066 
## F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07</code></pre>
<p>But maybe we shouldn’t. Only the <code>wt</code> coefficient is significantly different from 0. We’ll talk about model selection next section.</p>
</div>
</div>
</div>
<div id="indicator-variables" class="section level3 unnumbered hasAnchor">
<h3>Indicator variables<a href="regression.html#indicator-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can include categorical (non-numeric) variables in linear regression through the use of <em>indicator variables</em>. An indicator variable takes on the value 1 if the categorical variable takes on a specific value and a 0 otherwise.</p>
<p>For instance, in the <code>mtcars</code> data frame, the <code>am</code> (1 for manual, 0 for automatic transmission) and <code>vs</code> (0 for V-shaped engine, 1 for straight) variables are actually indicator variables.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-102" class="example"><strong>Example 5.13  </strong></span>If we find a linear model for fuel efficiency as a function of horsepower and transmission type, we get the following.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="regression.html#cb114-1" tabindex="-1"></a>fitind <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>hp<span class="sc">+</span>am, <span class="at">data =</span> mtcars)</span>
<span id="cb114-2"><a href="regression.html#cb114-2" tabindex="-1"></a><span class="fu">summary</span>(fitind)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ hp + am, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3843 -2.2642  0.1366  1.6968  5.8657 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 26.584914   1.425094  18.655  &lt; 2e-16 ***
## hp          -0.058888   0.007857  -7.495 2.92e-08 ***
## am           5.277085   1.079541   4.888 3.46e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.909 on 29 degrees of freedom
## Multiple R-squared:  0.782,  Adjusted R-squared:  0.767 
## F-statistic: 52.02 on 2 and 29 DF,  p-value: 2.55e-10</code></pre>
<p>The model is</p>
<p><span class="math display">\[\texttt{mpg}=26.585-0.059\texttt{hp}+5.277\texttt{am}.\]</span></p>
<p>Basically, the presence of a manual transmission adds 5.277 miles per gallon to the estimated fuel efficiency. We get two regression lines: one for automatics and one for manuals (Fig. <a href="regression.html#fig:indicator">5.20</a>).</p>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:indicator"></span>
<img src="images/multiint.png" alt="A model with an indicator term" width="75%" />
<p class="caption">
Figure 5.20: A model with an indicator term
</p>
</div>
<p>Many categorical variables take on more than two different values or <em>levels</em>. In that case, a different indicator variable is included for each level, <em>except one</em>. For instance, if the season is an important variable, there could be one indicator variable for Spring (1 if Spring, 0 otherwise), one for Summer, and one for Autumn. The final season, Winter doesn’t get one, because that would make the variables dependent. Winter would be the baseline value, to which all others are compared.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-103" class="example"><strong>Example 5.14  </strong></span>Here is a fit of the population of a state as a function of the state’s area and its region. <code>Region</code> is a categorical variable. The Northeast region is treated as the baseline. The other regions get their own indicators.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="regression.html#cb116-1" tabindex="-1"></a>state <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(state.x77)</span>
<span id="cb116-2"><a href="regression.html#cb116-2" tabindex="-1"></a>state<span class="sc">$</span>Region <span class="ot">&lt;-</span> state.region</span>
<span id="cb116-3"><a href="regression.html#cb116-3" tabindex="-1"></a>fitstate <span class="ot">&lt;-</span> <span class="fu">lm</span>(Population<span class="sc">~</span>Area<span class="sc">+</span>Region, <span class="at">data =</span> state)</span>
<span id="cb116-4"><a href="regression.html#cb116-4" tabindex="-1"></a><span class="fu">summary</span>(fitstate)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Population ~ Area + Region, data = state)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5952.2 -2226.6  -858.2   713.2 18110.2 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          5.352e+03  1.514e+03   3.534 0.000959 ***
## Area                 7.875e-03  8.631e-03   0.912 0.366383    
## RegionSouth         -1.574e+03  1.909e+03  -0.825 0.413931    
## RegionNorth Central -1.043e+03  2.029e+03  -0.514 0.609901    
## RegionWest          -3.496e+03  2.202e+03  -1.588 0.119327    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4519 on 45 degrees of freedom
## Multiple R-squared:  0.05919,    Adjusted R-squared:  -0.02444 
## F-statistic: 0.7078 on 4 and 45 DF,  p-value: 0.5908</code></pre>
<p>Note how R pastes the level of the variable onto the variable name in the output. Also note the terrible coefficient of determination. This is <em>not</em> a good model.</p>
</div>
</div>
</div>
<div id="fitting-polynomials" class="section level3 unnumbered hasAnchor">
<h3>Fitting polynomials<a href="regression.html#fitting-polynomials" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can actually use linear regression to fit polynomial functions to data. For instance, what if we want to find the best fitting quadratic function</p>
<p><span class="math display">\[y=b_0+b_1x+b_2x^2\]</span></p>
<p>for a set of data? You may recall that three points (with different <span class="math inline">\(x\)</span> values) will determine a quadratic. What if we have more than 3 points?</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-104" class="example"><strong>Example 5.15  </strong></span>Let’s find the least squares best fitting quadratic that goes through the points <span class="math inline">\((1,3),(2,5),(3,6),\)</span> and <span class="math inline">\((4,7)\)</span>.
The linear system of equations for <span class="math inline">\(y=b_0+b_1x+b_2x^2\)</span> with this data would be</p>
<p><span class="math display">\[\begin{align*}
    b_0+1b_1+1^2b_2&amp;=3\\
    b_0+2b_1+2^2b_2&amp;=5\\
    b_0+3b_1+3^2b_2&amp;=6\\
    b_0+4b_1+4^2b_2&amp;=7
\end{align*}\]</span></p>
<p>or in matrix form</p>
<p><span class="math display">\[\begin{bmatrix}1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 4\\1 &amp; 3 &amp; 9\\1 &amp; 4 &amp; 16\end{bmatrix}\begin{bmatrix}b_0\\b_1\\b_2\end{bmatrix}=\begin{bmatrix}3\\5\\6\\7\end{bmatrix}.\]</span></p>
<p>Solving the normal equations gives us</p>
<p><span class="math display">\[\hat{y}=\frac{3}{4}+\frac{51}{20}x-\frac{1}{4}x^2.\]</span></p>
<p>To do this in R, we need to be a little careful.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="regression.html#cb118-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb118-2"><a href="regression.html#cb118-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>)</span>
<span id="cb118-3"><a href="regression.html#cb118-3" tabindex="-1"></a>fitq <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb118-4"><a href="regression.html#cb118-4" tabindex="-1"></a><span class="fu">summary</span>(fitq)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + I(x^2))
## 
## Residuals:
##     1     2     3     4 
## -0.05  0.15 -0.15  0.05 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   0.7500     0.6225   1.205    0.441
## x             2.5500     0.5679   4.490    0.140
## I(x^2)       -0.2500     0.1118  -2.236    0.268
## 
## Residual standard error: 0.2236 on 1 degrees of freedom
## Multiple R-squared:  0.9943, Adjusted R-squared:  0.9829 
## F-statistic:    87 on 2 and 1 DF,  p-value: 0.07559</code></pre>
<p>To use powers of a variable in a model in R, we need to wrap them in the <code>I</code> function. See the fit in Figure <a href="regression.html#fig:qfit">5.21</a>.</p>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qfit"></span>
<img src="_main_files/figure-html/qfit-1.png" alt="A quadratic fit using linear regression" width="75%" />
<p class="caption">
Figure 5.21: A quadratic fit using linear regression
</p>
</div>
<p>In the last example, our design matrix looked like</p>
<p><span class="math display">\[\mathbf{X}=\begin{bmatrix} 1&amp; x_1 &amp; x_1^2 \\1 &amp; x_2 &amp; x_2^2\\ \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_n &amp; x_n^2\end{bmatrix}.\]</span></p>
<p>We actually had functions of <span class="math inline">\(x\)</span> in the columns. We can do this for any function. For instance, if we wanted a fit of the form</p>
<p><span class="math display">\[y=b_0 +b_1\sin x+b_2\cos x,\]</span></p>
<p>the design matrix would look like</p>
<p><span class="math display">\[\mathbf{X}=\begin{bmatrix}1 &amp; \sin x_1 &amp; \cos x_1\\1 &amp; \sin x_2 &amp; \cos x_2 \\ \vdots &amp; \vdots &amp; \vdots \\
1 &amp; \sin x_n &amp; \cos x_n\end{bmatrix}.\]</span></p>
</div>
<div id="interactions" class="section level3 unnumbered hasAnchor">
<h3>Interactions<a href="regression.html#interactions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s say you want to model an individual’s response to a medication. The response is going to depend on a number of things, including the dosage and the person’s weight. In our previous models, the response to a change in one variable wasn’t dependent on the value of the other. It’s not hard to imagine, however, that the response to a change in dosage will depend on the person’s weight. There is an <em>interaction</em> between these two variables.</p>
<p>The simplest way to include an interaction between variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is to include the product <span class="math inline">\(x_1 x_2\)</span> as a separate independent variable. We then have the model</p>
<p><span class="math display">\[y=b_0+b_1 x_1+b_2 x_2+b_3 x_1 x_2.\]</span></p>
<p>Adding this new term doesn’t really complicate things at all.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-105" class="example"><strong>Example 5.16  </strong></span>The price of a diamond in U.S. Dollars is a function of many things, including its weight in carats and its depth percentage. The <code>ggplot2</code> package contains a data set with this information.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="regression.html#cb120-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb120-2"><a href="regression.html#cb120-2" tabindex="-1"></a>diamondfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(price<span class="sc">~</span>carat<span class="sc">*</span>depth, <span class="at">data =</span> diamonds)</span>
<span id="cb120-3"><a href="regression.html#cb120-3" tabindex="-1"></a><span class="fu">summary</span>(diamondfit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = price ~ carat * depth, data = diamonds)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15039.2   -799.3    -18.1    539.6  12666.5 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -7823.738    592.049 -13.215   &lt;2e-16 ***
## carat       20742.600    567.672  36.540   &lt;2e-16 ***
## depth          90.043      9.588   9.391   &lt;2e-16 ***
## carat:depth  -210.075      9.187 -22.868   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1534 on 53936 degrees of freedom
## Multiple R-squared:  0.8521, Adjusted R-squared:  0.8521 
## F-statistic: 1.036e+05 on 3 and 53936 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The interaction term is <code>carat:depth</code>. (If you only want the interaction term and not the individual variables, use the colon instead of the asterisk.) Our model with interaction term is</p>
<p><span class="math display">\[\texttt{price}=-7824+20743\texttt{carat}+90\texttt{depth}-210(\texttt{carat})(\texttt{depth}).\]</span></p>
<p>A diamond with a depth percentage of 60 would have a price function of</p>
<p><span class="math display">\[\begin{align*}
\texttt{price}&amp;=-7824+20743\texttt{carat}+(90)(60)-(210)(60)\texttt{carat}\\
&amp;=(-7824+5400)+(20743-12600)\texttt{carat}\\
&amp;=-2424+8143\texttt{carat}.
\end{align*}\]</span></p>
</div>
</div>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-106" class="example"><strong>Example 5.17  </strong></span>When we included the <code>am</code> variable in our earlier <code>mpg</code> versus <code>hp</code> model, this had the effect of creating two models with the same slope and different intercepts. When we include an interaction term, we get something different. For this example, we will be using <code>wt</code> and <code>am</code> as the interacting terms.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="regression.html#cb122-1" tabindex="-1"></a>fitam <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg<span class="sc">~</span>wt<span class="sc">*</span>am, <span class="at">data =</span> mtcars)</span>
<span id="cb122-2"><a href="regression.html#cb122-2" tabindex="-1"></a><span class="fu">summary</span>(fitam)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt * am, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6004 -1.5446 -0.5325  0.9012  6.0909 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  31.4161     3.0201  10.402 4.00e-11 ***
## wt           -3.7859     0.7856  -4.819 4.55e-05 ***
## am           14.8784     4.2640   3.489  0.00162 ** 
## wt:am        -5.2984     1.4447  -3.667  0.00102 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.591 on 28 degrees of freedom
## Multiple R-squared:  0.833,  Adjusted R-squared:  0.8151 
## F-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11</code></pre>
<p>The overall model is</p>
<p><span class="math display">\[\texttt{mpg}=31.416-3.786\texttt{wt}+14.878\texttt{am}-5.298\texttt{wt}\cdot \texttt{am}.\]</span></p>
<p>We basically have two different models. For automatics, we set <span class="math inline">\(\texttt{am}=0\)</span> and get</p>
<p><span class="math display">\[\texttt{mpg}=31.416-3.786\texttt{wt}.\]</span></p>
<p>Changing <code>am</code> to 1 will change the intercept <em>and</em> the slope.</p>
<p><span class="math display">\[\begin{align*}
\texttt{mpg}&amp;=31.416-3.786\texttt{wt}+14.878-5.298\texttt{wt} \\
    &amp;=46.294-9.084\texttt{wt}.
\end{align*}\]</span></p>
<p>When we use the indicator variable (coerced into a factor variable) as the color in <code>ggplot</code>, we get the two separate lines (Fig. <a href="regression.html#fig:aminteraction">5.22</a>).</p>
</div>
</div>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="regression.html#cb124-1" tabindex="-1"></a><span class="co"># Change the am variable to a factor variable for ggplot2</span></span>
<span id="cb124-2"><a href="regression.html#cb124-2" tabindex="-1"></a>mt2<span class="ot">&lt;-</span>mtcars</span>
<span id="cb124-3"><a href="regression.html#cb124-3" tabindex="-1"></a>mt2<span class="sc">$</span>am <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(mtcars<span class="sc">$</span>am)</span>
<span id="cb124-4"><a href="regression.html#cb124-4" tabindex="-1"></a>ginteract <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(mt2, <span class="fu">aes</span>(mpg, wt, <span class="at">color =</span> am))<span class="sc">+</span></span>
<span id="cb124-5"><a href="regression.html#cb124-5" tabindex="-1"></a>        <span class="fu">geom_point</span>()<span class="sc">+</span></span>
<span id="cb124-6"><a href="regression.html#cb124-6" tabindex="-1"></a>        <span class="fu">geom_smooth</span>(<span class="at">method=</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb124-7"><a href="regression.html#cb124-7" tabindex="-1"></a>ginteract</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aminteraction"></span>
<img src="_main_files/figure-html/aminteraction-1.png" alt="Model with a categorical variable in an interaction" width="75%" />
<p class="caption">
Figure 5.22: Model with a categorical variable in an interaction
</p>
</div>
</div>
</div>
<div id="model-selection" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Model Selection<a href="regression.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Given a data set with many independent variables, it is tempting to choose the model that uses all of them. This might not be wise, however. Some of the variables might have nothing to do with the response. Some variables might be so closely related to others that they add nothing to the model. Some statisticians believe in the <em>principle of parsimony</em> <span class="citation">[<a href="#ref-Navidi">1</a>]</span>.</p>
<div class="notebox">
<p><strong>Principle of Parsimony</strong>
A model should contain the smallest number of variables necessary to fit the data.</p>
</div>
<p>We have already observed that the value of <span class="math inline">\(R^2\)</span> increases as the number of independent variables increases. Using <span class="math inline">\(R^2\)</span> as a criterion for choosing the best model would therefore be in conflict with the principle of parsimony. For this reason, among others, we consider the <em>adjusted</em> <span class="math inline">\(R^2\)</span>.</p>
<div class="defbox">
<div class="definition">
<p><span id="def:unlabeled-div-107" class="definition"><strong>Definition 5.1  </strong></span><span class="math display">\[\text{Adjusted } R^2=R^2-\left(\frac{k}{n-k-1}\right)(1-R^2),
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(k\)</span> is the number of independent variables in the model.</p>
</div>
</div>
<p>Since we’re always subtracting a positive amount from <span class="math inline">\(R^2\)</span>, the adjusted <span class="math inline">\(R^2\)</span> value is always less than <span class="math inline">\(R^2\)</span>, though if <span class="math inline">\(n\)</span> is large, the difference can be negligible.</p>
<p>In earlier sections, we developed models using the <code>mtcars</code> data frame for the fuel efficiency (<code>mpg</code>) of a car as a function of several variables.</p>
<ul>
<li>With only the variable <code>wt</code>, we had <span class="math inline">\(R^2=0.7528\)</span>. With <span class="math inline">\(n=32\)</span> and <span class="math inline">\(k=1\)</span>,</li>
</ul>
<p><span class="math display">\[\text{Adjusted } R^2=0.7528-\frac{1}{30}(1-0.7528)=0.74456.\]</span></p>
<ul>
<li>When the variables were horsepower and weight, we had <span class="math inline">\(R^2=0.8268\)</span>. With <span class="math inline">\(k=2\)</span>, we have <span class="math display">\[\text{Adjusted }R^2=0.8268-\frac{2}{29}(1-0.8268)=0.8149,\]</span></li>
</ul>
<p>an improvement over the single variable model.</p>
<ul>
<li>Adding displacement to the model barely changed <span class="math inline">\(R^2\)</span>, as it’s still 0.8268 to 4 decimal places, but with <span class="math inline">\(k=3\)</span>, we have</li>
</ul>
<p><span class="math display">\[ \text{Adjusted } R^2=0.8268-\frac{3}{28}(1-0.8268)=0.8082,\]</span></p>
<p>a decrease from our two-variable model.</p>
<p>There are other ways of comparing the qualities of multilinear models, including AIC (Akaike information criterion) <span class="citation">[<a href="#ref-AIC">18</a>]</span>, and BIC (Bayesian information criterion) <span class="citation">[<a href="#ref-BIC">19</a>]</span>. Having chosen a criterion, there are different ways of identifying the <em>best</em> model.</p>
<div id="best-subsets-regression" class="section level3 unnumbered hasAnchor">
<h3>Best subsets regression<a href="regression.html#best-subsets-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>best subsets regression</em> procedure simply compares all possible models with a given number of independent variables and selects the best one. Depending on the total number of independent variables available, this can be an inefficient procedure. The <code>regsubsets</code> function from the R package <code>leaps</code> handles best subsets regression.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-108" class="example"><strong>Example 5.18  </strong></span>In this example, we use best subsets on the <code>swiss</code> data frame to find the best model to predict <code>Fertility</code>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="regression.html#cb125-1" tabindex="-1"></a><span class="fu">library</span>(leaps)</span>
<span id="cb125-2"><a href="regression.html#cb125-2" tabindex="-1"></a>fitbest <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(Fertility<span class="sc">~</span>., <span class="at">data =</span> swiss)</span>
<span id="cb125-3"><a href="regression.html#cb125-3" tabindex="-1"></a><span class="fu">summary</span>(fitbest)<span class="sc">$</span>outmat</span></code></pre></div>
<pre><code>##          Agriculture Examination Education Catholic Infant.Mortality
## 1  ( 1 ) &quot; &quot;         &quot; &quot;         &quot;*&quot;       &quot; &quot;      &quot; &quot;             
## 2  ( 1 ) &quot; &quot;         &quot; &quot;         &quot;*&quot;       &quot;*&quot;      &quot; &quot;             
## 3  ( 1 ) &quot; &quot;         &quot; &quot;         &quot;*&quot;       &quot;*&quot;      &quot;*&quot;             
## 4  ( 1 ) &quot;*&quot;         &quot; &quot;         &quot;*&quot;       &quot;*&quot;      &quot;*&quot;             
## 5  ( 1 ) &quot;*&quot;         &quot;*&quot;         &quot;*&quot;       &quot;*&quot;      &quot;*&quot;</code></pre>
<p>The best one-variable model uses <code>Education</code> as the independent variable. The best two-variable model uses <code>Education</code> and <code>Catholic</code>. To find the best overall model, you can apply the plot function to the <code>regsubsets</code> output (Fig. <a href="regression.html#fig:leapsplot">5.23</a>). The best model appears at the top. In this example, the model that includes every variable except <code>Examination</code> is the best. The default selection statistic is BIC. For BIC, the lower the better.</p>
</div>
</div>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="regression.html#cb127-1" tabindex="-1"></a><span class="fu">plot</span>(fitbest)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:leapsplot"></span>
<img src="_main_files/figure-html/leapsplot-1.png" alt="A best subsets plot" width="75%" />
<p class="caption">
Figure 5.23: A best subsets plot
</p>
</div>
</div>
<div id="stepwise-regression" class="section level3 unnumbered hasAnchor">
<h3>Stepwise regression<a href="regression.html#stepwise-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A somewhat more efficient method for finding the best model is <em>stepwise regression</em>. In stepwise regression, you either start with the empty model or full model. If you start with the empty model, here is a summary of the forwards regression process.</p>
<ol style="list-style-type: decimal">
<li><p>Find the single variable that produces the best model.</p></li>
<li><p>Find the variable that, when added to the first, improves the model the most.</p></li>
<li><p>Repeat, until no additions will improve the situation.</p></li>
</ol>
<p>Note: it is possible that after the addition of one variable, the removal of one of the already included variables will improve the model. You can include this if desired.</p>
<p>Backwards regression starts with the full model and eliminates variables one-by-one. It is a little easier to perform in R. The <code>step</code> function is the function to use.</p>
<div class="examplebox">
<div class="example">
<p><span id="exm:unlabeled-div-109" class="example"><strong>Example 5.19  </strong></span>For our stepwise regression example, we again use the <code>swiss</code> data frame.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="regression.html#cb128-1" tabindex="-1"></a>fitall <span class="ot">&lt;-</span> <span class="fu">lm</span>(Fertility<span class="sc">~</span>., <span class="at">data =</span> swiss)</span>
<span id="cb128-2"><a href="regression.html#cb128-2" tabindex="-1"></a>fitstep <span class="ot">&lt;-</span> <span class="fu">step</span>(fitall)</span></code></pre></div>
<pre><code>## Start:  AIC=190.69
## Fertility ~ Agriculture + Examination + Education + Catholic + 
##     Infant.Mortality
## 
##                    Df Sum of Sq    RSS    AIC
## - Examination       1     53.03 2158.1 189.86
## &lt;none&gt;                          2105.0 190.69
## - Agriculture       1    307.72 2412.8 195.10
## - Infant.Mortality  1    408.75 2513.8 197.03
## - Catholic          1    447.71 2552.8 197.75
## - Education         1   1162.56 3267.6 209.36
## 
## Step:  AIC=189.86
## Fertility ~ Agriculture + Education + Catholic + Infant.Mortality
## 
##                    Df Sum of Sq    RSS    AIC
## &lt;none&gt;                          2158.1 189.86
## - Agriculture       1    264.18 2422.2 193.29
## - Infant.Mortality  1    409.81 2567.9 196.03
## - Catholic          1    956.57 3114.6 205.10
## - Education         1   2249.97 4408.0 221.43</code></pre>
<p>The output shows all steps in the process. The tables show the effect of removing each variable. Removing any variable above the <code>&lt;none&gt;</code> line improves the model (based on AIC). The variable at the top (if it is above <code>&lt;none&gt;</code>) goes. In this example, <code>Examination</code> gets dropped first. After that, the process stops, because the removal of any of the other variables will make the model worse.</p>
<p>The <code>step</code> output includes the final model, which is handy.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="regression.html#cb130-1" tabindex="-1"></a><span class="fu">summary</span>(fitstep)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Fertility ~ Agriculture + Education + Catholic + 
##     Infant.Mortality, data = swiss)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.6765  -6.0522   0.7514   3.1664  16.1422 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      62.10131    9.60489   6.466 8.49e-08 ***
## Agriculture      -0.15462    0.06819  -2.267  0.02857 *  
## Education        -0.98026    0.14814  -6.617 5.14e-08 ***
## Catholic          0.12467    0.02889   4.315 9.50e-05 ***
## Infant.Mortality  1.07844    0.38187   2.824  0.00722 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.168 on 42 degrees of freedom
## Multiple R-squared:  0.6993, Adjusted R-squared:  0.6707 
## F-statistic: 24.42 on 4 and 42 DF,  p-value: 1.717e-10</code></pre>
<p>Here is the process for forwards regression. In addition to the full model, we need to create an intercept-only model.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="regression.html#cb132-1" tabindex="-1"></a>fitio <span class="ot">&lt;-</span> <span class="fu">lm</span>(Fertility<span class="sc">~</span><span class="dv">1</span>, <span class="at">data =</span> swiss)</span>
<span id="cb132-2"><a href="regression.html#cb132-2" tabindex="-1"></a>fitallf <span class="ot">&lt;-</span> <span class="fu">formula</span>(fitall)</span>
<span id="cb132-3"><a href="regression.html#cb132-3" tabindex="-1"></a>fitstepf <span class="ot">&lt;-</span> <span class="fu">step</span>(fitio, <span class="at">direction =</span> <span class="st">&quot;forward&quot;</span>,  </span>
<span id="cb132-4"><a href="regression.html#cb132-4" tabindex="-1"></a>                  <span class="at">scope=</span>fitallf)</span></code></pre></div>
<pre><code>## Start:  AIC=238.35
## Fertility ~ 1
## 
##                    Df Sum of Sq    RSS    AIC
## + Education         1    3162.7 4015.2 213.04
## + Examination       1    2994.4 4183.6 214.97
## + Catholic          1    1543.3 5634.7 228.97
## + Infant.Mortality  1    1245.5 5932.4 231.39
## + Agriculture       1     894.8 6283.1 234.09
## &lt;none&gt;                          7178.0 238.34
## 
## Step:  AIC=213.04
## Fertility ~ Education
## 
##                    Df Sum of Sq    RSS    AIC
## + Catholic          1    961.07 3054.2 202.18
## + Infant.Mortality  1    891.25 3124.0 203.25
## + Examination       1    465.63 3549.6 209.25
## &lt;none&gt;                          4015.2 213.04
## + Agriculture       1     61.97 3953.3 214.31
## 
## Step:  AIC=202.18
## Fertility ~ Education + Catholic
## 
##                    Df Sum of Sq    RSS    AIC
## + Infant.Mortality  1    631.92 2422.2 193.29
## + Agriculture       1    486.28 2567.9 196.03
## &lt;none&gt;                          3054.2 202.18
## + Examination       1      2.46 3051.7 204.15
## 
## Step:  AIC=193.29
## Fertility ~ Education + Catholic + Infant.Mortality
## 
##               Df Sum of Sq    RSS    AIC
## + Agriculture  1   264.176 2158.1 189.86
## &lt;none&gt;                     2422.2 193.29
## + Examination  1     9.486 2412.8 195.10
## 
## Step:  AIC=189.86
## Fertility ~ Education + Catholic + Infant.Mortality + Agriculture
## 
##               Df Sum of Sq    RSS    AIC
## &lt;none&gt;                     2158.1 189.86
## + Examination  1    53.027 2105.0 190.69</code></pre>
<p>The output is similar. Notice the plus sign in front of the variables instead of minus signs. In this process, the variable at the top is added. <em>Replacing “forward” with “both” allows for potential additions and subtractions.</em></p>
</div>
</div>
</div>
<div id="some-warnings" class="section level3 unnumbered hasAnchor">
<h3>Some warnings<a href="regression.html#some-warnings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we undertake any model selection, we should consider what we are trying to accomplish.</p>
<ul>
<li><p>Are we trying to gain some understanding of the underlying relationships between variables? Note that adding and removing variables will change the coefficients in the model, unless those variables are completely uncorrelated with the other variables.</p></li>
<li><p>Are we trying to make predictions with our models? Oftentimes, using too many variables can lead to overfitting, which can lead to bad predictions.</p></li>
</ul>
</div>
</div>
<div id="Logistic" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Logistic Regression<a href="regression.html#Logistic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression is frequently used to make predictions about an object’s classification based upon numerical measures. Here are some examples:</p>
<ol style="list-style-type: decimal">
<li><p>Can you use the number of typos in an email to tell whether or not it is spam?</p></li>
<li><p>Can you use a baseball player’s college batting average to determine whether or not they will make it to the Major Leagues?</p></li>
</ol>
<p>Even if we can’t offer a guaranteed yes or no to these questions, we can at least try to give a probability that it is one thing or another.</p>
<p>Let’s start with a motivating example. The <code>Pima.tr</code> data frame in R contains diabetes-related data obtained from a sample of Akimel O’odham, or Pima, women. If we create a boxplot of blood glucose levels grouped by diabetes status (Fig. <a href="regression.html#fig:pima1">5.24</a>), we see that those with diabetes tend to have a higher glucose level.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="regression.html#cb134-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb134-2"><a href="regression.html#cb134-2" tabindex="-1"></a>Pima <span class="ot">&lt;-</span> MASS<span class="sc">::</span>Pima.tr</span>
<span id="cb134-3"><a href="regression.html#cb134-3" tabindex="-1"></a>pimabox <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(Pima, <span class="fu">aes</span>(glu, <span class="at">fill =</span> type))<span class="sc">+</span><span class="fu">geom_boxplot</span>()</span>
<span id="cb134-4"><a href="regression.html#cb134-4" tabindex="-1"></a>pimabox</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pima1"></span>
<img src="_main_files/figure-html/pima1-1.png" alt="Blood glucose levels grouped by diabetes status" width="672" />
<p class="caption">
Figure 5.24: Blood glucose levels grouped by diabetes status
</p>
</div>
<p>Can we use the glucose level to estimate the probability that an individual has diabetes? That is what <em>logistic regression</em> tries to do. In Figure <a href="regression.html#fig:pima2">5.25</a>, the types <code>No</code> and <code>Yes</code> are recoded as 0 and 1. If we try to fit a straight line to the data, some bad things happen. First, it’s not a very good fit, and second, if we try to use the model to estimate probabilities, we will get some negative probabilities and some that are greater than one.</p>
<div class="figure"><span style="display:block;" id="fig:pima2"></span>
<img src="_main_files/figure-html/pima2-1.png" alt="A straight line fit to 0-1 data" width="50%" /><img src="_main_files/figure-html/pima2-2.png" alt="A straight line fit to 0-1 data" width="50%" />
<p class="caption">
Figure 5.25: A straight line fit to 0-1 data
</p>
</div>
<p>If we instead try to fit a s-shaped <em>logistic</em> curve to the data, we get the graph in Figure <a href="regression.html#fig:pima3">5.26</a>. The function only takes on values between 0 and 1, so we can at least attempt to use it to estimate probabilities.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pima3"></span>
<img src="_main_files/figure-html/pima3-1.png" alt="A logistic fit" width="75%" />
<p class="caption">
Figure 5.26: A logistic fit
</p>
</div>
<p>So, how do we find the logistic curve? To get to the logistic model, we start with the <em>odds</em>. If <span class="math inline">\(p\)</span> is the probability of an event, we define the odds to be
<span class="math display">\[\text{odds}=\frac{p}{1-p}.\]</span></p>
<p>Note that these are the odds <em>in favor</em> of the event. In gambling, people usually give the odds <em>against</em> the event, which is just the reciprocal of these odds. Probabilities are between 0 and 1, whereas odds can be any non-negative number, unless <span class="math inline">\(p=1.\)</span> In that case, the odds are undefined.</p>
<p>The last step before regression involves taking the (natural) log of the odds. This gives us the <em>logit function</em>.</p>
<p><span class="math display">\[\text{logit}(p)=\ln\left(\frac{p}{1-p}\right).\]</span></p>
<p>The logit function has a range of <span class="math inline">\((-\infty,\infty).\)</span> We want to fit a model of the form</p>
<p><span class="math display">\[\text{logit}(p)=b_0+b_1 x,\]</span></p>
<p>but there is a little problem. In our data, the values of <span class="math inline">\(p\)</span> we have are 0 (<code>type = No</code>) and 1 (<code>type = Yes</code>). The logit function is undefined for both of those values. Long story short, we can’t use our linear regression formulas to find <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1.\)</span> Software packages use algorithmic methods, such as gradient descent, to find <em>maximum likelihood estimates</em> for the coefficients.</p>
<p>Let’s say we have the coefficients, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1.\)</span> If we solve</p>
<p><span class="math display">\[ \text{logit}(p)=\ln\frac{p}{1-p}=b_0+b_1x\]</span></p>
<p>for the odds (<span class="math inline">\(p/(1-p)\)</span>), we get</p>
<p><span class="math display">\[\text{odds}=e^{b_0+b_1 x}=e^{b_0}({e^{b_1}})^{ x}.\]</span></p>
<p>The value <span class="math inline">\(e^{b_1}\)</span> is the <em>odds ratio</em>. If <span class="math inline">\(x\)</span> increases by one unit, the odds will be multiplied by <span class="math inline">\(e^{b_1}\)</span>. If we solve the odds equation for <span class="math inline">\(p\)</span>, we get</p>
<p><span class="math display" id="eq:logreg">\[\begin{equation}
p=\frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}=\frac{1}{e^{-(b_0+b_1x)}+1},\tag{5.11}
\end{equation}\]</span></p>
<p>which is our logistic function.</p>
<p>Let’s use R to find our coefficients. We use the <code>glm</code> function with <code>family="binomial"</code>. (When a <em>function</em> of our response variable is a linear function of independent variables, we are in the category of <em>generalized linear models</em>.) We don’t have to recode the Yes and No values.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="regression.html#cb135-1" tabindex="-1"></a>logfit <span class="ot">&lt;-</span> <span class="fu">glm</span>(type<span class="sc">~</span>glu, <span class="at">data =</span> Pima, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb135-2"><a href="regression.html#cb135-2" tabindex="-1"></a><span class="fu">summary</span>(logfit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = type ~ glu, family = &quot;binomial&quot;, data = Pima)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -5.503636   0.836077  -6.583 4.62e-11 ***
## glu          0.037784   0.006278   6.019 1.76e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 256.41  on 199  degrees of freedom
## Residual deviance: 207.37  on 198  degrees of freedom
## AIC: 211.37
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>If we exponentiate the coefficients, we get the following.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="regression.html#cb137-1" tabindex="-1"></a><span class="fu">exp</span>(logfit<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>## (Intercept)         glu 
##  0.00407194  1.03850660</code></pre>
<p>Our odds ratio is around 1.0385, so each additional unit increase in the glucose value leads to an estimated 3.85% increase in the <em>odds</em> of having diabetes. The effect on the <em>probability</em> is not constant.</p>
<p>The logit function (see Figure <a href="regression.html#fig:pima3">5.26</a> for the graph) is</p>
<p><span class="math display">\[p=\frac{e^{-5.5036+0.0378\text{glu}}}{1+e^{-5.5036+0.0378\text{glu}}}.\]</span></p>
<p>The <code>predict</code> function works with <code>glu</code> output. To get probabilities, we need to set <code>type = "response"</code>.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="regression.html#cb139-1" tabindex="-1"></a><span class="fu">predict</span>(logfit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">glu =</span> <span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">200</span>)),  </span>
<span id="cb139-2"><a href="regression.html#cb139-2" tabindex="-1"></a>        <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1         2 
## 0.1511944 0.8862613</code></pre>
<p>The predicted probability that a woman with a glucose value of 100 has diabetes is 0.151, while a woman with a value of 200 has a probability of 0.886.</p>
<p>What about the other output? The standard error, <span class="math inline">\(z\)</span>-value, and <span class="math inline">\(p\)</span>-value have similar interpretations as in the linear regression case. Both coefficients are significantly different from zero. We will talk about the null and residual deviances a little later.</p>
<div id="logistic-regression-with-multiple-independent-variables" class="section level3 unnumbered hasAnchor">
<h3>Logistic regression with multiple independent variables<a href="regression.html#logistic-regression-with-multiple-independent-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As is the case with linear regression, we can add independent variables and interactions. Let’s add <code>age</code> as a second independent variable in our diabetes model.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="regression.html#cb141-1" tabindex="-1"></a>logfit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(type<span class="sc">~</span>glu<span class="sc">+</span>age, <span class="at">data =</span> Pima,  </span>
<span id="cb141-2"><a href="regression.html#cb141-2" tabindex="-1"></a>            <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb141-3"><a href="regression.html#cb141-3" tabindex="-1"></a><span class="fu">summary</span>(logfit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = type ~ glu + age, family = &quot;binomial&quot;, data = Pima)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -6.590597   0.950274  -6.935 4.05e-12 ***
## glu          0.032860   0.006375   5.155 2.54e-07 ***
## age          0.052295   0.016796   3.114  0.00185 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 256.41  on 199  degrees of freedom
## Residual deviance: 197.11  on 197  degrees of freedom
## AIC: 203.11
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>All coefficients are significantly different from zero. When we exponentiate those coefficients, we get the odds ratios for both glucose and age.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="regression.html#cb143-1" tabindex="-1"></a><span class="fu">exp</span>(logfit2<span class="sc">$</span>coefficients)</span></code></pre></div>
<pre><code>## (Intercept)         glu         age 
##  0.00137322  1.03340625  1.05368617</code></pre>
<p>Again, we can make predictions.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="regression.html#cb145-1" tabindex="-1"></a>pdata <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">glu =</span> <span class="fu">c</span>(<span class="dv">100</span>,<span class="dv">200</span>), <span class="at">age =</span> <span class="fu">c</span>(<span class="dv">20</span>,<span class="dv">30</span>))</span>
<span id="cb145-2"><a href="regression.html#cb145-2" tabindex="-1"></a><span class="fu">predict</span>(logfit2, <span class="at">newdata =</span> pdata,  <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##          1          2 
## 0.09460463 0.82495991</code></pre>
</div>
<div id="likelihood-and-deviance" class="section level3 unnumbered hasAnchor">
<h3>Likelihood and deviance<a href="regression.html#likelihood-and-deviance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The diagnostic output includes null and residual deviances. Let’s talk about those. First, we need to talk about the likelihood.</p>
<p>Suppose our <span class="math inline">\(i\)</span>th observation is given by a pair <span class="math inline">\((x_i,y_i),\)</span> where <span class="math inline">\(y_i\)</span> is either 0 or 1. When we create our logistic model, we get an estimated probability <span class="math inline">\(\hat{p}_i\)</span> that <span class="math inline">\(y=1\)</span> given that <span class="math inline">\(x=x_i\)</span>. The <em>likelihood</em> of this particular observation is then <span class="math inline">\(\hat{p}_i\)</span> if <span class="math inline">\(y_i=1\)</span> or <span class="math inline">\(1-\hat{p}_i\)</span> if <span class="math inline">\(y_i=0.\)</span> We can combine this likelihood into a single product:</p>
<p><span class="math display">\[\hat{p}_i^{y_i}(1-\hat{p}_i)^{1-y_i}.\]</span></p>
<p>If <span class="math inline">\(y_i=1,\)</span> then the <span class="math inline">\((1-\hat{p}_i)\)</span> exponent is 0, and similarly, when <span class="math inline">\(y_i=0,\)</span> the <span class="math inline">\(\hat{p}_i\)</span> exponent is 0. We assume that individual observations are independent of each other, which allows us to get a likelihood function for the whole data set by multiplying the individual likelihoods. (Assume we have <span class="math inline">\(n\)</span> observations.) This is a function of the coefficients <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span></p>
<p><span class="math display">\[L(b_0,b_1)=\prod_{i=1}^n \hat{p}_i^{y_i}(1-\hat{p}_i)^{1-y_i},\]</span></p>
<p>where</p>
<p><span class="math display">\[p_i=\frac{e^{b_0+b_1x_i}}{1+e^{b_0+b_1x_i}}.\]</span></p>
<p>The <span class="math inline">\(\Pi\)</span> represents a product of multiple terms much like a <span class="math inline">\(\Sigma\)</span> represnts a sum of multiple terms.</p>
<p>The maximum likelihood estimates are the values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that maximize this function. It is often easier in practice to maximize the natural logarithm of the likelihood function. Remember that the log of a product is the sum of the logs, so we have</p>
<p><span class="math display">\[\ell=\ln L=\sum_{i=1}^n\left( y_i\ln(\hat{p}_i)+(1-y_i)\ln(1-\hat{p}_i)\right).\]</span></p>
<p>For observation <span class="math inline">\(i\)</span>, the regular residual is <span class="math inline">\(e_i=y_i-\hat{p}_i.\)</span> We then define the <em>deviance residuals</em> to be</p>
<p><span class="math display">\[d_i=\text{sign}(e_i)\left[-2(y_i\ln(\hat{p}_i)+(1-y_i)\ln(1-\hat{p}_i))\right]^{1/2},\]</span></p>
<p>where <span class="math inline">\(\text{sign}(e_i)\)</span> is -1 if <span class="math inline">\(e_i&lt;0\)</span> and 1 if <span class="math inline">\(e_i&gt;0.\)</span> These deviance residuals are stored in the logistic regression output.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="regression.html#cb147-1" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">residuals</span>(logfit),<span class="dv">4</span>)</span></code></pre></div>
<pre><code>##          1          2          3          4 
## -0.4467738  0.5368785 -0.3795802 -1.4991921</code></pre>
<p>The <em>residual deviance</em> is then the sum of the squares of the deviance residuals.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="regression.html#cb149-1" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">residuals</span>(logfit))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 207.3727</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="regression.html#cb151-1" tabindex="-1"></a><span class="fu">deviance</span>(logfit)</span></code></pre></div>
<pre><code>## [1] 207.3727</code></pre>
<p>We define the <em>null deviance</em> similarly, but we set <span class="math inline">\(\hat{p}_i=\hat{p},\)</span> where <span class="math inline">\(\hat{p}\)</span> is the proportion of ones (or Yesses) in the data. In our Pima data set, <span class="math inline">\(\hat{p}=68/200=0.34.\)</span> The null model is basically an intercept-only model. If a logistic model is any good, it’s residual deviance should be significantly smaller than the null deviance.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="regression.html#cb153-1" tabindex="-1"></a>logfit<span class="sc">$</span>null.deviance</span></code></pre></div>
<pre><code>## [1] 256.4142</code></pre>
<p>One very simple <span class="math inline">\(R^2\)</span> analog for logistic regression is</p>
<p><span class="math display" id="eq:R2log">\[\begin{equation}R^2=1-\frac{\text{Residual Deviance}}{\text{Null Deviance}}.
\tag{5.12}
\end{equation}\]</span></p>
<p>In our one-variable example, we have</p>
<p><span class="math display">\[R^2=1-\frac{207.37}{256.41}=0.1913.\]</span></p>
<p>When we add age as the second independent variable, we get</p>
<p><span class="math display">\[R^2=1-\frac{197.11}{256.41}=0.2313.\]</span></p>
</div>
</div>
<div id="GDA" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> Gradient Descent<a href="regression.html#GDA" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Section <a href="regression.html#Logistic">5.9</a>, we couldn’t use a formula to find the coefficient estimates. There are various approximation techniques we can use to estimate parameters. One of them is the method of <em>gradient descent</em>, which can find minimum and maximum values of functions. This discussion does assume a working knowledge of partial differentiation, which is covered in Appendix A.</p>
<p>We will start with the one-variable case. Suppose we want to find the minimum value of the function</p>
<p><span class="math display">\[f(x)=x^4-3x^2+5x+10.\]</span></p>
<p>Its derivative is the cubic function</p>
<p><span class="math display">\[f&#39;(x)=4x^3-6x+5.\]</span></p>
<p>Finding the critical number involves solving a cubic equation, which is a pain in the neck. Instead, we will be taking advantage of the geometry of the graph to estimate the critical number. From the graph (Fig. <a href="regression.html#fig:GD1">5.27</a>), we see that the minimum value occurs between <span class="math inline">\(-2\)</span> and <span class="math inline">\(-1.\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD1"></span>
<img src="images/dg1.png" alt="Finding a local minimum" width="40%" /><img src="images/dg2.png" alt="Finding a local minimum" width="40%" />
<p class="caption">
Figure 5.27: Finding a local minimum
</p>
</div>
<p>Note that to the left of the minimum, <span class="math inline">\(f&#39;&lt;0\)</span>, while to the right, <span class="math inline">\(f&#39;&gt;0\)</span>. Also, because of the continuity of <span class="math inline">\(f&#39;\)</span>, the closer <span class="math inline">\(x\)</span> gets to the minimum, the closer <span class="math inline">\(f&#39;(x)\)</span> gets to 0.</p>
<p>Our algorithm will start with a guess <span class="math inline">\(x_0\)</span> that we hope is close to the critical number. The sign of <span class="math inline">\(f&#39;(x_0)\)</span> will tell us whether we need to move to the left or right: to the right if <span class="math inline">\(f&#39;(x_0)&lt;0\)</span> and to the left if <span class="math inline">\(f&#39;(x_0)&gt;0\)</span> (Fig. <a href="regression.html#fig:GD2">5.28</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD2"></span>
<img src="images/dg3.png" alt="Left or right" width="40%" /><img src="images/dg4.png" alt="Left or right" width="40%" />
<p class="caption">
Figure 5.28: Left or right
</p>
</div>
<p>The magnitude (absolute value) of <span class="math inline">\(f&#39;(x_0)\)</span> tells us approximately how far we need to move. Remember that <span class="math inline">\(f&#39;(x)\)</span> approaches 0 as <span class="math inline">\(x\)</span> approaches the critical number. How far we move in either direction can be adjusted by what is called the <em>learning rate</em>, which we will denote by <span class="math inline">\(\lambda\)</span>, where <span class="math inline">\(\lambda&gt;0\)</span>. Once we settle on a learning rate, we have our iteration formula.</p>
<p><span class="math display">\[\begin{align*}
    x_1&amp;=x_0-\lambda f&#39;(x_0)\\
    x_2&amp;=x_1-\lambda f&#39;(x_1)\\
    &amp;\vdots \\
    x_{n+1}&amp;=x_n-\lambda f&#39;(x_n).
\end{align*}\]</span></p>
<p>If <span class="math inline">\(f&#39;(x_n)&lt;0\)</span>, then, since we are subtracting <span class="math inline">\(\lambda f&#39;(x_n)\)</span>, we will move to the right. If <span class="math inline">\(f&#39;(x_n)&gt;0\)</span>, we will move to the left. The larger the value of <span class="math inline">\(|f&#39;(x_n)|\)</span>, the farther we move. We need to choose <span class="math inline">\(\lambda\)</span> so that we don’t move too far. This can involve trial and error. It also can be adjusted using a <em>learning rate schedule</em> as we get closer to our critical number.</p>
<p>Suppose we start with an initial guess of <span class="math inline">\(x_0=-2.0\)</span> and a learning rate of <span class="math inline">\(\lambda=0.001\)</span> Then</p>
<p><span class="math display">\[x_1=-2.0-0.001\cdot f&#39;(-2.0)=-2.0-0.001\cdot(-15)=-1.985.\]</span></p>
<p>Our first 5 steps look like this.</p>
<table>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(n\)</span></th>
<th align="right"><span class="math inline">\(x_n\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(0\)</span></td>
<td align="right"><span class="math inline">\(-2.0\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="right"><span class="math inline">\(-1.985\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(2\)</span></td>
<td align="right"><span class="math inline">\(-1.9706\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(3\)</span></td>
<td align="right"><span class="math inline">\(-1.9568\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(4\)</span></td>
<td align="right"><span class="math inline">\(-1.9436\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(5\)</span></td>
<td align="right"><span class="math inline">\(-1.9309\)</span></td>
</tr>
</tbody>
</table>
<p>If we look at the <span class="math inline">\((x_n,f(x_n))\)</span>-pairs on the graph (Fig. <a href="regression.html#fig:GD3">5.29</a>), we see that we are making slow, but steady progress.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD3"></span>
<img src="images/gd1.png" alt="Gradient descent" width="75%" />
<p class="caption">
Figure 5.29: Gradient descent
</p>
</div>
<p>Our gradient descent algorithm needs a mechanism for deciding when we’re close enough. We can choose to terminate the process when consecutive <span class="math inline">\(x\)</span> values agree within a certain amount, or perhaps when the absolute value of <span class="math inline">\(f&#39;(x_n)\)</span> gets below a certain amount. We should also set a maximum number of iterations that we want to go through, just in case there is a problem. Here is our algorithm.</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="regression.html#cb155-1" tabindex="-1"></a><span class="co"># Define f and f&#39;</span></span>
<span id="cb155-2"><a href="regression.html#cb155-2" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x<span class="sc">^</span><span class="dv">4-3</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="dv">5</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">10</span></span>
<span id="cb155-3"><a href="regression.html#cb155-3" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">4</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">3-6</span><span class="sc">*</span>x<span class="sc">+</span><span class="dv">5</span></span>
<span id="cb155-4"><a href="regression.html#cb155-4" tabindex="-1"></a></span>
<span id="cb155-5"><a href="regression.html#cb155-5" tabindex="-1"></a><span class="co"># initialization</span></span>
<span id="cb155-6"><a href="regression.html#cb155-6" tabindex="-1"></a>x0 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span> <span class="co">#initial guess</span></span>
<span id="cb155-7"><a href="regression.html#cb155-7" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.001</span> <span class="co">#learning rate</span></span>
<span id="cb155-8"><a href="regression.html#cb155-8" tabindex="-1"></a>tol <span class="ot">&lt;-</span> <span class="fl">0.0001</span> <span class="co">#tolerance (When do we stop?)</span></span>
<span id="cb155-9"><a href="regression.html#cb155-9" tabindex="-1"></a>maxsteps <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb155-10"><a href="regression.html#cb155-10" tabindex="-1"></a>steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb155-11"><a href="regression.html#cb155-11" tabindex="-1"></a></span>
<span id="cb155-12"><a href="regression.html#cb155-12" tabindex="-1"></a><span class="co"># The iteration</span></span>
<span id="cb155-13"><a href="regression.html#cb155-13" tabindex="-1"></a><span class="cf">while</span>(<span class="fu">abs</span>(<span class="fu">df</span>(x0)) <span class="sc">&gt;</span> tol <span class="sc">&amp;</span> steps <span class="sc">&lt;</span> maxsteps){</span>
<span id="cb155-14"><a href="regression.html#cb155-14" tabindex="-1"></a>  x0<span class="ot">&lt;-</span>x0<span class="sc">-</span>lambda<span class="sc">*</span><span class="fu">df</span>(<span class="at">x=</span>x0)</span>
<span id="cb155-15"><a href="regression.html#cb155-15" tabindex="-1"></a>  steps<span class="ot">&lt;-</span>steps<span class="sc">+</span><span class="dv">1</span>}</span>
<span id="cb155-16"><a href="regression.html#cb155-16" tabindex="-1"></a></span>
<span id="cb155-17"><a href="regression.html#cb155-17" tabindex="-1"></a>c1 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;x = &quot;</span>,<span class="fu">round</span>(x0,<span class="dv">3</span>))</span>
<span id="cb155-18"><a href="regression.html#cb155-18" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;minimum = &quot;</span>,<span class="fu">round</span>(<span class="fu">f</span>(x0),<span class="dv">3</span>))</span>
<span id="cb155-19"><a href="regression.html#cb155-19" tabindex="-1"></a>c3 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;steps = &quot;</span>, steps)</span>
<span id="cb155-20"><a href="regression.html#cb155-20" tabindex="-1"></a><span class="fu">cat</span>(c1,c2,c3,<span class="at">sep=</span><span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## x =  -1.523
## minimum =  0.807
## steps =  507</code></pre>
<p>We’ve found the location of the minimum in a measly 507 steps. If that seems too slow, we can increase the learning rate. When the learning rate is adjusted to <span class="math inline">\(\lambda=0.01\)</span>, we get convergence in 46 steps. With <span class="math inline">\(\lambda=0.05\)</span>, it converges in six (Fig. <a href="regression.html#fig:GD4">5.30</a>)!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD4"></span>
<img src="images/gd3.png" alt="Learning rates $\lambda=0.01$ (left),$\lambda=0.05$ (right)" width="40%" /><img src="images/gd4.png" alt="Learning rates $\lambda=0.01$ (left),$\lambda=0.05$ (right)" width="40%" />
<p class="caption">
Figure 5.30: Learning rates <span class="math inline">\(\lambda=0.01\)</span> (left),<span class="math inline">\(\lambda=0.05\)</span> (right)
</p>
</div>
<p>If we pick a value of <span class="math inline">\(\lambda\)</span> that is too high, for instance, <span class="math inline">\(\lambda=0.1\)</span>, something bad happens. It just bounces around between two values (Fig. <a href="regression.html#fig:GD5">5.31</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD5"></span>
<img src="images/gd5.png" alt="A learning rate that is too high" width="75%" />
<p class="caption">
Figure 5.31: A learning rate that is too high
</p>
</div>
<p>This function in Figure <a href="regression.html#fig:GD6">5.32</a> has two local minima. If we start near the one on the right, and use a small enough learning rate, we converge to it. If our learning rate is too large, we end up heading to the other minimum.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD6"></span>
<img src="images/gd6.png" alt="Two local minima" width="40%" /><img src="images/gd7.png" alt="Two local minima" width="40%" />
<p class="caption">
Figure 5.32: Two local minima
</p>
</div>
<div id="functions-of-several-variables" class="section level3 unnumbered hasAnchor">
<h3>Functions of several variables<a href="regression.html#functions-of-several-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now let’s turn to functions of several variables. As an example, let’s try to find the minimum value of</p>
<p><span class="math display">\[f(x,y)=x^4+y^4-x+2y.\]</span></p>
<p>The 3D and contour plots for <span class="math inline">\(f\)</span> appear in Figure <a href="regression.html#fig:GD7">5.33</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD7"></span>
<img src="images/gd8.png" alt="A function of two variables." width="40%" /><img src="images/gd9.png" alt="A function of two variables." width="40%" />
<p class="caption">
Figure 5.33: A function of two variables.
</p>
</div>
<p>Recall that at every point <span class="math inline">\((x,y)\)</span>, the gradient of <span class="math inline">\(f\)</span>, <span class="math inline">\(\nabla f(x,y)\)</span>, points in the direction of greatest increase (Fig. <a href="regression.html#fig:GD8">5.34</a>). If we want to find the minimum of <span class="math inline">\(f\)</span>, we need to go in the opposite direction.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD8"></span>
<img src="images/gd10.png" alt="The gradient points uphill" width="50%" />
<p class="caption">
Figure 5.34: The gradient points uphill
</p>
</div>
<p>If our initial guess in vector form is <span class="math inline">\(\begin{bmatrix}x_0\\y_0\end{bmatrix}\)</span>, then our iteration formula is</p>
<p><span class="math display">\[\begin{bmatrix}x_{n+1}\\y_{n+1}\end{bmatrix}=\begin{bmatrix}x_{n}\\y_{n}\end{bmatrix}-\lambda \nabla f(x_n,y_n).\]</span></p>
<p>The process can either terminate when successive approximations get to be within a certain amount of each other, or when the norm of <span class="math inline">\(\nabla f(x_n,y_n)\)</span> gets below a certain value. It can also terminate if it takes too long to converge.</p>
<p>With our example, <span class="math inline">\(f(x,y)=x^4+y^4-x+2y\)</span>, we have</p>
<p><span class="math display">\[\nabla f(x,y)=\begin{bmatrix}4x^3-1\\4y^3+2\end{bmatrix}.\]</span></p>
<p>Choosing an initial guess of <span class="math inline">\((x_0,y_0)=(0,0)\)</span>, and <span class="math inline">\(\lambda=0.01\)</span>, our next step is</p>
<p><span class="math display">\[\begin{bmatrix} x_1\\y_1\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}-0.01\left[\begin{array}{r}-1\\2\end{array}\right]=\left[\begin{array}{r}0.01\\-0.02\end{array}\right].\]</span></p>
<p>Here is the algorithm in R.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="regression.html#cb157-1" tabindex="-1"></a><span class="co"># Define the functions and partial derivatives</span></span>
<span id="cb157-2"><a href="regression.html#cb157-2" tabindex="-1"></a>f<span class="ot">&lt;-</span><span class="cf">function</span>(x,y) x<span class="sc">^</span><span class="dv">4</span><span class="sc">+</span>y<span class="sc">^</span><span class="dv">4</span><span class="sc">-</span>x<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>y</span>
<span id="cb157-3"><a href="regression.html#cb157-3" tabindex="-1"></a>fx<span class="ot">&lt;-</span><span class="cf">function</span>(x,y) <span class="dv">4</span><span class="sc">*</span>x<span class="sc">^</span><span class="dv">3-1</span></span>
<span id="cb157-4"><a href="regression.html#cb157-4" tabindex="-1"></a>fy<span class="ot">&lt;-</span><span class="cf">function</span>(x,y) <span class="dv">4</span><span class="sc">*</span>y<span class="sc">^</span><span class="dv">3</span><span class="sc">+</span><span class="dv">2</span></span>
<span id="cb157-5"><a href="regression.html#cb157-5" tabindex="-1"></a></span>
<span id="cb157-6"><a href="regression.html#cb157-6" tabindex="-1"></a><span class="co"># Initialize with (x0,y0)=(0,0)</span></span>
<span id="cb157-7"><a href="regression.html#cb157-7" tabindex="-1"></a>x0 <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb157-8"><a href="regression.html#cb157-8" tabindex="-1"></a>y0 <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb157-9"><a href="regression.html#cb157-9" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb157-10"><a href="regression.html#cb157-10" tabindex="-1"></a>maxsteps <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb157-11"><a href="regression.html#cb157-11" tabindex="-1"></a>steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb157-12"><a href="regression.html#cb157-12" tabindex="-1"></a>tol <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb157-13"><a href="regression.html#cb157-13" tabindex="-1"></a></span>
<span id="cb157-14"><a href="regression.html#cb157-14" tabindex="-1"></a><span class="co"># Find the norm of the gradient at the initial point</span></span>
<span id="cb157-15"><a href="regression.html#cb157-15" tabindex="-1"></a>gnorm <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">fx</span>(x0,y0)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="fu">fy</span>(x0,y0)<span class="sc">^</span><span class="dv">2</span>)  </span>
<span id="cb157-16"><a href="regression.html#cb157-16" tabindex="-1"></a></span>
<span id="cb157-17"><a href="regression.html#cb157-17" tabindex="-1"></a><span class="co"># The iteration</span></span>
<span id="cb157-18"><a href="regression.html#cb157-18" tabindex="-1"></a><span class="cf">while</span>(gnorm<span class="sc">&gt;</span>tol <span class="sc">&amp;</span> steps<span class="sc">&lt;</span>maxsteps){</span>
<span id="cb157-19"><a href="regression.html#cb157-19" tabindex="-1"></a>  x1 <span class="ot">&lt;-</span> x0<span class="sc">-</span>lambda<span class="sc">*</span><span class="fu">fx</span>(x0,y0)</span>
<span id="cb157-20"><a href="regression.html#cb157-20" tabindex="-1"></a>  y1 <span class="ot">&lt;-</span> y0<span class="sc">-</span>lambda<span class="sc">*</span><span class="fu">fy</span>(x0,y0)</span>
<span id="cb157-21"><a href="regression.html#cb157-21" tabindex="-1"></a>  x0 <span class="ot">&lt;-</span> x1</span>
<span id="cb157-22"><a href="regression.html#cb157-22" tabindex="-1"></a>  y0 <span class="ot">&lt;-</span> y1</span>
<span id="cb157-23"><a href="regression.html#cb157-23" tabindex="-1"></a>  steps<span class="ot">&lt;-</span>steps<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb157-24"><a href="regression.html#cb157-24" tabindex="-1"></a>  gnorm<span class="ot">&lt;-</span><span class="fu">sqrt</span>(<span class="fu">fx</span>(x0,y0)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="fu">fy</span>(x0,y0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb157-25"><a href="regression.html#cb157-25" tabindex="-1"></a>}</span>
<span id="cb157-26"><a href="regression.html#cb157-26" tabindex="-1"></a></span>
<span id="cb157-27"><a href="regression.html#cb157-27" tabindex="-1"></a>c1 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;(x,y) = (&quot;</span>,<span class="fu">round</span>(x0,<span class="dv">3</span>),<span class="st">&quot;,&quot;</span>,<span class="fu">round</span>(y0,<span class="dv">3</span>),<span class="st">&quot;)&quot;</span>)</span>
<span id="cb157-28"><a href="regression.html#cb157-28" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;minimum = &quot;</span>,<span class="fu">round</span>(<span class="fu">f</span>(x0,y0),<span class="dv">3</span>))</span>
<span id="cb157-29"><a href="regression.html#cb157-29" tabindex="-1"></a>c3 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;steps = &quot;</span>, steps)</span>
<span id="cb157-30"><a href="regression.html#cb157-30" tabindex="-1"></a><span class="fu">cat</span>(c1,c2,c3,<span class="at">sep=</span><span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## (x,y) = ( 0.63 , -0.794 )
## minimum =  -1.663
## steps =  243</code></pre>
<p>Figure <a href="regression.html#fig:GD9">5.35</a> shows the path to the minimum. Of course, we can improve covergence by adjusting the learning rate.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GD9"></span>
<img src="images/gd11.png" alt="Gradient descent to a minimum for a function of two variables" width="50%" />
<p class="caption">
Figure 5.35: Gradient descent to a minimum for a function of two variables
</p>
</div>
</div>
<div id="optimizing-parameters" class="section level3 unnumbered hasAnchor">
<h3>Optimizing parameters<a href="regression.html#optimizing-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An important application of gradient descent algorithms is in the optimization of parameters. As a simple example, we will use it to approximate the least squares regression coefficients. The gradient descent algorithm needs a function to minimize. In least squares regression, we have a natural candidate: the sum of squares of the residuals,</p>
<p><span class="math display">\[S=\sum_{i=1}^n(y_i-(\hat{b}_0+\hat{b}_1x_i))^2,\]</span></p>
<p>which is a function of <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>, the parameters that we are trying to estimate. We’re going to adjust this a little bit.</p>
<p>The generic term for the function that we try to minimize in gradient descent is the <em>cost function</em>. The traditional cost function for regression is</p>
<p><span class="math display">\[f(\hat{b}_0,\hat{b}_1)=\frac{1}{2n}\sum_{i=1}^n(y_i-(\hat{b}_0+\hat{b}_1x_i))^2\]</span></p>
<p>(or sometimes just <span class="math inline">\(1/n\)</span> in front). The scaling means that the term won’t grow as the sample size grows. The 2 goes away when we take the partial derivatives. Gradient descent requires the gradient, and the partial derivatives are</p>
<p><span class="math display">\[\frac{\partial f}{\partial \hat{b}_0}=-\frac{1}{n}\sum_{i=1}^n(y_i-(\hat{b}_0+\hat{b}_1x_i))\]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{\partial f}{\partial \hat{b}_1}=-\frac{1}{n}\sum_{i=1}^n(y_i-(\hat{b}_0+\hat{b}_1x_i))x_i.\]</span></p>
<p>We saw the unscaled version of these when we derived the formulas for <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>.</p>
<p>Let’s try to use gradient descent to find <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> for the following data.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(y_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">5</td>
</tr>
</tbody>
</table>
<p>We’ll start with initial guesses of <span class="math inline">\(\hat{b}_0=2\)</span> and <span class="math inline">\(\hat{b}_1=1\)</span>, and use a learning rate of <span class="math inline">\(\lambda=0.01\)</span>. We’re going to calculate each pair’s contribution to the gradient in this table.</p>
<table>
<colgroup>
<col width="7%" />
<col width="8%" />
<col width="38%" />
<col width="45%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><span class="math inline">\(x_i\)</span></th>
<th align="left"><span class="math inline">\(y_i\)</span></th>
<th align="left"><span class="math inline">\(-\frac{1}{4}(y_i-(\hat{b}_0+\hat{b}_1x_i))\)</span></th>
<th align="left"><span class="math inline">\(-\frac{1}{4}(y_i-(\hat{b}_0+\hat{b}_1x_i))x_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">3</td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(3-(2+1\cdot 1))=0\)</span></td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(3-(2+1\cdot 1))\cdot 1=0\)</span></td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">4</td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(4-(2+1\cdot 2))=0\)</span></td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(4-(2+1\cdot 2))\cdot 2=0\)</span></td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">2</td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(2-(2+1\cdot 3))=0.75\)</span></td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(2-(2+1\cdot 3)\cdot 3=2.25\)</span></td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">5</td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(5-(2+1\cdot 4))=0.25\)</span></td>
<td align="left"><span class="math inline">\(-\frac{1}{4}(5-(2+1\cdot 4))\cdot 4=1\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\sum\)</span></td>
<td align="left"><span class="math inline">\(1\)</span></td>
<td align="left"><span class="math inline">\(3.25\)</span></td>
</tr>
</tbody>
</table>
<p>Our next guess for <span class="math inline">\(\hat{b}_0\)</span> will be <span class="math inline">\(2-0.01\cdot 1=1.99\)</span> and our next guess for <span class="math inline">\(\hat{b}_1\)</span> will be <span class="math inline">\(1-0.01\cdot 3.25=0.9675\)</span>. We’ll continue with R.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="regression.html#cb159-1" tabindex="-1"></a><span class="co"># Cost function and its partials</span></span>
<span id="cb159-2"><a href="regression.html#cb159-2" tabindex="-1"></a>cost <span class="ot">&lt;-</span> <span class="cf">function</span>(x,y,b0,b1){</span>
<span id="cb159-3"><a href="regression.html#cb159-3" tabindex="-1"></a>  <span class="fu">sum</span>((y<span class="sc">-</span>(b0<span class="sc">+</span>b1<span class="sc">*</span>x))<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span><span class="fu">length</span>(x))}</span>
<span id="cb159-4"><a href="regression.html#cb159-4" tabindex="-1"></a>dcostb0 <span class="ot">&lt;-</span> <span class="cf">function</span>(x,y,b0,b1){</span>
<span id="cb159-5"><a href="regression.html#cb159-5" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(y<span class="sc">-</span>(b0<span class="sc">+</span>b1<span class="sc">*</span>x))<span class="sc">/</span><span class="fu">length</span>(x)}</span>
<span id="cb159-6"><a href="regression.html#cb159-6" tabindex="-1"></a>dcostb1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x,y,b0,b1){</span>
<span id="cb159-7"><a href="regression.html#cb159-7" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(y<span class="sc">*</span>x<span class="sc">-</span>b0<span class="sc">*</span>x<span class="sc">-</span>b1<span class="sc">*</span>x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">length</span>(x)}</span>
<span id="cb159-8"><a href="regression.html#cb159-8" tabindex="-1"></a></span>
<span id="cb159-9"><a href="regression.html#cb159-9" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb159-10"><a href="regression.html#cb159-10" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb159-11"><a href="regression.html#cb159-11" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">5</span>)</span>
<span id="cb159-12"><a href="regression.html#cb159-12" tabindex="-1"></a></span>
<span id="cb159-13"><a href="regression.html#cb159-13" tabindex="-1"></a><span class="co"># Initialization</span></span>
<span id="cb159-14"><a href="regression.html#cb159-14" tabindex="-1"></a>b0<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb159-15"><a href="regression.html#cb159-15" tabindex="-1"></a>b1<span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb159-16"><a href="regression.html#cb159-16" tabindex="-1"></a>lambda<span class="ot">&lt;-</span><span class="fl">0.1</span></span>
<span id="cb159-17"><a href="regression.html#cb159-17" tabindex="-1"></a>tol<span class="ot">&lt;-</span><span class="fl">0.00001</span></span>
<span id="cb159-18"><a href="regression.html#cb159-18" tabindex="-1"></a>maxsteps<span class="ot">&lt;-</span><span class="dv">10000</span></span>
<span id="cb159-19"><a href="regression.html#cb159-19" tabindex="-1"></a>gnorm<span class="ot">&lt;-</span><span class="fu">sqrt</span>(<span class="fu">dcostb0</span>(x,y,b0,b1)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="fu">dcostb1</span>(x,y,b0,b1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb159-20"><a href="regression.html#cb159-20" tabindex="-1"></a>steps<span class="ot">&lt;-</span><span class="dv">0</span></span></code></pre></div>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="regression.html#cb160-1" tabindex="-1"></a><span class="co">#Iteration</span></span>
<span id="cb160-2"><a href="regression.html#cb160-2" tabindex="-1"></a></span>
<span id="cb160-3"><a href="regression.html#cb160-3" tabindex="-1"></a><span class="cf">while</span>(gnorm<span class="sc">&gt;</span>tol <span class="sc">&amp;</span> steps <span class="sc">&lt;</span>maxsteps){</span>
<span id="cb160-4"><a href="regression.html#cb160-4" tabindex="-1"></a>  b0a<span class="ot">&lt;-</span>b0<span class="sc">-</span>lambda<span class="sc">*</span><span class="fu">dcostb0</span>(x,y,b0,b1)</span>
<span id="cb160-5"><a href="regression.html#cb160-5" tabindex="-1"></a>  b1a<span class="ot">&lt;-</span>b1<span class="sc">-</span>lambda<span class="sc">*</span><span class="fu">dcostb1</span>(x,y,b0,b1)</span>
<span id="cb160-6"><a href="regression.html#cb160-6" tabindex="-1"></a>  b0<span class="ot">&lt;-</span>b0a</span>
<span id="cb160-7"><a href="regression.html#cb160-7" tabindex="-1"></a>  b1<span class="ot">&lt;-</span>b1a</span>
<span id="cb160-8"><a href="regression.html#cb160-8" tabindex="-1"></a>  gnorm<span class="ot">&lt;-</span><span class="fu">sqrt</span>(<span class="fu">dcostb0</span>(x,y,b0,b1)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span><span class="fu">dcostb1</span>(x,y,b0,b1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb160-9"><a href="regression.html#cb160-9" tabindex="-1"></a>  steps<span class="ot">&lt;-</span>steps<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb160-10"><a href="regression.html#cb160-10" tabindex="-1"></a>}</span>
<span id="cb160-11"><a href="regression.html#cb160-11" tabindex="-1"></a></span>
<span id="cb160-12"><a href="regression.html#cb160-12" tabindex="-1"></a>c1 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;b0 = &quot;</span>,<span class="fu">round</span>(b0,<span class="dv">5</span>))</span>
<span id="cb160-13"><a href="regression.html#cb160-13" tabindex="-1"></a>c2 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;b1 = &quot;</span>, <span class="fu">round</span>(b1,<span class="dv">5</span>))</span>
<span id="cb160-14"><a href="regression.html#cb160-14" tabindex="-1"></a>c3 <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;steps = &quot;</span>,steps)</span>
<span id="cb160-15"><a href="regression.html#cb160-15" tabindex="-1"></a><span class="fu">cat</span>(c1,c2,c3,<span class="at">sep=</span><span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## b0 =  2.49994
## b1 =  0.40002
## steps =  611</code></pre>
<p>The actual intercept and slope are <span class="math inline">\(\hat{b}_0=2.5\)</span> and <span class="math inline">\(\hat{b}_1=0.4\)</span>, respectively. A larger learning rate will improve things, but we don’t really need to use gradient descent for linear regression on a small data set.</p>
<p>Using traditional gradient descent on large data sets can be computationally expensive and slow. <em>Stochastic gradient descent</em> intends to improve the speed by using a single randomly selected data point’s contribution to the gradient to update the parameters at each step. While it may take more steps for convergence, each step involves significantly fewer calculations.</p>
</div>
</div>
<div id="exercises-4" class="section level2 hasAnchor" number="5.11">
<h2><span class="header-section-number">5.11</span> Exercises<a href="regression.html#exercises-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Find the least squares solution to
<span class="math display">\[\begin{bmatrix} 4 &amp; 3\\2 &amp; 5\\2 &amp; 0\\4 &amp; 1\end{bmatrix}\mathbf{x}=\begin{bmatrix}2\\3\\1\\2\end{bmatrix}.\]</span></p></li>
<li><p>Find the least squares solution to
<span class="math display">\[\begin{bmatrix} 2 &amp; 1\\1 &amp; 3\\4 &amp; 2\\1 &amp; 5\end{bmatrix}\mathbf{x}=\begin{bmatrix}2\\0\\1\\5\end{bmatrix}.\]</span></p></li>
<li><p>Find the least squares solution to the following matrix equation. Note: the matrix on the left has dependent columns. You will probably want to use technology.
<span class="math display">\[\begin{bmatrix} 1 &amp; 1 &amp; 2\\1 &amp; 2 &amp; 3\\1 &amp; 3 &amp; 4\end{bmatrix}\mathbf{x}=\begin{bmatrix}3\\4\\6\end{bmatrix}\]</span></p></li>
<li><p>Find the least squares solution to the following matrix equation. Note: the matrix on the left has dependent columns. You will probably want to use technology.
<span class="math display">\[\begin{bmatrix} 1 &amp; 3 &amp; 2\\1 &amp; 2 &amp; 3\\1 &amp; 1 &amp; 4\end{bmatrix}\mathbf{x}=\begin{bmatrix}4\\3\\3\end{bmatrix}\]</span></p></li>
<li><p>Find the least squares error in the solution to Problem 1.</p></li>
<li><p>Find the least squares error in the solution to Problem 2.</p></li>
<li><p>Find the Massey ratings for the results we saw in Problem 39 from Chapter 1 and Problems 29 and 31 from Chapter 3.</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="left">Winner</th>
<th align="right">Score</th>
<th align="left">Loser</th>
<th align="right">Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Argos</td>
<td align="right">20</td>
<td align="left">Brahmas</td>
<td align="right">13</td>
</tr>
<tr class="even">
<td align="left">Cobbers</td>
<td align="right">31</td>
<td align="left">Brahmas</td>
<td align="right">10</td>
</tr>
<tr class="odd">
<td align="left">Argos</td>
<td align="right">24</td>
<td align="left">Cobbers</td>
<td align="right">21</td>
</tr>
<tr class="even">
<td align="left">Dukes</td>
<td align="right">10</td>
<td align="left">Brahmas</td>
<td align="right">7</td>
</tr>
<tr class="odd">
<td align="left">Brahmas</td>
<td align="right">35</td>
<td align="left">Dukes</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<ol start="8" style="list-style-type: decimal">
<li>Find the Massey ratings for the results we saw in Problem 40 from Chapter 1 and Problems 30 and 32 from Chapter 3.</li>
</ol>
<table>
<thead>
<tr class="header">
<th align="left">Winner</th>
<th align="right">Score</th>
<th align="left">Loser</th>
<th align="right">Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Argos</td>
<td align="right">28</td>
<td align="left">Cobbers</td>
<td align="right">14</td>
</tr>
<tr class="even">
<td align="left">Brahmas</td>
<td align="right">21</td>
<td align="left">Argos</td>
<td align="right">13</td>
</tr>
<tr class="odd">
<td align="left">Brahmas</td>
<td align="right">17</td>
<td align="left">Dukes</td>
<td align="right">10</td>
</tr>
<tr class="even">
<td align="left">Dukes</td>
<td align="right">31</td>
<td align="left">Brahmas</td>
<td align="right">28</td>
</tr>
</tbody>
</table>
<ol start="9" style="list-style-type: decimal">
<li><p>Find the offensive and defensive team ratings for the results in Problem 7.</p></li>
<li><p>Find the offensive and defensive team ratings for the results in Problem 8.</p></li>
<li><p>Suppose we have three <span class="math inline">\((x,y)\)</span>-pairs: <span class="math inline">\((2,5),(1,7),\)</span> and <span class="math inline">\((4,2)\)</span>. Find the least squares regression line <span class="math inline">\(\hat{y}=\hat{b}_0+\hat{b}_1x\)</span> using the normal equations (Theorem <a href="regression.html#thm:NormalEq">5.1</a>).</p></li>
<li><p>Suppose we have three <span class="math inline">\((x,y)\)</span>-pairs: <span class="math inline">\((4,1),(2,6),\)</span> and <span class="math inline">\((6,3)\)</span>. Find the least squares regression line <span class="math inline">\(\hat{y}=\hat{b}_0+\hat{b}_1x\)</span> using the normal equations.</p></li>
<li><p>Suppose we have four <span class="math inline">\((x,y,z)\)</span>-triples: <span class="math inline">\((1,2,3),(2,4,1),(5,0,2),\)</span> and <span class="math inline">\((3,3,1).\)</span>. Find the least squares regression equation <span class="math inline">\(\hat{z}=\hat{b}_0+\hat{b}_1x+\hat{b}_2y\)</span> using the normal equations.</p></li>
<li><p>Suppose we have four <span class="math inline">\((x,y,z)\)</span>-triples: <span class="math inline">\((2,1,4),(3,2,1),(5,1,1),\)</span> and <span class="math inline">\((0,2,3).\)</span>. Find the least squares regression equation <span class="math inline">\(\hat{z}=\hat{b}_0+\hat{b}_1x+\hat{b}_2y\)</span> using the normal equations.</p></li>
<li><p>Find Pearson’s correlation coefficient for the following data, using only basic calculator functions.</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="right">x</th>
<th align="right">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">5</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<ol start="16" style="list-style-type: decimal">
<li>Find Pearson’s correlation coefficient for the following data, using only basic calculator functions.</li>
</ol>
<table>
<thead>
<tr class="header">
<th align="right">x</th>
<th align="right">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">4</td>
</tr>
</tbody>
</table>
<ol start="17" style="list-style-type: decimal">
<li><em>Spearman’s rank correlation</em> ranks the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values and then finds the correlation of the ranks. For instance, for the data in Problem 15, when we rank the values, we get the table below. (Ties get averaged.) Find Spearman’s rank correlation by finding Pearson’s correlation of the ranks. (Note: you might find some special formulas for Spearman’s rank correlation, but those often only work when there are no ties in the data.)</li>
</ol>
<table>
<thead>
<tr class="header">
<th align="right">x</th>
<th align="right">y</th>
<th align="right">x.rank</th>
<th align="right">y.rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1.0</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">4.0</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">3</td>
<td align="right">4</td>
<td align="right">2.5</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">5.0</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">3</td>
<td align="right">2</td>
<td align="right">2.5</td>
</tr>
</tbody>
</table>
<ol start="18" style="list-style-type: decimal">
<li><p>Find Spearman’s correlation for the data in Problem 16.</p></li>
<li><p>Use R’s <code>cor</code> function to find the variable in the <code>swiss</code> data frame that is most strongly correlated with <code>Education</code> other than <code>Education</code> itself. Note: the strongest correlation might be negative.</p></li>
<li><p>Use R’s <code>cor</code> function to find the variable in the <code>swiss</code> data frame that is most strongly correlated with <code>Agriculture</code> other than <code>Agriculture</code> itself. Note: the strongest correlation might be negative.</p></li>
<li><p>Use Formulas <a href="regression.html#eq:slope">(5.9)</a> and <a href="regression.html#eq:intercept">(5.8)</a> to find the least squares regression equation for the data in Problem 15. Find the residual for each observation.</p></li>
<li><p>Use Formulas <a href="regression.html#eq:slope">(5.9)</a> and <a href="regression.html#eq:intercept">(5.8)</a> to find the least squares regression equation for the data in Problem 16. Find the residual for each observation.</p></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Find the least squares regression equation for <code>Education</code> as a function of <code>Examination</code> from the <code>swiss</code> data frame.</li>
<li>State the effect of a one-unit increase in the <code>Examination</code> value on the mean <code>Education</code> value.</li>
<li>State the coefficient of determination in the context of the data.</li>
<li>Determine which coefficents are significantly different from zero. (Which <span class="math inline">\(P\)</span>-values are below 0.05?)</li>
<li>Find 95% confidence intervals for the coefficients.</li>
<li>Find a 95% confidence interval for the mean <code>Education</code> value for an <code>Examination</code> value of 16.</li>
<li>Find a 95% prediction interval for the <code>Education</code> value for an <code>Examination</code> value of 16.</li>
</ol></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Find the least squares regression equation for <code>Agriculture</code> as a function of <code>Examination</code> from the <code>swiss</code> data frame.</li>
<li>State the effect of a one-unit increase in the <code>Examination</code> value on the mean <code>Agriculture</code> value.</li>
<li>State the coefficient of determination in the context of the data.</li>
<li>Determine which coefficents are significantly different from zero. (Which <span class="math inline">\(P\)</span>-values are below 0.05?)</li>
<li>Find 95% confidence intervals for the coefficients.</li>
<li>Find a 95% confidence interval for the mean <code>Agriculture</code> value for an <code>Examination</code> value of 16.</li>
<li>Find a 95% prediction interval for the <code>Agriculture</code> value for an <code>Examination</code> value of 16.</li>
</ol></li>
<li><p>Use R’s <code>lm</code> function to find a linear equation for <span class="math inline">\(z\)</span> as a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for the data in Problem 13.</p>
<ol style="list-style-type: lower-alpha">
<li>Write out the equation.</li>
<li>Find the coefficient of determination.</li>
<li>Identify which coefficients are significantly different from zero. (Which <span class="math inline">\(P\)</span>-values are below 0.05?)</li>
</ol></li>
<li><p>Use R’s <code>lm</code> function to find a linear equation for <span class="math inline">\(z\)</span> as a function of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for the data in Problem 14.</p>
<ol style="list-style-type: lower-alpha">
<li>Write out the equation.</li>
<li>Find the coefficient of determination.</li>
<li>Identify which coefficients are significantly different from zero. (Which <span class="math inline">\(P\)</span>-values are below 0.05?)</li>
</ol></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Write <code>Education</code> as a linear function of <code>Examination</code> and <code>Agriculture</code> from the <code>swiss</code> data frame.</li>
<li>Find the coefficient of determination.</li>
<li>Identify which coefficients are significantly different from zero.</li>
<li>Find a 95% prediction interval for the <code>Education</code> value if <code>Examination = 16</code> and <code>Agriculture = 50</code>.</li>
</ol></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Write <code>Agriculture</code> as a linear function of <code>Examination</code> and <code>Education</code> from the <code>swiss</code> data frame.</li>
<li>Find the coefficient of determination.</li>
<li>Identify which coefficients are significantly different from zero.</li>
<li>Find a 95% prediction interval for the <code>Agriculture</code> value if <code>Examination = 16</code> and <code>Education = 10</code>.</li>
</ol></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Write <code>mpg</code> as a linear function of <code>hp</code> and <code>vs</code> from the <code>mtcars</code> data frame.</li>
<li>The <code>vs</code> variable is an indicator variable. What effect does changing from a v-shaped engine (<code>vs = 0</code>) to a straight engine (<code>vs = 1</code>) have on <code>mpg</code>?</li>
</ol></li>
<li><p>Use R to do the following.</p>
<ol style="list-style-type: lower-alpha">
<li>Write <code>wt</code> as a linear function of <code>hp</code> and <code>vs</code> from the <code>mtcars</code> data frame.</li>
<li>The <code>vs</code> variable is an indicator variable. What effect does changing from a v-shaped engine (<code>vs = 0</code>) to a straight engine (<code>vs = 1</code>) have on <code>wt</code>?</li>
</ol></li>
<li><p>Create a linear model with the same variables as in Problem 29, but include an interaction term between <code>hp</code> and <code>vs</code>.</p>
<ol style="list-style-type: lower-alpha">
<li>How does going from <code>vs = 0</code> to <code>vs = 1</code> affect the intercept?</li>
<li>How does going from <code>vs = 0</code> to <code>vs = 1</code> affect the slope?</li>
</ol></li>
<li><p>Create a linear model with the same variables as in Problem 30, but include an interaction term between <code>hp</code> and <code>vs</code>.</p>
<ol style="list-style-type: lower-alpha">
<li>How does going from <code>vs = 0</code> to <code>vs = 1</code> affect the intercept?</li>
<li>How does going from <code>vs = 0</code> to <code>vs = 1</code> affect the slope?</li>
</ol></li>
<li><p>The <code>cars</code> data frame has data on stopping distances in feet (<code>dist</code>) for cars based on their speed in miles per hour (<code>speed</code>). Use R to find a model for stopping distance as a <em>quadratic</em> function of speed. Is the quadratic fit better than a linear fit? Explain.</p></li>
<li><p>(See Problem 33.) Use R to find a model for stopping distance as a <em>cubic</em> function (third degree polynomial) of speed. Include first and second degree terms. Is the cubic fit better than a linear or quadratic fit? Explain.</p></li>
<li><p>Suppose a model with four independent variables based on 50 observations has a regular <span class="math inline">\(R^2\)</span> value of 0.85. Find the adjusted <span class="math inline">\(R^2\)</span> value.</p></li>
<li><p>Suppose a model with five independent variables based on 40 observations has a regular <span class="math inline">\(R^2\)</span> value of 0.92. Find the adjusted <span class="math inline">\(R^2\)</span> value.</p></li>
<li><p>Use the <code>regsubsets</code> function from the <code>leaps</code> package on the <code>mtcars</code> data frame with <code>qsec</code> as the dependent variable, and all the other variables as independent variables.</p>
<ol style="list-style-type: lower-alpha">
<li>Which variables are involved in the best 2-variable model?</li>
<li>Which variables are involved in the best 3-variable model?</li>
<li>According to the BIC plot, which variables are involved in the best overall model?</li>
</ol></li>
<li><p>Use the <code>regsubsets</code> function from the <code>leaps</code> package on the <code>mtcars</code> data frame with <code>wt</code> as the dependent variable, and all the other variables as independent variables.</p>
<ol style="list-style-type: lower-alpha">
<li>Which variables are involved in the best 2-variable model?</li>
<li>Which variables are involved in the best 3-variable model?</li>
<li>According to the BIC plot, which variables are involved in the best overall model?</li>
</ol></li>
<li><p>Use the <code>step</code> function to perform backwards stepwise regression on the <code>mtcars</code> data frame with <code>qsec</code> as the dependent variable and all of the other variables as independent variables.</p>
<ol style="list-style-type: lower-alpha">
<li>Which is the first variable removed?</li>
<li>What variables are involved in the final model?</li>
<li>Does forward stepwise regression produce a different final model?</li>
</ol></li>
<li><p>Use the <code>step</code> function to perform backwards stepwise regression on the <code>mtcars</code> data frame with <code>wt</code> as the dependent variable and all of the other variables as independent variables.</p>
<ol style="list-style-type: lower-alpha">
<li>Which is the first variable removed?</li>
<li>What variables are involved in the final model?</li>
<li>Does forward stepwise regression produce a different final model?</li>
</ol></li>
<li><p>Remove the <code>setosa</code> species from the <code>iris</code> data frame, creating a new data frame called <code>iris2</code> by executing the command <code>iris2 &lt;- droplevels(subset(iris, Species != "setosa"))</code>. Now create a logistic model with <code>Species</code> as a function of <code>Petal.Width</code> (units are centimeters). The model will have <code>versicolor</code> as species 0, and <code>virginica</code> as species 1.</p>
<ol style="list-style-type: lower-alpha">
<li>Write the logistic model that gives the probability a flower is virginica as a function of its petal width.</li>
<li>How does an increase of 1 centimeter in petal width change the <em>odds</em> that a flower is a virginica?</li>
<li>Estimate the probabilities a flower is a virginica for petal widths of 1 and 2 centimeters.</li>
<li>Find the logistic “<span class="math inline">\(R^2\)</span>” value (Equation <a href="regression.html#eq:R2log">(5.12)</a>).</li>
</ol></li>
<li><p>Working with the same data as in Problem 41, create a logistic model with <code>Species</code> as a function of <code>Sepal.Length</code></p>
<ol style="list-style-type: lower-alpha">
<li>Write the logistic model that gives the probability a flower is virginica as a function of its sepal length.</li>
<li>How does an increase of 1 centimeter in sepal length change the <em>odds</em> that a flower is a virginica?</li>
<li>Estimate the probabilities a flower is a virginica for sepal lengths of 6 and 7 centimeters.</li>
<li>Find the logistic “<span class="math inline">\(R^2\)</span>” value.</li>
</ol></li>
<li><p>Working with the same data as in Problem 41, create a logistic model with <code>Species</code> as a function of <code>Petal.Width</code> and <code>Petal.Length</code>. Estimate the probability a flower is a virginica if it has a petal length of 5cm and a petal width of 1.5cm.</p></li>
<li><p>Working with the same data as in Problem 41, create a logistic model with <code>Species</code> as a function of <code>Sepal.Width</code> and <code>Sepal.Length</code>. Estimate the probability a flower is a virginica if it has a sepal length of 6cm and a sepal width of 3cm.</p></li>
<li><p>The function <span class="math inline">\(f(x)=x^4-2x^3+4x^2-x\)</span> has a local minimum between <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Apply the gradient descent algorithm, starting with an initial guess of <span class="math inline">\(x_0=1\)</span> and a learning rate of <span class="math inline">\(\lambda=0.1.\)</span> Calculate <span class="math inline">\(x_1,x_2,\)</span> and <span class="math inline">\(x_3\)</span> by hand.</li>
<li>Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001.</li>
</ol></li>
<li><p>The function <span class="math inline">\(f(x)=x^4+2x^3+3x^2-2x\)</span> has a local minimum between <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Apply the gradient descent algorithm, starting with an initial guess of <span class="math inline">\(x_0=1\)</span> and a learning rate of <span class="math inline">\(\lambda=0.1.\)</span> Calculate <span class="math inline">\(x_1,x_2,\)</span> and <span class="math inline">\(x_3\)</span> by hand.</li>
<li>Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001.</li>
</ol></li>
<li><p>The function <span class="math inline">\(f(x,y)=x^4+y^2-x-y\)</span> has a global minimum in the square <span class="math inline">\(0\leq x \leq 1, 0\leq y\leq 1.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Apply the gradient descent algorithm, starting with an initial guess of <span class="math inline">\((x_0,y_0)=(1,1)\)</span> and a learning rate of <span class="math inline">\(\lambda=0.1.\)</span> Calculate <span class="math inline">\((x_1,y_1)\)</span> and <span class="math inline">\((x_2,y_2)\)</span> by hand.</li>
<li>Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001.</li>
</ol></li>
<li><p>The function <span class="math inline">\(f(x,y)=x^4+2y^2-2x-3y\)</span> has a global minimum in the square <span class="math inline">\(0\leq x \leq 1, 0\leq y\leq 1.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Apply the gradient descent algorithm, starting with an initial guess of <span class="math inline">\((x_0,y_0)=(1,1)\)</span> and a learning rate of <span class="math inline">\(\lambda=0.1.\)</span> Calculate <span class="math inline">\((x_1,y_1)\)</span> and <span class="math inline">\((x_2,y_2)\)</span> by hand.</li>
<li>Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001.</li>
</ol></li>
</ol>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body" entry-spacing="0">
<div id="ref-Navidi" class="csl-entry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Navidi W (2015) Statistics for engineers and scientists, 4th ed., McGraw-Hill Education.</div>
</div>
<div id="ref-OA" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Olofsson P, Andersson M (2012) Probability, statistics, and stochastic processes, 2nd ed., John Wiley &amp; Sons, Inc.</div>
</div>
<div id="ref-No1" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Langville AN, Meyer CD (2012) Who’s #1? The science of rating and ranking, Princeton University Press.</div>
</div>
<div id="ref-Massey" class="csl-entry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Massey K (1997) Statistical models applied to the rating of sports teams.</div>
</div>
<div id="ref-GP" class="csl-entry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Smith B (2015) <a href="https://beersmith.com/blog/2015/01/30/calculating-original-gravity-for-beer-recipe-design/">Calculating original gravity for beer recipe design</a>.</div>
</div>
<div id="ref-statology" class="csl-entry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Bobbitt Z (2021) <a href="https://www.statology.org/diagnostic-plots-in-r/">How to interpret diagnostic plots in <span>R</span></a>.</div>
</div>
<div id="ref-AIC" class="csl-entry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Akaike H (1974) A new look at the statistical model identification. <em>IEEE Trans Automatic Control</em> AC-19: 716–723.</div>
</div>
<div id="ref-BIC" class="csl-entry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Schwarz G (1978) Estimating the dimension of a model. <em>Ann Statist</em> 6: 461–464.</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>Or mean ABV. We’ll discuss this later.<a href="regression.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="orthogonality.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="svd-and-pca.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"split_bib": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
