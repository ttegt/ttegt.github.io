[["index.html", "Linear Algebra for Data Science Preface", " Linear Algebra for Data Science Tom Tegtmeyer Preface This book was assembled from lecture notes for a Math for Data Science course, created and refined over the course of five semesters. The intent of the course is to provide students with the mathematical background for some of the ideas that they might see in a Data Science course. While the book does illustrate some data science techniques, it is not intended to be a course on Data Science. It is also not a Statistics course, and outside of the mathematical aspects of regression and correlation, statistical content is left for other courses. As the course was taught remotely the first few semesters, some of the lectures slides were cobbled together from previously existing slides from several courses. The texts used for those courses included books by Navidi[1]; Lial, et. al.[2]; Edwards, Penney, and Calvis [3]; Neuhauser[4]; and Lay, Lay, and McDonald[5]. Some of the theorem and definition statements come from them. The only prerequisite for the course is one semester of Calculus, though some students may have a deeper background in mathematics. There is one topic, gradient descent, that requires some understanding of partial differentiation. When the course is taught, some time is spent introducing students to that material. To maintain a more coherent outline, That material appears in Appendix A. In order to avoid relying on external data sets, all of the examples and exercises use data that either comes with the base R package or comes with easily available R packages. Appendix C contains information the packages used in the book. Speaking of R, the book does require the use of R. To download and install R and the integrated development environment RStudio, see the following links and follow the installation instructions. R: https://www.r-project.org/ RStudio: https://posit.co/download/rstudio-desktop/ © (2024) Tom Tegtmeyer. All rights reserved. References 1. Navidi W (2015) Statistics for engineers and scientists, 4th ed., McGraw-Hill Education. 2. Lial ML, Hungerford TW, Holcomb JP, et al. (2018) Mathematics with applications in the management, natural, and social sciences, 12th ed., Pearson. 3. Edwards CH, Penney DE, Calvis DT (2017) Differential equations and linear algebra, 4th ed., Pearson. 4. Neuhauser C (2004) Calculus for biology and medicine, 2nd ed., Pearson/Prentice Hall. 5. Lay DC, Lay SR, McDonald JJ (2015) Linear algebra and its applications, Pearson Education. "],["matrices-and-systems-of-equations.html", "Chapter 1 Matrices and Systems of Equations 1.1 Systems of Equations 1.2 Method of Solution: Gaussian Elimination 1.3 Matrices 1.4 Basic Matrix Operations 1.5 Matrix Inverses 1.6 Determinants 1.7 Systems of Linear Equations as Matrix Equations 1.8 Application: The Colley Matrix Method 1.9 Exercises", " Chapter 1 Matrices and Systems of Equations Matrices play an important role in Data Science. For instance, the infamous mtcars data frame has 11 columns of variables. Each row in the matrix represents a vehicle. mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 The grayscale values of an image of Hahns Peak (Fig. 1.1) in Routt County, Colorado, can be stored in a matrix. Figure 1.1: Hahns Peak (author photo) Here are the first ten rows and five columns of that matrix. [,1] [,2] [,3] [,4] [,5] [1,] 0.8236078 0.8196863 0.8157647 0.8118431 0.8040000 [2,] 0.8196863 0.8196863 0.8157647 0.8079216 0.8040000 [3,] 0.8157647 0.8157647 0.8118431 0.8079216 0.8040000 [4,] 0.8118431 0.8079216 0.8079216 0.8040000 0.8000784 [5,] 0.8040000 0.8040000 0.8040000 0.8000784 0.7961569 [6,] 0.8000784 0.8000784 0.8000784 0.7961569 0.7961569 [7,] 0.7961569 0.7961569 0.7961569 0.7922353 0.7922353 [8,] 0.7961569 0.7922353 0.7922353 0.7922353 0.7922353 [9,] 0.7922353 0.7883137 0.7804706 0.7765490 0.7726275 [10,] 0.7883137 0.7843922 0.7804706 0.7765490 0.7726275 Before we do any fancy things with these matrices, we need to start with the basics. We need to talk about systems of equations. 1.1 Systems of Equations The simplest example of a system of linear equations is a system of two equations and two variables, such as \\[\\begin{equation} \\left\\{ \\begin{array}{ccl} x+y&amp;=&amp;150\\\\ 12x+6y&amp;=&amp;1500 \\end{array}\\right. . \\tag{1.1} \\end{equation}\\] This system of equations is linear because both equations in the system are linear. A solution to the system of equations is an \\((x,y)\\)-pair that satisfies both equations. The geometry A single linear equation in two variables has a line as its graph. A linear system of two equations in two variables involves two lines (Fig. 1.2). Figure 1.2: Two intersecting lines A solution to the system must lie on both lines. In this case, it is the point of intersection. Not all systems of two linear equations in two variables have a solution. If the two lines are parallel, there is no solution (Fig. 1.3). Such systems are called inconsistent. (A system with a solution is consistent.) Figure 1.3: Two parallel lines Sometimes the two equations actually represent the same line. In that case the system is called dependent (Fig. 1.4). Since every point on the line is a solution to both equations, we have infinitely many solutions. Figure 1.4: Two equations, one line The three cases With a system of two linear equations in two variables, there are three possibilities. Exactly one solution (intersecting lines) Infinitely many solutions (just one line) No solutions (parallel lines) It turns out, when we increase the number of variables and/or equations, there are still these three possibilities for the number of solutions. 1.2 Method of Solution: Gaussian Elimination Lets return to our original system of equations and try to solve it. \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+y&amp;=&amp;150\\\\ 12x+6y&amp;=&amp;1500 \\end{array}\\right. . \\end{equation*}\\] The method of Gaussian elimination involves getting the system into a form that looks like this. \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} *x+*y &amp; = &amp;* \\\\ *y &amp; = &amp; * \\end{array}\\right. , \\end{equation*}\\] where the asterisks represent different constants. If we have it in this upper triangular form, we can immediately solve for \\(y\\), and once we know \\(y\\), we can use its value the first equation to solve for \\(x\\). We can put a system of equations into upper triangular form using a sequence of operations on the equations. The following operations do not change the solution set for a linear system. Multiplying both sides of an equation by a nonzero constant Adding a multiple of one equation to another (on both sides) Rearranging the order of equations Once the system is in upper triangular form, we can use back substitution to solve for all the variables. In the system \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+y&amp;=&amp;150\\\\ 12x+6y&amp;=&amp;1500 \\end{array}\\right. \\end{equation*}\\] we want to turn the \\(x\\) coefficient in the second equation into a \\(0\\). We can do this by multiplying the first equation by \\(-12\\) and adding it to the second one. Note that we keep the first equation as is. This puts us into upper triangular form. \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+y&amp;=&amp;150\\\\ -6y&amp;=&amp;-300 \\end{array}\\right. \\tag{1.2} \\end{equation*}\\] We can now solve for \\(y: y=\\frac{-300}{-6}=50\\). Plugging this value in for \\(y\\) in the first equation gives us \\(x+50=150\\) or \\(x=100\\). Our solution is \\((x,y)=(100,50).\\) Refer back to Figure 1.2 for the geometry. Lets verify that \\((x,y)=(100,50)\\) satisfies the original system (1.1). For the first equation, \\[x+y=100+50=150\\,\\checkmark\\] For the second equation, \\[12x+6y=12\\cdot 100+6\\cdot 50=1200+300=1500\\,\\checkmark\\] Example 1.1 Solve \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+2y&amp;=&amp;3\\\\ 2x-3y&amp;=&amp;10 \\end{array}\\right. . \\end{equation*}\\] If we add \\(-2\\) times the first equation to the second, we get \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+2y&amp;=&amp;3\\\\ -7y&amp;=&amp;4 \\end{array}\\right. . \\end{equation*}\\] Thus, \\(y=-4/7\\) and \\(x=3-2\\left(-\\frac{4}{7}\\right)=\\frac{21}{7}+\\frac{8}{7}=\\frac{29}{7}.\\) Our solution is \\(\\left(-\\frac{4}{7},\\frac{29}{7}\\right).\\) What could go wrong? Example 1.2 Solve \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} 2x+3y&amp;=&amp;6\\\\ 4x+6y&amp;=&amp;7 \\end{array}\\right. . \\end{equation*}\\] If we multiply the top equation by \\(-2\\) and add it to the second, we get \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} 2x+3y&amp;=&amp;6\\\\ 0x+0y&amp;=&amp;-5 \\end{array}\\right. . \\end{equation*}\\] Since \\(0\\neq -5\\), this equation has no solution, i.e. it is inconsistent. The original lines were parallel. What else could we encounter? Example 1.3 If we tweak the last system a little bit \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} 2x+3y&amp;=&amp;6\\\\ 4x+6y&amp;=&amp;12 \\end{array}\\right. , \\end{equation*}\\] we see that by multiplying the top equation by 2 gives us identical equations. So the two equations represent the same line, and every point on that line is a solution, and there are infinitely many solutions. Parameters When a system of equation has infinitely many solutions, we like to write the solutions in terms of parameters. For instance, in the last example, whose solution was the line \\(2x+3y=6\\), we can let \\(y=t\\) and solve for \\(x\\): \\(2x+3t=6\\) or \\(x=3-\\frac{3}{2}t\\), and our solution is \\[\\left\\{(3-\\frac{3}{2}t,t),t\\in \\mathbb{R}\\right\\}.\\] The last expression can be read as the set of all ordered pairs of the form \\((3-\\frac{3}{2}t,t)\\), where \\(t\\) is an element of the real numbers. Different values of \\(t\\) lead to different solutions. For instance. if \\(t=0,\\) we have \\((x,y)=(3,0),\\) while if \\(t=1,\\) we get \\((x,y)=\\left(\\frac{3}{2},1\\right).\\) Note that we can use any real number for \\(t,\\) not just an integer. Parameterizations are not unique. Letting \\(x=t\\) and solving for \\(y\\) gives us a different-looking solution: \\[\\left\\{(t,2-\\frac{2}{3}t),t\\in \\mathbb{R}\\right\\}.\\] In the next section, we will learn a systematic way to parameterize solutions. More equations, more variables Heres a system of three linear equations in three variables. Example 1.4 Solve \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} 2x-2y+z&amp;=&amp;12\\\\ x+y-2z&amp;=&amp;-1\\\\ 3x+y+2z&amp;=&amp;15 \\end{array} \\right. . \\end{equation*}\\] A single linear equation of the form \\[ax+by+cz=d,\\] where \\(a,b,\\) and \\(c\\) are constants (and not all zero), represents a plane in three dimensions. A solution to this system of equations would lie on the intersection of three planes. Again, the possibilities are zero, one, or infinitely many solutions. We can again use Gaussian elimination to get the system into upper triangular form. It works best if the \\(x\\)-coefficient in the first equation is a 1. If we interchange the first and second equations, we make that happen. \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+y-2z&amp;=&amp;-1\\\\ 2x-2y+z&amp;=&amp;12\\\\ 3x+y+2z&amp;=&amp;15 \\end{array} \\right. . \\end{equation*}\\] We can use the \\(x\\) term in the first equation as a pivot to eliminate the \\(x\\) term from the other two equations. Replace Equation 2 with \\(-2\\times\\) Equation 1 + Equation 2 (\\(-2E1+E2\\to E2\\)), and replace Equation 3 with \\(-3\\times\\) Equation 1 + Equation 3 (\\(-3E1+E3\\to E3\\)). \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+y-2z&amp;=&amp;-1\\\\ -4y+5z&amp;=&amp;14\\\\ -2y+8z&amp;=&amp;18 \\end{array} \\right. . \\end{equation*}\\] Lets divide Equation 3 by \\(-2\\) and then interchange Equations 2 and 3. (-\\(\\frac{1}{2}E3\\to E3,E2\\leftrightarrow E3\\).) \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+y-2z&amp;=&amp;-1\\\\ y-4z&amp;=&amp;-9\\\\ -4y+5z&amp;=&amp;14 \\end{array} \\right. . \\end{equation*}\\] Now well use the \\(y\\) in Equation 2 as a pivot to eliminate the \\(y\\) term in Equation 3. \\((4E2+E3\\to E3.)\\) \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+y-2z&amp;=&amp;-1\\\\ y-4z&amp;=&amp;-9\\\\ -11z&amp;=&amp;-22 \\end{array} \\right. . \\end{equation*}\\] We are now in triangular form, and we can solve immediately for \\(z\\): \\(z=2\\). We can use it in the second equation to solve for \\(y\\): \\(y-4(2)=-9, y=-1\\). Finally, knowing the values of \\(y\\) and \\(z\\) allows us to use Equation 1 to solve for \\(x\\): \\(x+(-1)-2(2)=-1\\) or \\(x=4.\\) Our solution is \\((x,y,z)=(4,-1,2)\\). What would a no-solution case look like? After Gaussian elimination, perhaps we end up with something like this: Example 1.5 \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+2y+4z&amp;=&amp;2\\\\ y-3z&amp;=&amp;1\\\\ 0&amp;=&amp;7 \\end{array} \\right. . \\end{equation*}\\] Similar to the \\(2\\times 2\\) case, a bottom row with \\(0=a\\), where \\(a\\neq 0\\) would indicate an inconsistent system. Example 1.6 By tweaking the last example just a bit, we can arrive at a case with infinitely many solutions. \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+2y+4z&amp;=&amp;2\\\\ y-3z&amp;=&amp;1\\\\ 0&amp;=&amp;0 \\end{array} \\right. . \\end{equation*}\\] The last equation is no longer a problem, but now we basically have more equations than unknowns. Again, well use a parameter. Let \\(z=t\\). Then \\(y-3t=1\\) or \\(y=1+3t\\). Also, \\(x+2(1+3t)+4t=2\\), which leads to \\(x=-10t\\). We write the solution as \\[\\left\\{(-10t,1+3t,t),t\\in\\mathbb{R}\\right\\}.\\] Again, this parameterization is not unique. Because there is a single parameter, our solution is a line in 3 dimensions. Example 1.7 It is also possible after Gaussian elimination to end up with a system that looks like this. \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+4y-3z&amp;=&amp;2\\\\ 0&amp;=&amp;0\\\\ 0&amp;=&amp;0 \\end{array} \\right. \\end{equation*}\\] That is, the three equations actually represent the same plane. In this case we use two parameters, \\(s\\) and \\(t\\) and solve for the third in terms of them. With \\(y=s\\) and \\(z=t\\), we get \\(x=2-4y+3z=2-4s+3t\\). Our solution is \\[\\left\\{(2-4s+3t,s,t),(s,t)\\in \\mathbb{R}^2\\right\\}.\\] Overdetermined and underdetermined systems We dont have to have the same number of equations as variables. When we have more equations than variables, the system is called overdetermined. For example, \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} x+2y&amp;=&amp;3\\\\ 2x-y&amp;=&amp;5\\\\ 3x+y&amp;=&amp;8 \\end{array} \\right. \\end{equation*}\\] has three equations but only two unknowns. Though the system does have a single solution (left as an exercise), overdetermined systems often have no solution. They show up in linear regression and in other cases where there are more observations than there are variables. Underdetermined systems have more variables than equations. For instance, \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+2y-3z&amp;=&amp;1\\\\ y+2z&amp;=&amp;4 \\end{array} \\right. \\end{equation*}\\] is underdetermined. Underdetermined systems dont have to have a solution, but if theres one solution, there are infinitely many. Why? 1.3 Matrices A matrix is a rectangular array of numbers. Matrices can be used to store the coefficients in a system of equations. For instance, this system we saw in the last section, \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} 2x-2y+z&amp;=&amp;12\\\\ x+y-2z&amp;=&amp;-1\\\\ 3x+y+2z&amp;=&amp;15 \\end{array}\\right. \\end{equation*}\\] can be written \\[\\left[\\begin{array}{rrr|r} 2&amp;-2&amp;1&amp;12\\\\ 1 &amp; 1 &amp; -2 &amp; -1\\\\ 3 &amp; 1 &amp; 2 &amp; 15 \\end{array}\\right].\\] This is what we call an augmented matrix, with the constants separated from the coefficients by a vertical line. The coefficient matrix is the matrix to the left of the line. We will be studying important properties of the coefficient matrix in the future. Elementary row operations The three operations on systems of equations from the previous section have matrix equivalents, called elementary row operations. They are as follows. Interchanging two rows Multiplying a row by a nonzero constant Adding a constant multiple of one row to another We can use these three operations to solve a system of equations. (We are basically doing Gaussian elimination, but were not writing down the variables all the time.) Example 1.8 Solve using matrices: \\[ \\left\\{ \\begin{array}{rcl} y-4z&amp;=&amp;8\\\\ 2x-3y+2z&amp;=&amp;1\\\\ 4x-8y+12z&amp;=&amp;1 \\end{array} \\right. \\] The augmented matrix is \\[\\left[\\begin{array}{rrr|r} 0&amp;1&amp;-4&amp;8\\\\ 2 &amp; -3 &amp; 2 &amp; 1\\\\ 4 &amp; -8 &amp; 12 &amp; 1 \\end{array}\\right].\\] Here is one possible sequence of steps. \\[\\left[\\begin{array}{rrr|r} 0&amp;1&amp;-4&amp;8\\\\ 2 &amp; -3 &amp; 2 &amp; 1\\\\ 4 &amp; -8 &amp; 12 &amp; 1 \\end{array}\\right] \\xrightarrow{R1\\leftrightarrow R2} \\left[\\begin{array}{rrr|r} 2 &amp; -3 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; -4 &amp; 8\\\\ 4 &amp; -8 &amp; 12 &amp; 1 \\end{array}\\right]\\] \\[\\xrightarrow{-2R1+R3\\to R3} \\left[\\begin{array}{rrr|r} 2 &amp; -3 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; -4 &amp; 8 \\\\ 0 &amp; -2 &amp; 8 &amp; -1 \\end{array}\\right]\\xrightarrow{2R2+R3\\to R3} \\left[\\begin{array}{rrr|r} 2 &amp; -3 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; -4 &amp; 8 \\\\ 0 &amp; 0 &amp; 0 &amp; 15 \\end{array}\\right]. \\] The last line in the last matrix is equivalent to the equation \\(0=15\\). The system has no solution. (It is inconsistent.) Row echelon form Lets do one last thing to the final matrix in the last example. \\[\\left[\\begin{array}{rrr|r} 2 &amp; -3 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; -4 &amp; 8 \\\\ 0 &amp; 0 &amp; 0 &amp; 15 \\end{array}\\right] \\xrightarrow[\\frac{1}{15}R3\\to R3]{\\frac{1}{2}R1\\to R1} \\left[\\begin{array}{rrr|r} 1 &amp; -\\frac32 &amp; 1 &amp; \\frac12 \\\\ 0 &amp; 1 &amp; -4 &amp; 8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array}\\right] \\] This new matrix satisfies the following properties. All rows consisting entirely of zeros (if any) are at the bottom. The first nonzero entry in each row is 1. This first nonzero entry is called the leading 1. Each leading 1 appears to the right of the leading 1 in any row above it. A matrix satisfying these three properties is said to be in row echelon form. (Note: not everyone requires the first nonzero term in each row to be a 1.) Gaussian elimination on a matrix will leave it in row echelon form. Reduced row echelon form In the process of solving this system in Example 1.4 \\[\\begin{equation*} \\left\\{ \\begin{array}{ccl} 2x-2y+z&amp;=&amp;12\\\\ x+y-2z&amp;=&amp;-1\\\\ 3x+y+2z&amp;=&amp;15 \\end{array} \\right. , \\end{equation*}\\] we reduced it to this \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+y-2z&amp;=&amp;-1\\\\ y-4z&amp;=&amp;-9\\\\ -11z&amp;=&amp;-22 \\end{array} \\right. , \\end{equation*}\\] which has row echelon matrix form \\[\\left[\\begin{array}{rrr|r} 1 &amp; 1 &amp; -2 &amp; -1\\\\ 0 &amp; 1 &amp; -4 &amp; -9\\\\ 0 &amp; 0 &amp; 1 &amp; 2 \\end{array}\\right].\\] We used back substitution to solve the system, but we can also do more with the matrix: \\[\\left[\\begin{array}{rrr|r} 1 &amp; 1 &amp; -2 &amp; -1\\\\ 0 &amp; 1 &amp; -4 &amp; -9\\\\ 0 &amp; 0 &amp; 1 &amp; 2 \\end{array}\\right] \\xrightarrow{-R2+R1\\to R1} \\left[\\begin{array}{rrr|r} 1 &amp; 0 &amp; 2 &amp; 8\\\\ 0 &amp; 1 &amp; -4 &amp; -9\\\\ 0 &amp; 0 &amp; 1 &amp; 2 \\end{array}\\right]\\] \\[\\xrightarrow[4R3+R2\\to R2]{-2R3+R1\\to R1} \\left[\\begin{array}{rrr|r} 1 &amp; 0 &amp; 0 &amp; 4\\\\ 0 &amp; 1 &amp; 0 &amp; -1\\\\ 0 &amp; 0 &amp; 1 &amp; 2 \\end{array}\\right]. \\] We can immediately read off the solution to the system without having to back substitute: \\(x=4,y=-1,z=2\\). This matrix is in what we call reduced row echelon form. A matrix is in reduced row echelon form if it is in row echelon form and each leading 1 is the only nonzero entry in its column. (Note: while not everyone requires leading ones in row echelon form, the standard definition of reduced row echelon form does require them.) The reduced row echelon form of a matrix is unique. The process used to put a matrix into reduced row echelon form is called Gauss-Jordan elimination. Most graphing calculators can put matrices into reduced row echelon form. In the future, we will learn how to do it with R. Example 1.9 Which of the following augmented matrices are in reduced row echelon form? \\[\\mathbf{A}=\\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 2\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 3\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -2\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -3\\end{array}\\right] \\] \\[\\mathbf{B}=\\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 2\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 3\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right] \\] \\[\\mathbf{C}=\\left[\\begin{array}{rrr|r} 1 &amp; 1 &amp; 0 &amp; 2\\\\ 0 &amp; 1 &amp; 0 &amp; 3\\\\ 0 &amp; 0 &amp; 1 &amp; -2\\end{array}\\right] \\] \\[\\mathbf{D}=\\left[\\begin{array}{rrr|r} 1 &amp; 0 &amp; 1 &amp; 3\\\\ 0 &amp; 1 &amp; 1 &amp; 2\\end{array}\\right] \\] Only matrix \\(\\mathbf{C}\\) is not in reduced row echelon form. The leading 1 in the second row has a nonzero value above it. In the second matrix, the leading ones are in columns 1, 2, and 4. After using Gauss-Jordan elimination to get a matrix into reduced row echelon form, we can read the solution to the corresponding system of equations fairly easily. For instance (assuming the columns represent \\(x,y,z,\\) and \\(w\\), respectively), the system of equations corresponding to the first matrix in the last example \\[\\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 2\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 3\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; -2\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -3\\end{array}\\right] \\] has the solution \\(x=2,y=3,z=-2\\), and \\(w=-3\\). What about the solution to this one? \\[\\left[\\begin{array}{rrr|r} 1 &amp; 0 &amp; 1 &amp; 3\\\\ 0 &amp; 1 &amp; 1 &amp; 2\\end{array}\\right] \\] Note that there is no leading 1 in the third column. Variables whose columns have no leading 1 are called free variables and can actually take on any value. The rest of the solution depends on them. Its traditional to assign parameters to these free variables. Here \\(z\\) is free, so we let \\(z=t\\) and solve for the rest. We use the second row to solve for \\(y\\): \\[y+z=2\\to y=2-z\\to y=2-t\\] Similarly, we can use the first row to solve for \\(x\\): \\[x+z=3\\to x=3-z\\to x=3-t\\] Our solution is \\(\\{(3-t,2-t,t),t\\in\\mathbb{R}\\}\\). It turns out that the reduced row echelon form of a matrix is unique, so our system of assigning parameters to the free variables gives us a systematic way of producing consistent parameterizations for solutions. Example 1.10 This matrix also has a free variable. \\[\\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 2\\\\ 0 &amp; 1 &amp; 2 &amp; 0 &amp; 3\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -2\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right] \\] If the variables are \\(x,y,z\\), and \\(w\\), then \\(z\\) is again a free variable. Let \\(z=t\\). Note that \\(w=-2\\), regardless of the value of \\(z\\). We also get \\(x=2-3z=2-3t\\) and \\(y=3-2z=3-2t\\), so the general solution is \\[\\{(2-3t,3-2t,t,-2),t\\in\\mathbb{R}\\}\\] Another nice feature of this approach is that we can solve for the non-free or basic variables in terms of the free variables alone. Example 1.11 What if we have more than one free variable? We saw the following (reduced) system in Example 1.7 \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} x+4y-3z&amp;=&amp;2\\\\ 0&amp;=&amp;0\\\\ 0&amp;=&amp;0 \\end{array} \\right. \\end{equation*}\\] The corresponding matrix is \\[\\left[\\begin{array}{rrr|r} 1 &amp; 4 &amp; -3 &amp; 2\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right].\\] Here, \\(y\\) and \\(z\\) are free variables. In this system, \\(x\\) is the only basic variable. If we let \\(y=s\\) and \\(z=t\\), we can solve for \\(x\\) and get the solution we saw before. Matrices with Technology We use small-scale examples with nice solutions to illustrate the concepts, but in the real world, we use software packages to perform our computations. For small matrices, the Desmos[6] matrix calculator is handy for basic matrix operations, and will even put a matrix in reduced row echelon form with the touch of the rref button. In R, there are a few different ways to create a matrix. The matrix function is one of those ways. M &lt;- matrix(c(2,1,3,-2,1,1,1,-2,2,12,-1,15), nrow = 3) M ## [,1] [,2] [,3] [,4] ## [1,] 2 -2 1 12 ## [2,] 1 1 -2 -1 ## [3,] 3 1 2 15 We have created the augmented matrix that goes with Example 1.4. (Note that neither Desmos nor R will display the augmentation bar. We just have to imagine it being there.) By default, when we feed the matrix function a vector (created by the c function), it will fill the matrix column-by-column. We specified the number of rows to be 3 with the nrow = 3 argument, so when the first column fills up, the next 3 values go in column 2. If we want to enter the values by row, we can add a byrow = TRUE argument. M2 &lt;- matrix(c(2,1,3,-2,1,1,1,-2,2,12,-1,15), byrow = TRUE, nrow = 3) M2 ## [,1] [,2] [,3] [,4] ## [1,] 2 1 3 -2 ## [2,] 1 1 1 -2 ## [3,] 2 12 -1 15 To put a matrix into RREF form, we need the rref function from the pracma package. library(pracma) rref(M) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 4 ## [2,] 0 1 0 -1 ## [3,] 0 0 1 2 With no free variables, we can read off our solution in the 4th column: \\({(x,y,z)=(4,-1,2)}\\). 1.4 Basic Matrix Operations So far, weve only used matrices as shorthand for an underlying linear system. It turns out that matrices have useful algebraic properties, which we will now study. The dimension or size of a matrix is the number of rows and columns. An \\(m\\times n\\) matrix has \\(m\\) rows (horizontal) and \\(n\\) columns (vertical). Here are some examples. \\(\\begin{bmatrix} 2 &amp; 3 &amp; 2\\\\ 1 &amp; 2 &amp; 4 \\end{bmatrix} {\\text{ is } 2\\times 3}\\) \\(\\begin{bmatrix} -1 &amp; 3 &amp; 6 &amp; 2\\\\ 2 &amp; 1 &amp; 0 &amp; 0\\\\ 3 &amp; 5 &amp; 1 &amp; 2 \\end{bmatrix}{\\text{ is } 3\\times 4}\\) \\(\\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; -2 \\end{bmatrix}{\\text{ is } 2\\times 2 \\text{ (a } \\textit{square matrix}\\text{)}}\\) Row and column vectors An \\(m\\times 1\\) matrix is called a column vector. \\[\\begin{bmatrix} 2\\\\4\\\\-1\\\\6\\\\7 \\end{bmatrix} \\] A \\(1\\times n\\) matrix is called a row vector. \\[\\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; 5 &amp; 4 &amp; 8 \\end{bmatrix}\\] Matrix addition Two matrices of the same dimension can be added together. The addition is entry-wise. \\[\\begin{align*} \\begin{bmatrix} 2 &amp; 4 &amp; -1\\\\ 3 &amp; 2 &amp; 3 \\end{bmatrix}+ \\begin{bmatrix} 4 &amp; 2 &amp; 1\\\\ 2 &amp; -2 &amp; 3 \\end{bmatrix}&amp;= \\begin{bmatrix} 2+4 &amp; 4+2 &amp; -1+1\\\\ 3+2 &amp; 2+-2 &amp; 3+3 \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} 6 &amp; 6 &amp; 0\\\\ 5 &amp; 0 &amp; 6 \\end{bmatrix} \\end{align*}\\] Matrix subtraction is similar. Again, the matrices must be of the same size. \\[\\begin{align*} \\begin{bmatrix} 2 &amp; 4 &amp; -1\\\\ 3 &amp; 2 &amp; 3 \\end{bmatrix}- \\begin{bmatrix} 4 &amp; 2 &amp; 1\\\\ 2 &amp; -2 &amp; 3 \\end{bmatrix}&amp;= \\begin{bmatrix} 2-4 &amp; 4-2 &amp; -1-1\\\\ 3-2 &amp; 2-(-2) &amp; 3-3 \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} -2 &amp; 2 &amp; -2\\\\ 1 &amp; 4 &amp; 0 \\end{bmatrix} \\end{align*}\\] Scalar Multiplication A matrix can be multiplied by a constant. This process is called scalar multiplication to avoid confusion with matrix multiplication, which we will see in a bit. In scalar multiplication, each entry in the matrix is multiplied by the constant. For instance \\[2\\begin{bmatrix} 2 &amp; 1\\\\-3 &amp; 4 \\end{bmatrix}= \\begin{bmatrix} 2\\cdot 2 &amp; 2\\cdot 1\\\\ 2\\cdot (-3) &amp; 2\\cdot 4 \\end{bmatrix}= \\begin{bmatrix} 4 &amp; 2\\\\-6 &amp; 8 \\end{bmatrix}.\\] The additive inverse of a matrix \\(\\mathbf{A}\\) is \\(-\\mathbf{A}=(-1)\\mathbf{A}\\). All of these operations are easily performed in Desmos and R. With \\(\\mathbf{M}\\) and \\(\\mathbf{M2}\\) from before, we get the following. M ## [,1] [,2] [,3] [,4] ## [1,] 2 -2 1 12 ## [2,] 1 1 -2 -1 ## [3,] 3 1 2 15 M2 ## [,1] [,2] [,3] [,4] ## [1,] 2 1 3 -2 ## [2,] 1 1 1 -2 ## [3,] 2 12 -1 15 M+M2 ## [,1] [,2] [,3] [,4] ## [1,] 4 -1 4 10 ## [2,] 2 2 -1 -3 ## [3,] 5 13 1 30 M-M2 ## [,1] [,2] [,3] [,4] ## [1,] 0 -3 -2 14 ## [2,] 0 0 -3 1 ## [3,] 1 -11 3 0 2*M ## [,1] [,2] [,3] [,4] ## [1,] 4 -4 2 24 ## [2,] 2 2 -4 -2 ## [3,] 6 2 4 30 -M ## [,1] [,2] [,3] [,4] ## [1,] -2 2 -1 -12 ## [2,] -1 -1 2 1 ## [3,] -3 -1 -2 -15 Make sure to use the * for any scalar multiplication. The \\(m\\times n\\) zero matrix \\(\\mathbf{0}\\) is the \\(m\\times n\\) matrix with all zero entries. For instance, the \\(2\\times 3\\) zero matrix is \\[\\mathbf{0}= \\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 \\end{bmatrix}.\\] Properties of matrix arithmetic Let \\(\\mathbf{A}, \\mathbf{B},\\) and \\(\\mathbf{C}\\) be any \\(m\\times n\\) matrices, and let \\(\\mathbf{0}\\) be the \\(m\\times n\\) zero matrix. We have the following properties. Commutative property: \\(\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A}\\) Associative property: \\(\\mathbf{A}+(\\mathbf{B}+\\mathbf{C})=(\\mathbf{A}+\\mathbf{B})+\\mathbf{C}\\) Identity property of addition: \\(\\mathbf{A}+\\mathbf{0}=\\mathbf{0}+\\mathbf{A}=\\mathbf{A}\\) Inverse property of addition: \\(\\mathbf{A}+(-\\mathbf{A})=(-\\mathbf{A})+\\mathbf{A}=\\mathbf{0}\\) These properties follow from the analogous properties for real numbers. Matrix multiplication It would be simple to define the product of two matrices in a similar manner. (In fact, the * operator in R does exactly that.) It turns out that a different definition is more useful for applications, but it will take a little motivation. Example 1.12 A movie theater sold 200 tickets for a screening. Adult tickets cost $12, senior tickets cost $8, and childrens tickets cost $6 each. The theater received $1,960 in revenue and sold 10 more senior tickets than childrens tickets. How many of each type of ticket were sold? Let \\(x,y,\\) and \\(z\\) represent the number of adult, senior, and childrens tickets sold, respectively. There are three equations here. Tickets Sold: \\(x+y+z=200\\) Revenue: \\(12x+8y+6z=1960\\) Senior/child: \\(y-z=10\\) The augmented matrix for the system and its reduced row echelon form look like this. \\[\\left[\\begin{array}{ccc|l} 1&amp;1&amp;1&amp;200\\\\12 &amp; 8 &amp; 6 &amp; 1960\\\\0 &amp; 1 &amp; -1 &amp; 10\\end{array}\\right]\\leadsto \\left[\\begin{array}{ccc|l} 1&amp;0&amp;0&amp;110\\\\0 &amp; 1 &amp; 0 &amp; 50\\\\0 &amp; 0 &amp; 1 &amp; 40\\end{array}\\right].\\] The theater sold 110 adult tickets, 50 senior tickets, and 40 childrens tickets. Lets store these numbers in a column vector. \\[\\mathbf{v}=\\begin{bmatrix} 110\\\\50\\\\40\\end{bmatrix}\\] Lets look at the coefficient matrix, this solution vector, and the augmented vector together. \\[\\begin{bmatrix}1&amp;1&amp;1\\\\12 &amp; 8 &amp; 6\\\\0 &amp; 1 &amp; -1 \\end{bmatrix},\\begin{bmatrix} 110\\\\50\\\\40\\end{bmatrix},\\text{ and }\\begin{bmatrix} 200\\\\1960\\\\10\\end{bmatrix}\\] We can get the revenue by combining the middle row and the solution vector: \\[\\text{Revenue} = 12\\cdot110+8\\cdot 50+6\\cdot 40=1960\\] \\[\\begin{bmatrix}&amp;&amp;\\\\12 &amp; 8 &amp; 6\\\\ &amp; &amp; \\end{bmatrix},\\begin{bmatrix} 110\\\\50\\\\40\\end{bmatrix},\\begin{bmatrix} \\\\1960\\\\ \\\\ \\end{bmatrix}\\] You might recognize this as a dot product of vectors: \\[(a_1,a_2,a_3)\\cdot (b_1,b_2,b_3)=a_1b_1+a_2b_2+a_3b_3.\\] The total revenue, the second entry in the augmented vector is the dot product of the second row of the coefficient matrix and the solution vector. Similarly, the first entry (ticket sales) is the dot product of the first row and the solution vector, and the third entry is the dot product of the third row and the solution vector. This is how we are going to define the product of a matrix and a vector. \\[\\begin{bmatrix}1&amp;1&amp;1\\\\12 &amp; 8 &amp; 6\\\\0 &amp; 1 &amp; -1 \\end{bmatrix}\\begin{bmatrix} 110\\\\50\\\\40\\end{bmatrix}=\\begin{bmatrix} 200\\\\1960\\\\10\\end{bmatrix}\\] Heres an another example Example 1.13 \\[ \\underbrace{\\begin{bmatrix} 2 &amp; 4 &amp; 1 &amp; 5\\\\ 1 &amp; -3 &amp; 2 &amp; 4\\end{bmatrix}}_{2\\times 4}\\underbrace{\\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\\\ 3 \\end{bmatrix}}_{4\\times 1}=\\begin{bmatrix} 2\\cdot 3+4\\cdot (-1)+1\\cdot 2+5\\cdot 3\\\\ 1\\cdot 3+(-3)\\cdot (-1)+2\\cdot 2+4\\cdot 3 \\end{bmatrix} =\\underbrace{\\begin{bmatrix} 19\\\\22\\end{bmatrix}}_{2\\times 1} \\] Multiplying two matrices Having defined what it means to multiply a vector by a matrix, we can now talk about multiplying two matrices. \\[\\underbrace{\\begin{bmatrix} 2 &amp; 4 &amp; 1 &amp; 5\\\\ 1 &amp; -3 &amp; 2 &amp; 4 \\end{bmatrix}}_{2\\times 4} \\underbrace{\\begin{bmatrix} 1 &amp; 2\\\\ -2 &amp; 3\\\\ 2 &amp; 2\\\\ -1 &amp; 4 \\end{bmatrix}}_{4\\times 2}= \\underbrace{ \\begin{bmatrix} -9 &amp; 38 \\\\ 7 &amp; 13\\end{bmatrix}}_{2\\times 2}\\] If \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n\\times k\\) matrix, then their product \\(\\mathbf{A}\\mathbf{B}\\) is an \\(m\\times k\\) matrix whose \\(i\\)th column is the product of \\(\\mathbf{A}\\) and the \\(i\\)th column of \\(\\mathbf{B}\\). The first column in the product is \\[\\begin{bmatrix} 2 &amp; 4 &amp; 1 &amp; 5\\\\ 1 &amp; -3 &amp; 2 &amp; 4 \\end{bmatrix} \\begin{bmatrix} 1 &amp; *\\\\ -2 &amp; *\\\\ 2 &amp; *\\\\ -1 &amp; * \\end{bmatrix}= \\begin{bmatrix} -9 &amp; * \\\\ 7 &amp; *\\end{bmatrix} \\] and the second column is \\[\\begin{bmatrix} 2 &amp; 4 &amp; 1 &amp; 5\\\\ 1 &amp; -3 &amp; 2 &amp; 4 \\end{bmatrix} \\begin{bmatrix} * &amp; 2\\\\ * &amp; 3\\\\ * &amp; 2\\\\ * &amp; 4 \\end{bmatrix}= \\begin{bmatrix} * &amp; 38 \\\\ * &amp; 13\\end{bmatrix}.\\] Example 1.14 Multiply \\[\\begin{bmatrix} 2 &amp; -1 \\\\ 3 &amp; -2\\end{bmatrix} \\begin{bmatrix}1 &amp; 2\\\\3 &amp; 4\\end{bmatrix}.\\] Lets introduce some notation. Suppose \\(\\mathbf{C}\\) is an \\(m\\times n\\) matrix. We denote the row \\(i\\), column \\(j\\) entry of \\(\\mathbf{C}\\) by \\(c_{ij}\\). If \\(\\mathbf{C}=\\mathbf{A}\\mathbf{B}\\), then \\(c_{ij}\\) is the dot product of row \\(i\\) of \\(\\mathbf{A}\\) and column \\(j\\) of \\(\\mathbf{B}\\). The row 1, column 1 entry in our product is \\[\\begin{bmatrix} 2 &amp; -1 \\\\ * &amp; *\\end{bmatrix} \\begin{bmatrix}1 &amp; *\\\\3 &amp; *\\end{bmatrix}=\\begin{bmatrix} 2\\cdot 1+(-1)\\cdot 3 &amp; *\\\\ * &amp; *\\end{bmatrix}=\\begin{bmatrix} -1 &amp; *\\\\ * &amp; *\\end{bmatrix}\\] Pairing all of the other possible rows and columns gives us \\[\\begin{bmatrix} 2 &amp; -1 \\\\ 3 &amp; -2\\end{bmatrix} \\begin{bmatrix}1 &amp; 2\\\\3 &amp; 4\\end{bmatrix}=\\begin{bmatrix}-1 &amp; 0\\\\-3 &amp; -2\\end{bmatrix}\\] You cant just multiply any pair of matrices. For example \\[\\underbrace{\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\2 &amp; 4 &amp; 1\\end{bmatrix}}_{2\\times 3}\\underbrace{\\begin{bmatrix} 2&amp; 3\\\\1 &amp; 4\\end{bmatrix}}_{2\\times 2}\\] doesnt work, because the number of columns on the left and the number of rows on the right dont match up. What happens when we multiply the two matrices from Example 1.14, but in the opposite order? \\[\\begin{bmatrix}1 &amp; 2\\\\3 &amp; 4\\end{bmatrix} \\begin{bmatrix} 2 &amp; -1 \\\\ 3 &amp; -2\\end{bmatrix} =\\begin{bmatrix} 8 &amp; -5 \\\\18 &amp; -11 \\end{bmatrix}.\\] We get a different answer. Matrix multiplication is not commutative. In fact, \\[\\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\2 &amp; 1 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 &amp; -1\\\\ 2 &amp; 1 &amp; 4 \\\\1 &amp; -1 &amp; -1 \\end{bmatrix}= \\begin{bmatrix} 6 &amp; 3 &amp; 6\\\\7 &amp; 2 &amp; -1 \\end{bmatrix},\\] but \\[\\begin{bmatrix} 1 &amp; 2 &amp; -1\\\\ 2 &amp; 1 &amp; 4 \\\\1 &amp; -1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 &amp; 1\\\\2 &amp; 1 &amp; 3\\end{bmatrix} \\] isnt even defined. Properties of Matrix Multiplication If \\(\\mathbf{A},\\mathbf{B},\\) and \\(\\mathbf{C}\\) are matrices such that all the indicated sums and products exist and \\(r\\) is a scalar, then the following properties are satisfied. Associative: \\(\\mathbf{A}(\\mathbf{BC})=(\\mathbf{AB})\\mathbf{C}\\) Distributive: \\(\\mathbf{A}(\\mathbf{B}+\\mathbf{C})=\\mathbf{AB}+\\mathbf{AC}\\) \\((\\mathbf{A}+\\mathbf{B})\\mathbf{C}=\\mathbf{AC}+\\mathbf{BC}\\) Scalar Commutativity: \\(\\mathbf{A}(r\\mathbf{B})=r\\mathbf{A}\\mathbf{B}\\) These properties are a little harder to prove, but note that associativity means that we can multiply three or more matrices by multiplying two at a time, and the pairing does not matter. Identity Matrices The main diagonal, or more simply, the diagonal of a square (\\(n\\times n\\)) matrix \\(\\mathbf{A}\\) are the entries of the form \\(a_{ii}\\). That is, they are the entries whose row and column indices match. The diagonal of this matrix is highlighted. \\[\\begin{bmatrix} \\mathbf{3} &amp; 2 &amp; 1\\\\ 4 &amp; \\mathbf{-3} &amp; 1\\\\4 &amp; 2 &amp; \\mathbf{9}\\end{bmatrix}\\] The \\(n\\times n\\) identity matrix \\(\\mathbf{I}\\) has ones along its main diagonal and zeros everywhere else. For example \\(2\\times 2:\\mathbf{I}=\\displaystyle{\\begin{bmatrix} 1 &amp; 0 \\\\0 &amp; 1\\end{bmatrix}}\\) \\(3\\times 3:\\mathbf{I}=\\displaystyle{\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1 \\end{bmatrix}}\\) If \\(\\mathbf{A}\\) is any matrix, and \\(\\mathbf{I}\\) is the appropriately sized identity matrix, then \\(\\mathbf{AI}=\\mathbf{A}\\). Similarly, \\(\\mathbf{IA}=\\mathbf{A}\\) for the appropriately sized \\(\\mathbf{I}.\\) For example, \\[\\begin{bmatrix} 1 &amp; 0 \\\\0 &amp; 1\\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 &amp; 4\\\\3 &amp; -2 &amp; 6\\end{bmatrix}=\\begin{bmatrix} 2 &amp; 1 &amp; 4\\\\3 &amp; -2 &amp; 6\\end{bmatrix}\\] and \\[\\begin{bmatrix} 2 &amp; 1 &amp; 4\\\\3 &amp; -2 &amp; 6\\end{bmatrix}\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} 2 &amp; 1 &amp; 4\\\\3 &amp; -2 &amp; 6\\end{bmatrix}.\\] Technology In Desmos, we can multiply matrices using *. (It will show a \\(\\cdot\\)). In R, we must use the %*% operator to perform matrix multiplication as we defined it. (As mentioned before, * will multiply the corresponding entries in the two matrices, which is useful for some things, but not for our purposes.) A &lt;- matrix(c(2,1,3,-2,1,1,1,-2,2), nrow = 3) A ## [,1] [,2] [,3] ## [1,] 2 -2 1 ## [2,] 1 1 -2 ## [3,] 3 1 2 B &lt;- matrix(c(1,2,3,5,1,2), nrow = 3) B ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 1 ## [3,] 3 2 A%*%B ## [,1] [,2] ## [1,] 1 10 ## [2,] -3 2 ## [3,] 11 20 B%*%A ## Error in B %*% A: non-conformable arguments The transpose of a matrix One operation on a matrix that will come in handy in the future is the transpose. Definition 1.1 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. The transpose of \\(\\mathbf{A}\\), denoted \\(\\mathbf{A}^T,\\) is the \\(n\\times m\\) matrix whose rows are the columns of \\(\\mathbf{A}\\) and whose columns are the rows of \\(\\mathbf{A}\\). Here are some examples. If \\[\\mathbf{A}=\\begin{bmatrix} a &amp; b\\\\c &amp; d\\end{bmatrix},\\] then \\[\\mathbf{A}^T=\\begin{bmatrix} a&amp; c\\\\b&amp; d\\end{bmatrix}.\\] If \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 2 &amp; 3\\\\4 &amp; 5 &amp; 6\\end{bmatrix},\\] then \\[\\mathbf{A}^T=\\begin{bmatrix} 1 &amp; 4\\\\2 &amp; 5\\\\3 &amp; 6\\end{bmatrix}.\\] Properties of the Transpose If and are matrices of compatible sizes, and \\(c\\) is a constant, then we have the following properties. \\(\\left(\\textbf{A}^T\\right)^T=\\textbf{A}\\) \\((\\textbf{A}+\\textbf{B})^T=\\textbf{A}^T+\\textbf{B}^T\\) \\((c\\textbf{A})^T=c\\textbf{A}^T\\) \\((\\textbf{A}\\textbf{B})^T=\\textbf{B}^T\\textbf{A}^T\\) In R, the function t will take the transpose of a matrix. B &lt;- matrix(c(1,2,3,5,1,2), nrow = 3) B ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 1 ## [3,] 3 2 t(B) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 5 1 2 1.5 Matrix Inverses An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is invertible if there is an \\(n\\times n\\) matrix \\(\\mathbf{A}^{-1}\\) such that \\[\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}.\\] The matrix \\(\\mathbf{A}^{-1}\\) is the inverse of \\(\\mathbf{A}\\). For instance, if \\(\\displaystyle{\\mathbf{A}=\\begin{bmatrix} 2 &amp; 1\\\\1 &amp; 1 \\end{bmatrix}}\\) and \\(\\mathbf{B}=\\displaystyle{\\begin{bmatrix} 1 &amp; -1\\\\-1 &amp; 2 \\end{bmatrix}},\\) lets show that \\(\\mathbf{B}=\\mathbf{A}^{-1}.\\) We have \\[\\mathbf{AB}=\\begin{bmatrix} 2 &amp; 1\\\\1 &amp; 1 \\end{bmatrix}\\begin{bmatrix} 1 &amp; -1\\\\-1 &amp; 2 \\end{bmatrix}=\\begin{bmatrix} 1 &amp; 0 \\\\0 &amp; 1 \\end{bmatrix}=\\mathbf{I},\\] and \\[\\mathbf{BA}=\\begin{bmatrix} 1 &amp; -1\\\\-1 &amp; 2 \\end{bmatrix}\\begin{bmatrix} 2 &amp; 1\\\\1 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} 1 &amp; 0 \\\\0 &amp; 1 \\end{bmatrix}=\\mathbf{I},\\] so \\(\\mathbf{B}=\\mathbf{A}^{-1}\\) (and \\(\\mathbf{A}=\\mathbf{B}^{-1})\\). Finding inverses We can find the inverse of a matrix \\(\\mathbf{A}\\) by solving the matrix equation \\(\\mathbf{AX}=\\mathbf{I}\\). The way we do this is by row reduction with the matrix \\(\\mathbf{A}\\) augmented by the columns of \\(\\mathbf{I}\\). For example, to find \\(\\mathbf{A}^{-1}\\) for \\(\\mathbf{A}=\\displaystyle{\\begin{bmatrix} 2 &amp; 1\\\\1 &amp; 1 \\end{bmatrix}}\\), we row reduce \\[\\left[\\begin{array}{cc|cc} 2 &amp; 1 &amp; 1 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 1 \\end{array}\\right].\\] \\[\\begin{align*}\\left[\\begin{array}{cc|cc} 2 &amp; 1 &amp; 1 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 1 \\end{array}\\right]&amp;\\xrightarrow{R1\\leftrightarrow R2}\\left[\\begin{array}{cc|cc} 1 &amp; 1 &amp; 0 &amp; 1\\\\2 &amp; 1 &amp; 1 &amp; 0 \\end{array}\\right] \\xrightarrow{-2R1+R2\\to R2} \\left[\\begin{array}{cc|cc} 1&amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; -1 &amp; 1 &amp; -2 \\end{array}\\right] \\\\ &amp;\\xrightarrow{-R2\\to R2} \\left[\\begin{array}{cc|cc} 1&amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 2 \\end{array}\\right] \\xrightarrow{-R2+R1\\to R1} \\left[\\begin{array}{rr|rr} 1&amp; 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 &amp; 2 \\end{array}\\right] \\end{align*}\\] and our inverse is \\[\\mathbf{A}^{-1}=\\begin{bmatrix} 1 &amp; -1\\\\-1 &amp; 2 \\end{bmatrix}.\\] Example 1.15 Heres a \\(3\\times 3\\) example, though we will skip the messy bit. Find \\(\\mathbf{A}^{-1}\\) if \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\3 &amp; 1 &amp; 1\\\\1 &amp; 4 &amp; 5\\end{bmatrix}.\\] We augment \\(\\mathbf{A}\\) with the \\(3\\times 3\\) identity matrix and reduce. \\[\\left[\\begin{array}{ccc|ccc} 1 &amp; 2 &amp; 3 &amp; 1 &amp; 0 &amp; 0\\\\3 &amp; 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 4 &amp; 5 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right] \\leadsto \\left[\\begin{array}{ccc|ccc} 1 &amp; 0 &amp; 0 &amp; \\frac{1}{6} &amp; \\frac{1}{3} &amp; -\\frac{1}{6}\\\\0 &amp; 1 &amp; 0 &amp; -\\frac{7}{3} &amp; \\frac{1}{3} &amp; \\frac{4}{3} \\\\ 0 &amp; 0 &amp; 1 &amp; \\frac{11}{6} &amp; -\\frac{1}{3} &amp; -\\frac{5}{6}\\end{array}\\right]\\] Our inverse is \\[\\mathbf{A}^{-1}=\\begin{bmatrix}\\frac{1}{6} &amp; \\frac{1}{3} &amp; -\\frac{1}{6}\\\\-\\frac{7}{3} &amp; \\frac{1}{3} &amp; \\frac{4}{3} \\\\ \\frac{11}{6} &amp;-\\frac{1}{3} &amp; -\\frac{5}{6}\\end{bmatrix}.\\] They arent always pretty. Not all square matrices have inverses. For instance, if \\(\\mathbf{A}=\\displaystyle{\\begin{bmatrix} 1 &amp; 2\\\\ -2 &amp; -4 \\end{bmatrix}}\\), when we try to find our inverse, we get \\[\\left[\\begin{array}{rr|rr} 1 &amp; 2 &amp; 1 &amp; 0\\\\ -2 &amp; -4 &amp; 0 &amp; 1 \\end{array}\\right]\\xrightarrow{2R1+R2\\to R2} \\left[\\begin{array}{rr|rr} 1 &amp; 2 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 2 &amp; 1 \\end{array}\\right]\\] and were stuck. This matrix has no inverse. Noninvertible matrices are also called singular. They play an important role in Linear Algebra. A formula for 2 x 2 matrices Using row reduction on the augmented matrix \\[\\left[\\mathbf{A}|\\mathbf{I}\\right]=\\left[\\begin{array}{rr|rr} a &amp; b &amp; 1 &amp; 0\\\\ c &amp; d &amp; 0 &amp; 1 \\end{array}\\right] \\] leads to the following formula for \\(\\mathbf{A}^{-1}\\). Well state it as our first theorem. Theorem 1.1 If \\(\\mathbf{A}=\\displaystyle{\\begin{bmatrix}a &amp; b\\\\c &amp; d\\end{bmatrix}}\\) and \\(ad-bc\\neq 0\\), then \\(\\mathbf{A}\\) is invertible and \\[\\mathbf{A}^{-1}=\\frac{1}{ad-bc}\\begin{bmatrix} d &amp; -b\\\\-c &amp; a \\end{bmatrix}.\\tag{1.3}\\] This result is very handy. Here are some examples. Example 1.16 Let \\(\\mathbf{A}=\\displaystyle{\\begin{bmatrix} 2 &amp; -1 \\\\ 3 &amp; 4 \\end{bmatrix}}.\\) Then \\[\\mathbf{A}^{-1}=\\frac{1}{2\\cdot 4-(-1)\\cdot 3}\\begin{bmatrix} 4 &amp; 1\\\\-3 &amp; 2 \\end{bmatrix}=\\frac{1}{11}\\begin{bmatrix} 4 &amp; 1\\\\-3 &amp; 2 \\end{bmatrix}=\\begin{bmatrix} \\frac{4}{11} &amp; \\frac{1}{11}\\\\ -\\frac{3}{11} &amp; \\frac{2}{11} \\end{bmatrix}.\\] Example 1.17 If \\(\\mathbf{B}=\\displaystyle{\\begin{bmatrix} 2 &amp; 3\\\\6 &amp; 9 \\end{bmatrix}},\\) then \\[\\mathbf{B}^{-1}=\\frac{1}{2\\cdot 9-6\\cdot 3}\\begin{bmatrix} 9 &amp; -3 \\\\ -6 &amp; 2 \\end{bmatrix}=\\frac{1}{0}\\begin{bmatrix} 9 &amp; -3 \\\\ -6 &amp; 2 \\end{bmatrix}\\dots\\] This matrix is not invertible. Note that the second row of \\(\\mathbf{B}\\) is a multiple of the first row. The second column is also a multiple of the first column. For singular \\(2\\times 2\\) matrices, this is always the case. Its more complicated for bigger matrices. Speaking of larger matrices, there is a general formula for the inverse of an \\(n\\times n\\) matrix, but it involves concepts that we are not going to get to. You might see it in a higher level Linear Algebra or Matrix Algebra course. As is usually the case, most of the time we will be using technology to find inverses. Desmos has an inverse matrix button. In R, we use the multipurpose solve function. A &lt;- matrix(c(2,1,3,-2,1,1,1,-2,2), nrow = 3) A ## [,1] [,2] [,3] ## [1,] 2 -2 1 ## [2,] 1 1 -2 ## [3,] 3 1 2 solve(A) ## [,1] [,2] [,3] ## [1,] 0.18181818 0.22727273 0.1363636 ## [2,] -0.36363636 0.04545455 0.2272727 ## [3,] -0.09090909 -0.36363636 0.1818182 The following properties of are helpful. Properties of matrix inverses Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be invertible \\(n\\times n\\) matrices. Then \\(\\mathbf{A}^{-1}\\) is invertible and \\((\\mathbf{A}^{-1})^{-1}=\\mathbf{A}.\\) \\(\\mathbf{A}\\mathbf{B}\\) is invertible and \\((\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}\\). Property 2 is easy to prove. For instance, \\[(\\mathbf{AB})(\\mathbf{B}^{-1}\\mathbf{A}^{-1})=\\mathbf{A}(\\mathbf{B}\\mathbf{B}^{-1})\\mathbf{A}^{-1}=\\mathbf{A}\\mathbf{I}\\mathbf{A}^{-1}=\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}.\\] Showing \\[(\\mathbf{B}^{-1}\\mathbf{A}^{-1})(\\mathbf{AB})=\\mathbf{I}\\] is similar. 1.6 Determinants In Section 1.5, we learned that the \\(2\\times 2\\) matrix \\[\\begin{bmatrix} a &amp; b\\\\c&amp; d \\end{bmatrix}\\] is invertible if and only if \\(ad-bc\\neq 0\\). We give this value a name. Definition 1.2 The \\(\\mathbf{determinant}\\) of the \\(2\\times 2\\) matrix \\[\\mathbf{A}=\\begin{bmatrix} a &amp; b\\\\c&amp; d \\end{bmatrix},\\] denoted \\(\\det \\mathbf{A}\\) or \\(|\\mathbf{A}|\\), is \\(ad-bc\\). Here are some examples. Example 1.18 \\[\\begin{vmatrix} 2 &amp; 4\\\\3 &amp; 1 \\end{vmatrix}=2\\cdot 1-4\\cdot 3=-10\\] The matrix \\(\\begin{bmatrix}2 &amp; 4\\\\3 &amp; 1 \\end{bmatrix}\\) is invertible. Example 1.19 \\[\\begin{vmatrix} 4 &amp; 1\\\\ 8 &amp; 2 \\end{vmatrix}=4\\cdot 2-1\\cdot 8=0\\] The corresponding matrix is not invertible. Recall that another name for a noninvertible matrix is singular. Larger square matrices also have determinants, though it can get complicated. Heres a trick for a \\(3\\times 3\\) matrix. If \\[\\mathbf{A}= \\begin{bmatrix} a_{11}&amp;a_{12}&amp;a_{13}\\\\a_{21}&amp;a_{22}&amp;a_{23}\\\\a_{31}&amp;a_{32}&amp;a_{33} \\end{bmatrix}\\] augment the matrix by adding copies of the first two columns on the right. \\[\\left|\\begin{array}{rrr|rr} a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{11}&amp;a_{12}\\\\a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{21}&amp;a_{22}\\\\a_{31}&amp;a_{32}&amp;a_{33}&amp;a_{31}&amp;a_{32} \\end{array}\\right|\\] We have \\[\\det\\mathbf{A}=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{31}a_{22}a_{13}-a_{32}a_{23}a_{11}-a_{33}a_{21}a_{12}.\\] The first three terms are the downward (left to right) diagonal products. The second three are the upward (left to right) diagonal products. Downward terms are added and upward terms are subtracted. \\[\\left|\\begin{array}{rrr|rr} a_{11}&amp;a_{12}&amp;a_{13}&amp;&amp;\\\\&amp;a_{22}&amp;a_{23}&amp;a_{21}&amp;\\\\&amp;&amp;a_{33}&amp;a_{31}&amp;a_{32} \\end{array}\\right|\\] \\[\\left|\\begin{array}{rrr|rr} &amp;&amp;a_{13}&amp;a_{11}&amp;a_{12}\\\\&amp;a_{22}&amp;a_{23}&amp;a_{21}&amp;\\\\a_{31}&amp;a_{32}&amp;a_{33}&amp;&amp; \\end{array}\\right|\\] Example 1.20 If \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\0 &amp; 2 &amp; 1\\\\1 &amp; 3 &amp; 0 \\end{bmatrix}\\] \\[\\begin{align*} |\\mathbf{A}|&amp;=(1)(2)(0)+(2)(1)(1)+(3)(0)(3)-(1)(2)(3)-(3)(1)(1)-(0)(0)(2)\\\\ &amp;=0+2+0-6-3-0=-7. \\end{align*}\\] There are formulas for determinants of larger matrices that involve finding determinants of smaller submatrices. For instance, one way to find the determinant of a \\(4\\times 4\\) matrix involves finding four \\(3\\times 3\\) determinants. As usual, we will leave that task for technology. Desmos has a det button. Base R has the det function. A &lt;- matrix(c(2,1,3,-2,1,1,1,-2,2), nrow = 3) A ## [,1] [,2] [,3] ## [1,] 2 -2 1 ## [2,] 1 1 -2 ## [3,] 3 1 2 det(A) ## [1] 22 The relationship between the determinant of a \\(2\\times 2\\) matrix and its invertibility holds for all square matrices. Theorem 1.2 An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is invertible if and only if \\(\\det\\mathbf{A}\\neq 0\\). We also have the following nice theorem involving products. Theorem 1.3 Let \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) be \\(n\\times n\\) matrices. Then \\[\\det(\\mathbf{A}\\mathbf{B})=\\det(\\mathbf{B}\\mathbf{A})=\\det\\mathbf{A}\\det \\mathbf{B}.\\] Well leave proofs of both of these for another course. There are some matrices for which the determinant is easy to compute. Triangular and diagonal matrices We defined the diagonal or main diagonal of a square matrix in Section 1.4 when we introduced identity matrices. A diagonal matrix is a square matrix whose off-diagonal terms are all zero. For example \\[\\mathbf{A}=\\begin{bmatrix} 2 &amp; 0 &amp; 0\\\\0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 4\\end{bmatrix}\\] is a diagonal matrix. The determinant of a diagonal matrix is the product of the diagonal entries. This is clear in the \\(2\\times 2\\) case: \\[\\begin{vmatrix} a &amp; 0\\\\0 &amp; d\\end{vmatrix}=ad-0=ad.\\] In the \\(3\\times 3\\) case, with the trick we learned, the the only downward or upward diagonal product that doesnt contain a zero is the main diagonal. For our matrix \\(\\mathbf{A}\\) above, we have \\(\\det \\mathbf{A}=(2)(-1)(4)=-8.\\) Upper and lower triangular matrices A square matrix is called upper triangular if all of the entries below the main diagonal are zero. A matrix is lower triangular if all of the entries above the main diagonal are zero. For example \\[\\mathbf{U}=\\begin{bmatrix} 2 &amp; 1 &amp; 3\\\\0 &amp; 5 &amp; 1\\\\0 &amp; 0 &amp; 2\\end{bmatrix} \\text{ and } \\mathbf{L}=\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\2 &amp; -1 &amp; 0\\\\4 &amp; 5 &amp; 6\\end{bmatrix}\\] are upper and lower triangular, respectively. The determinant of a triangular matrix is also the product of the diagonal entries, so \\[\\det\\mathbf{U}=20 \\text{ and } \\det\\mathbf{L}=-6.\\] One last fact about determinants that we will state but not prove is the following. Theorem 1.4 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) matrix. Then \\(\\det \\mathbf{A}^T=\\det\\mathbf{A}.\\) This is clearly true for \\(2\\times 2\\) matrices, as \\[\\begin{vmatrix}a &amp; b\\\\c&amp; d\\end{vmatrix}=ad-bc=ad-cb=\\begin{vmatrix}a &amp; c\\\\b&amp; d\\end{vmatrix}.\\] 1.7 Systems of Linear Equations as Matrix Equations Consider the product \\[\\begin{align*} \\mathbf{A}\\mathbf{x}&amp;=\\begin{bmatrix} a_{11}&amp;a_{12}&amp;a_{13}\\\\a_{21}&amp;a_{22}&amp;a_{23}\\\\a_{31}&amp;a_{32}&amp;a_{33} \\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} a_{11}x_1+a_{12}x_2+a_{13}x_3\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3 \\end{bmatrix} \\end{align*}\\] The entries in \\(\\mathbf{A}\\mathbf{x}\\) make up the left hand side of a system of equations! We can write a system of linear equations as a matrix equation. The system \\[\\begin{align*} a_{11}x_1+a_{12}x_2+a_{13}x_3&amp;=b_1\\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3&amp;=b_2\\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3&amp;=b_3 \\end{align*}\\] can be written \\[\\begin{bmatrix} a_{11}&amp;a_{12}&amp;a_{13}\\\\a_{21}&amp;a_{22}&amp;a_{23}\\\\a_{31}&amp;a_{32}&amp;a_{33} \\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}b_1\\\\b_2\\\\b_3\\end{bmatrix}\\] or \\[\\mathbf{A}\\mathbf{x}=\\mathbf{b}.\\] If \\(\\mathbf{A}\\) is an invertible \\(n\\times n\\) matrix, then multiplying both sides of the equation on the left by \\(\\mathbf{A}^{-1}\\) gives us our solution. \\[\\begin{align*} \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})&amp;=\\mathbf{A}^{-1}\\mathbf{b}\\\\ \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b} \\end{align*}\\] Theorem 1.5 Let \\(\\mathbf{A}\\) be an invertible \\(n\\times n\\) matrix. Then the matrix equation \\[\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\] has the unique solution \\[\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}.\\] This theorem, together with Theorem 1.1 makes it easy to solve \\(2\\times 2\\) systems. Example 1.21 Solve \\[\\begin{bmatrix}3&amp;1\\\\2&amp;1\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix}4\\\\3\\end{bmatrix}.\\] If \\(\\mathbf{A}=\\begin{bmatrix}3&amp;1\\\\2&amp;1\\end{bmatrix}\\), then \\[\\mathbf{A}^{-1}=\\frac{1}{3\\cdot 1-1\\cdot 2}\\begin{bmatrix}1 &amp; -1\\\\-2 &amp; 3\\end{bmatrix}=\\begin{bmatrix}1 &amp; -1\\\\-2 &amp; 3\\end{bmatrix},\\] so \\[\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix}1 &amp; -1\\\\-2 &amp; 3\\end{bmatrix}\\begin{bmatrix}4\\\\3\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\end{bmatrix}.\\] There are a few things to keep in mind. It is often more efficient for machines to solve a system of equations by row reduction than by taking inverses. If \\(\\mathbf{A}\\) is singular, there still can be a solution (actually, infinitely many). Row reduction will always work. Here is a little trick to help avoid the messiness of fractions in systems. Example 1.22 Solve \\[\\begin{bmatrix} 5 &amp; 2\\\\3 &amp; 4\\end{bmatrix}\\begin{bmatrix}x\\\\y\\end{bmatrix}=\\begin{bmatrix} 4\\\\2\\end{bmatrix}.\\] By Theorem 1.5, \\[\\begin{align*} \\begin{bmatrix}x\\\\y\\end{bmatrix}&amp;=\\frac{1}{5\\cdot4-3\\cdot2}\\begin{bmatrix}4 &amp; -2\\\\-3 &amp; 5\\end{bmatrix}\\begin{bmatrix} 4\\\\2\\end{bmatrix}\\\\ &amp;=\\frac{1}{14}\\begin{bmatrix}12\\\\-2\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}\\frac{6}{7}\\\\-\\frac{1}{7}\\end{bmatrix}. \\end{align*}\\] By leaving the fraction from the inverse outside of the matrix, the matrix multiplication was much easier. How does row reduction work? To understand how the elementary row operations lead to solutions of systems of linear equations, we are going to consider elementary matrices. An elementary matrix is a square matrix that performs an elementary row operation by left multiplication. For example, if \\[\\mathbf{E}=\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\2 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix},\\] and \\[\\mathbf{A}=\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33}\\end{bmatrix},\\] is an arbitrary \\(3\\times 3\\) matrix, then \\[\\begin{align*} \\mathbf{E}\\mathbf{A}&amp;=\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\2 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\a_{21} &amp; a_{22} &amp; a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33}\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\2a_{11}+a_{21} &amp; 2a_{12}+a_{22} &amp; 2a_{13}+a_{23}\\\\ a_{31} &amp; a_{32} &amp; a_{33}\\end{bmatrix}. \\end{align*}\\] Multiplication on the left by the elementary matrix \\(\\mathbf{E}\\) added twice row 1 to row 2 of \\(\\mathbf{A}.\\) In fact, multiplication on the left by this matrix \\(\\mathbf{E}\\) will perform that elementary row operation on any matrix with three rows. Every elementary row operation can be performed by multiplication on the left by one of these elementary matrices. How do we find the right matrix for the job? Note what multiplication by \\(\\mathbf{E}\\) does to the identity matrix of the same size: \\[\\mathbf{E}\\mathbf{I}=\\mathbf{E}.\\] To figure out the right elementary matrix to use, figure out what it does to the identity matrix. For instance, if we want to interchange rows 2 and 4 of a \\(4\\times 4\\) matrix, we just interchange rows 2 and 4 of the \\(4\\times 4\\) identity: \\[\\mathbf{I}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\\\mathbf{0} &amp; \\mathbf{1} &amp; \\mathbf{0} &amp; \\mathbf{0}\\\\0 &amp; 0 &amp; 1 &amp; 0\\\\\\mathbf{0} &amp; \\mathbf{0} &amp;\\mathbf{0} &amp;\\mathbf{1}\\end{bmatrix},\\mathbf{E}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\\\mathbf{0} &amp; \\mathbf{0} &amp;\\mathbf{0} &amp;\\mathbf{1}\\\\0 &amp; 0 &amp; 1 &amp; 0\\\\\\mathbf{0} &amp; \\mathbf{1} &amp; \\mathbf{0} &amp; \\mathbf{0}\\end{bmatrix}.\\] If we want to multiply row 2 of a \\(2\\times 2\\) matrix by \\(-3\\), the elementary matrix that does that is \\[\\mathbf{E}=\\begin{bmatrix} 1 &amp; 0 \\\\-3\\cdot 0 &amp; -3\\cdot 1\\end{bmatrix}=\\begin{bmatrix} 1&amp;0\\\\0&amp;-3\\end{bmatrix}.\\] When we perform elementary row operations on an augmented matrix \\(\\left[\\begin{array}{c|c}\\mathbf{A} &amp; \\mathbf{b}\\end{array}\\right],\\) we are basically performing the same operations on the coefficient matrix \\(\\mathbf{A}\\) and the vector of constants \\(\\mathbf{b}\\) simultaneously. This is equivalent to multiplying both sides of the equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) on the left by elementary matrices. Moreover, if \\(\\mathbf{x}\\) is a solution to \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), then it is also a solution to \\(\\mathbf{E}\\mathbf{A}\\mathbf{x}=\\mathbf{E}\\mathbf{b}\\). This means that when we perform Gaussian elimination on an augmented matrix, we are preserving solutions to the underlying system of equations. Even better, elementary row operations are always invertible! For example, if multiplication by \\[\\mathbf{E}=\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\2 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\] adds twice row one to row 2, its inverse will subtract twice row one from row two: \\[\\mathbf{E}^{-1}=\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\-2 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}.\\] Elementary matrices that perform row interchanges are their own inverses! If you multiply one row by a nonzero constant, you can multiply that same row by the reciprocal of that constant to undo it. Why this talk of inverses? Because, if \\(\\mathbf{x}\\) is a solution to \\((\\mathbf{E}\\mathbf{A})\\mathbf{x}=\\mathbf{E}\\mathbf{b},\\) then \\(\\mathbf{x}\\) is also a solution to \\[\\mathbf{E}^{-1}\\mathbf{E}\\mathbf{A}\\mathbf{x}=\\mathbf{E}^{-1}\\mathbf{E}\\mathbf{b},\\] or \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}.\\) In other words, we do not gain or lose solutions when we perform elementary row operations. When we perform them in a strategic manner, we can get to a place where we can find the complete set of solutions, if there is one. 1.8 Application: The Colley Matrix Method Suppose we want to rank websites or sports teams or individuals. For teams and individuals, if everyone has plays everyone else, we can often use just the winning percentages. Frequently thats not the case, or we need to break ties. For these and other reasons, several different ways of ranking teams based on pairwise comparisons have been developed. Note: for website rankings, we can consider a link from Site A to Site B as a win for Site B. That way, we get similar pairwise comparisons. In this book, we will be considering several different ranking methods. The first one we will look at only requires solving a simple linear system of equations. It is the Colley Matrix Method. Ratings and Rankings The terms rating and ranking are often confused. Here are some definitions. Definition 1.3 A rating is a numerical value that designates a team or individuals quality. A ranking is an ordered list of teams or individuals according to their relative ratings, or a team or individuals position in the overall ranking. Example 1.23 Well be using this example throughout the book. Suppose we have a 5-team college football conference. The results appear below, with the score on the left coming from the team on the left. Aardvarks Beagles Crocs Donkeys Egrets Aardvarks x 17-31 6-42 x 17-7 Beagles x 3-59 10-35 x Crocs x 21-20 49-0 Donkeys x 28-21 Egrets x Here are the standings. (W-L = Win-Loss Record, PCT = Winning Percentage as a decimal, PF = Points For, PA = Points Against, PD = Point Difference, i.e. PF-PA) Team W-L PCT PF PA PD Crocs 4-0 1.000 171 29 142 Donkeys 2-1 0.667 83 52 31 Aardvarks 1-2 0.333 40 80 -40 Beagles 1-2 0.333 44 111 -67 Egrets 0-3 0.000 28 94 -66 The Crocs are on top, followed by the Donkeys. The Aardvarks and Beagles are tied for 3rd, while the Egrets are in the basement. The BCS and Wes Colley The Bowl Championship Series (BCS) involved a consortium of college bowl games. Before the 4-team playoff was instituted, the BCS used a collection of human polls and computer polls to pick the top two teams to play for the national title [7]. One of the stipulations for being included as a computer poll was that the method couldnt take margin of victory into consideration. One of the computer polls chosen was the Colley Matrix Method [8], developed by physicist Wes Colley. Note: the following discussion goes through the derivation of the Colley Matrix Method. The actual matrices involved at the end are fairly straightforward. The first step in Colleys method is to use what is known as Laplaces rule of succession [see 9] to adjust a teams winning percentage. If team \\(i\\) has \\(w_i\\) wins and \\(l_i\\) losses, the adjusted rating \\(r_i\\) is \\[r_i=\\frac{w_i+1}{\\underbrace{w_i+l_i}_{t_i}+2}=\\frac{w_i+1}{t_i+2}.\\] It basically adds one win and one loss to each teams record. It moves everyones rating towards 0.500. The next step involves an algebraic trick. We can rewrite a teams win total as \\[\\begin{align*} w_i&amp;=\\frac{w_i-l_i}{2}+\\frac{w_i+l_i}{2}\\\\ &amp;=\\frac{w_i-l_i}{2}+\\frac{t_i}{2}\\\\ &amp;=\\frac{w_i-l_i}{2}+\\sum_{j=1}^{t_i}\\frac{1}{2}. \\end{align*}\\] The \\(\\Sigma\\) in the last line means add \\(t_i\\) copies of \\(\\frac{1}{2}\\). One way to think about the \\(\\frac{1}{2}\\) term is as the average rating of all teams. Colleys big idea is to replace it with the ratings of the actual teams faced by team \\(i\\). Let \\(O_i\\) be the collection of teams played by team \\(i\\). (If a team plays an opponent more than once, it appears in \\(O_i\\) that number of times.) The adjusted win total based on strength of opponents, \\(w_i^*,\\) is now \\[w^*_i=\\frac{w_i-l_i}{2}+\\sum_{j\\in O_i}r_j.\\] We use this to adjust team \\(i\\)s rating in the rule of succession: \\[r_i=\\frac{1+(w_i-l_i)/2+\\sum_{j\\in O_i}r_j}{2+t_i}.\\] This looks a little unwieldy, so lets consider an individual team. Suppose team 1 plays teams 2, 3, and 5 in a 5-team league, winning one and losing two. Then their rating is \\[\\begin{align*} r_1&amp;=\\frac{1+(1-2)/2+(r_2+r_3+r_5)}{2+3}\\\\ &amp;=\\frac{\\frac{1}{2}+r_2+r_3+r_5}{5} \\end{align*}\\] Multiplying both sides by 5 and rearranging gives us \\[5r_1-r_2-r_3-r_5=\\frac{1}{2},\\] or \\[5r_1-r_2-r_3+0r_4-r_5=\\frac{1}{2}.\\] This equation is one linear equation. There are four more for teams 2-5. We get a system of linear equations that we can solve to get the ratings. What do the coefficients look like? Lets look at our example equation. The 5 in front of \\(r_1\\) is 2 more than the number of games team 1 played, i.e. \\(2+t_1\\). Each team that team 1 played has a \\(-1\\) in front of it. Note: if the teams play \\(n\\) times, the coefficient would be \\(-n\\). Teams that didnt play team 1 have a 0 coefficient. The right hand side is \\(1+(w_1-l_1)/2\\). The Colley Matrix The Colley Matrix \\(\\mathbf{C}\\) is the the coefficient matrix. The entries in \\(\\mathbf{C}\\) are given by \\[\\mathbf{C}_{ij}=\\begin{cases} 2+t_i,&amp; i=j \\text{ (diagonal entries)}\\\\-n_{ij}, &amp;i\\neq j\\end{cases},\\] where \\(n_{ij}\\) is the number of times team \\(i\\) and team \\(j\\) have played each other. The right hand side vector \\(\\mathbf{b}\\) has entries \\(b_i=1+(w_i-l_i)/2\\). The system of equations to solve in matrix form is \\[\\mathbf{C}\\mathbf{r}=\\mathbf{b},\\] where \\(\\mathbf{r}\\) is the vector of ratings. It can be shown that \\(\\mathbf{C}\\) is invertible [8]. Example 1.24 For our toy example, the Colley matrix is \\[\\mathbf{C}=\\begin{bmatrix} 5 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\\\ -1 &amp; 5 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; -1 &amp; 6 &amp; -1 &amp; -1\\\\ 0 &amp; -1 &amp; -1 &amp; 5 &amp; -1\\\\ -1 &amp; 0 &amp; -1 &amp; -1 &amp; 5 \\end{bmatrix}.\\] The diagonal entries tell us that most teams have played \\(5-2=3\\) games, while \\(c_{33}=6\\) tells us that the Crocs have played four. The off-diagonal zeros represent teams that have not met. The other matchups have occurred exactly once. From the standings, we can construct the \\(\\mathbf{b}\\) vector. Team W-L Aardvarks 1-2 Beagles 1-2 Crocs 4-0 Donkeys 2-1 Egrets 0-3 We get \\[\\mathbf{b}=\\begin{bmatrix}1+(1-2)/2\\\\1+(1-2)/2\\\\1+(4-0)/2\\\\1+(2-1)/2\\\\1+(0-3)/2\\end{bmatrix}=\\begin{bmatrix} 1/2\\\\1/2\\\\3\\\\3/2\\\\-1/2\\end{bmatrix}\\] That makes our equation \\[\\begin{bmatrix} 5 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\\\ -1 &amp; 5 &amp; -1 &amp; -1 &amp; 0\\\\ -1 &amp; -1 &amp; 6 &amp; -1 &amp; -1\\\\ 0 &amp; -1 &amp; -1 &amp; 5 &amp; -1\\\\ -1 &amp; 0 &amp; -1 &amp; -1 &amp; 5 \\end{bmatrix}\\begin{bmatrix}r_1\\\\r_2\\\\r_3\\\\r_4\\\\r_5\\end{bmatrix}=\\begin{bmatrix} 1/2\\\\1/2\\\\3\\\\3/2\\\\-1/2\\end{bmatrix}.\\] The solution is \\[\\mathbf{r}=\\begin{bmatrix} 0.400\\\\0.457\\\\0.786\\\\0.600\\\\0.257\\end{bmatrix}.\\] With the Colley ratings calculated, we can now rank the teams accordingly. Team W-L PCT Rating PF PA PD Crocs 4-0 1.000 0.786 171 29 142 Donkeys 2-1 0.667 0.600 83 52 31 Beagles 1-2 0.333 0.457 44 111 -67 Aardvarks 1-2 0.333 0.400 40 80 -40 Egrets 0-3 0.000 0.257 28 94 -66 The rankings tend to follow the winning percentages, but the Beagles get the edge over the Aardvarks. The main difference is that the Beagles didnt play the cellar-dwelling Egrets. (The fact that the Beagles beat the Aardvarks in their head-to-head matchup didnt actually matter. Also note, the points for and against do not come into play in this method.) 1.9 Exercises We saw that a system of two linear equations in two variables can be described in terms of two lines. How those lines intersect determines how many solutions, if any, there are. Describe, geometrically, the possibilities when we have a system of three linear equations in two variables. A linear equation in three variables can be represented by a plane in three dimensions. Describe geometrically the possibilities when we have a system of three linear equations in three variables. Consider the system of equations \\[\\begin{align*} \\left\\{\\begin{array}{rrr} ax+2y&amp;=&amp;1\\\\ 4x+5y&amp;=&amp;3 \\end{array}\\right. . \\end{align*}\\] For what values of \\(a\\) (if any) does the system have a unique solution? For what values of \\(a\\) (if any) does the system have no solutions? For what values of \\(a\\) (if any) does the system have infinitly many solutions? Consider the system of equations \\[\\begin{align*} \\left\\{\\begin{array}{rrr} 2x+by&amp;=&amp;4\\\\ 3x+4y&amp;=&amp;6 \\end{array}\\right. . \\end{align*}\\] For what values of \\(b\\) (if any) does the system have a unique solution? For what values of \\(b\\) (if any) does the system have no solutions? For what values of \\(b\\) (if any) does the system have infinitly many solutions? A barbershop charges $20 for an adult haircut and $12 for a youth haircut. One day the barbershop has 40 customers and has $664 in revenue. Set up and solve a system of equations to determine how many adult and how many youth customers they had. A golf tournament sells reserved seats for $120 and general admission tickets for $58. For the Sunday final round, the total attendance was 8,425, and the ticket revenue was $563,980. Set up and solve a system of equations to determine how many of each type of ticket was sold. Perform Gaussian elimination on this system of equations and solve. \\[\\begin{align*} \\left\\{ \\begin{array}{rrr} 2x+4y+6z&amp;=&amp;2\\\\ 2x+5y+10z&amp;=&amp;3\\\\ 4x+7y+10z&amp;=&amp;2 \\end{array}\\right. \\end{align*}\\] Perform Gaussian elimination on this system of equations and solve. \\[\\begin{align*} \\left\\{ \\begin{array}{rrr} 2x+2y+4z&amp;=&amp;2\\\\ 2x+3y+8z&amp;=&amp;3\\\\ 4x+2y+2z&amp;=&amp;1 \\end{array}\\right. \\end{align*}\\] Find a parametric solution to the following system of equations. \\[\\begin{align*} \\left\\{ \\begin{array}{rrc} x+3y+4z&amp;=&amp;2\\\\ 3y+8z&amp;=&amp;3 \\end{array}\\right. \\end{align*}\\] Find a parametric solution to the following system of equations. \\[\\begin{align*} \\left\\{ \\begin{array}{rrc} x+2y-3z&amp;=&amp;1\\\\ 2y+8z&amp;=&amp;1 \\end{array}\\right. \\end{align*}\\] Put this augmented matrix in reduced row echelon form and solve the underlying system. Assume the variables are \\(x,y,\\) and \\(z\\). \\[\\left[\\begin{array}{rrr|r} 1 &amp; 1 &amp; 3 &amp; 2\\\\ 0 &amp; 1 &amp; 2 &amp; 3\\\\ 0 &amp; 0 &amp; 1 &amp; -2\\end{array}\\right] \\] Put this augmented matrix in reduced row echelon form and solve the underlying system. Assume the variables are \\(x,y,\\) and \\(z\\). \\[\\left[\\begin{array}{rrr|r} 1 &amp; 3 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; 2 &amp; 2\\\\ 0 &amp; 0 &amp; 1 &amp; -2\\end{array}\\right] \\] Find a parametric solution to the system of equations whose augmented matrix has the following reduced row echelon form. Assume the variables in order are \\(x,y,z,\\) and \\(w.\\) \\[\\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 2\\\\0 &amp; 1 &amp; 2 &amp; 0 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 3\\end{array}\\right]\\] Find a parametric solution to the system of equations whose augmented matrix has the following reduced row echelon form. Assume the variables in order are \\(x,y,z,\\) and \\(w.\\) \\[\\left[\\begin{array}{rrrr|r} 1 &amp; 2 &amp; 0 &amp; 2 &amp; 3\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\] Write the corresponding system of equations in augmented matrix form and solve by putting into reduced row echelon form. \\[ \\left\\{ \\begin{array}{rrr} x+2y+3z-2w&amp;=&amp; 2\\\\ 3x+y-z+w&amp;=&amp;10\\\\ x+y+z+w&amp;=&amp;2\\\\ 5x-y+3z+2w&amp;=&amp;7 \\end{array}\\right. \\] Write the corresponding system of equations in augmented matrix form and solve by putting into reduced row echelon form. \\[ \\left\\{ \\begin{array}{rrr} x+2y+3z-3w&amp;=&amp; 2\\\\ 4x+y-2z+w&amp;=&amp;5\\\\ 2x+y+3z+w&amp;=&amp;2\\\\ 2x-2y+z+3w&amp;=&amp;6 \\end{array}\\right. \\] For \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 2\\\\4&amp; 1\\end{bmatrix},\\mathbf{B}=\\begin{bmatrix}2 &amp; -1\\\\1 &amp; 4\\end{bmatrix},\\] find \\(\\mathbf{A}+\\mathbf{B}\\) \\(\\mathbf{A}-\\mathbf{B}\\) \\(2\\mathbf{A}+\\mathbf{B}\\) \\(\\mathbf{A}^T\\) \\(\\mathbf{B}^T\\) \\(\\mathbf{A}\\mathbf{B}\\) \\(\\mathbf{B}\\mathbf{A}\\) For \\[\\mathbf{A}=\\begin{bmatrix}2 &amp; 1\\\\1&amp; 4\\end{bmatrix},\\mathbf{B}=\\begin{bmatrix}3 &amp; -2\\\\1 &amp; 2\\end{bmatrix},\\] find \\(\\mathbf{A}+\\mathbf{B}\\) \\(\\mathbf{A}-\\mathbf{B}\\) \\(2\\mathbf{A}+\\mathbf{B}\\) \\(\\mathbf{A}^T\\) \\(\\mathbf{B}^T\\) \\(\\mathbf{A}\\mathbf{B}\\) \\(\\mathbf{B}\\mathbf{A}\\) For \\[\\mathbf{A}=\\begin{bmatrix}3 &amp; 1\\\\2&amp; 4\\\\1 &amp; 4\\end{bmatrix},\\mathbf{B}=\\begin{bmatrix}3 &amp; -2\\\\1 &amp; 2\\end{bmatrix},\\] find the following, if possible. \\(\\mathbf{A}\\mathbf{B}\\) \\(\\mathbf{B}\\mathbf{A}\\) For \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 3\\\\2 &amp; 5\\end{bmatrix},\\mathbf{B}=\\begin{bmatrix}2 &amp; -2\\\\3 &amp; 3\\\\4&amp;4\\end{bmatrix},\\] find the following, if possible. \\(\\mathbf{A}\\mathbf{B}\\) \\(\\mathbf{B}\\mathbf{A}\\) Find \\(\\mathbf{A}^{-1}\\) if \\(\\mathbf{A}=\\begin{bmatrix} 2 &amp; 1\\\\5 &amp; 2\\end{bmatrix}\\) by hand. Find \\(\\mathbf{B}^{-1}\\) if \\(\\mathbf{B}=\\begin{bmatrix} 1 &amp; 1\\\\3 &amp; 2\\end{bmatrix}\\) by hand. Find \\(\\mathbf{A}^{-1}\\) if \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\2 &amp; 1 &amp; 1\\\\3 &amp; 1 &amp; -2\\end{bmatrix}.\\] Find \\(\\mathbf{B}^{-1}\\) if \\[\\mathbf{B}=\\begin{bmatrix} 2 &amp; 3 &amp; 8\\\\1 &amp; 2 &amp; -1\\\\2 &amp; 2 &amp; 4\\end{bmatrix}.\\] Find \\(\\det\\mathbf{A}\\) if \\(\\mathbf{A}=\\begin{bmatrix} 1 &amp; 1\\\\3 &amp; -1\\end{bmatrix}.\\) Find \\(\\det\\mathbf{B}\\) if \\(\\mathbf{B}=\\begin{bmatrix} 2 &amp; 1\\\\4 &amp; 1\\end{bmatrix}.\\) Find \\(\\det\\mathbf{A}\\) if \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\1 &amp; 4 &amp; 9\\\\1 &amp; 8 &amp; 27\\end{bmatrix}.\\] Find \\(\\det\\mathbf{B}\\) if \\[\\mathbf{B}=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\1 &amp; 3 &amp; 4\\\\1 &amp; 4 &amp; 5\\end{bmatrix}.\\] Find the value of \\(a\\) that makes \\(\\det\\mathbf{A}=0,\\) if \\(\\mathbf{A}=\\begin{bmatrix} a &amp; 2\\\\4 &amp; 5\\end{bmatrix}.\\) Find the value of \\(b\\) that makes \\(\\det\\mathbf{B}=0,\\) if \\(\\mathbf{B}=\\begin{bmatrix} 2 &amp; b\\\\3 &amp; 4\\end{bmatrix}.\\) Solve by hand, using inverses. \\[\\begin{bmatrix} 2 &amp; 5\\\\1 &amp; 3\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}1\\\\4\\end{bmatrix}\\] Solve by hand, using inverses. \\[\\begin{bmatrix} 4 &amp; 2\\\\5 &amp; -1\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}3\\\\2\\end{bmatrix}\\] Find elementary \\(3\\times 3\\) matrices that perform the following operations. Add 5 times row 2 to row 1. Interchange rows 2 and 3. Multiply row 1 by 7. Find elementary \\(3\\times 3\\) matrices that perform the following operations. Subtract 3 times row 3 from row 1. Interchange rows 1 and 3. Multiply row 2 by -5. Find the inverses of the matrices in Problem 29. Find the inverses of the matrices in Problem 30. A six team league has the following Colley matrix \\(\\mathbf{C}\\) and \\(\\mathbf{b}\\) vector. \\[\\mathbf{C}=\\begin{bmatrix} 5 &amp; -1 &amp; 0 &amp; 0 &amp; -1 &amp; -1 \\\\ -1 &amp; 5 &amp; -2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -2 &amp; 7 &amp; 0 &amp; -1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 3 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 &amp; 5 &amp; -1 \\\\ -1 &amp; 0 &amp; -2 &amp; -1 &amp; -1 &amp; 7\\end{bmatrix},\\mathbf{b}=\\begin{bmatrix} 0.5\\\\1.5\\\\0.5\\\\0.5\\\\0.5\\\\2.5\\end{bmatrix}\\] Find the following. How many games did each team play? Which teams played against each other more than once? Can you determine what Team 6s Win-Loss record was? Find the Colley ratings vector \\(\\mathbf{r}\\). A six team league has the following Colley matrix \\(\\mathbf{C}\\) and \\(\\mathbf{b}\\) vector. \\[\\mathbf{C}=\\begin{bmatrix} 6 &amp; -1 &amp; 0 &amp; -1 &amp; -1 &amp; -1 \\\\ -1 &amp; 6 &amp; -2 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; -2 &amp; 5 &amp; -1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; -1 &amp; 5 &amp; 0 &amp; -1 \\\\ -1 &amp; 0 &amp; 0 &amp; 0 &amp; 3 &amp; 0 \\\\ -1 &amp; -1 &amp; 0 &amp; -1 &amp; 0 &amp; 5\\end{bmatrix},\\mathbf{b}=\\begin{bmatrix} 0.0\\\\0.0\\\\1.5\\\\0.5\\\\1.5\\\\2.5\\end{bmatrix}\\] Find the following. How many games did each team play? Which teams played against each other more than once? Can you determine what Team 6s Win-Loss record was? Find the Colley ratings vector \\(\\mathbf{r}\\). Early in the season, a football division has the following results. Use the Colley Matrix Method to find the division ratings. Winner Score Loser Score Argos 20 Brahmas 13 Cobbers 31 Brahmas 10 Argos 24 Cobbers 21 Dukes 10 Brahmas 7 Brahmas 35 Dukes 3 Early in the season, a football division has the following results. Use the Colley Matrix Method to find the division ratings. Winner Score Loser Score Argos 28 Cobbers 14 Brahmas 21 Argos 13 Brahmas 17 Dukes 10 Dukes 31 Brahmas 28 References 6. Desmos matrix calculator. 7. Callaghan T, Mucha PJ, Porter MA (2004) The bowl championship series: A mathematical review. Notices Amer Math Soc 51: 887893. 8. Colley WN (2002) The Colley matrix method. 9. Olofsson P, Andersson M (2012) Probability, statistics, and stochastic processes, 2nd ed., John Wiley &amp; Sons, Inc. "],["vector-spaces.html", "Chapter 2 Vector Spaces 2.1 The Vector Space \\(\\mathbb{R}^n\\) 2.2 Subspaces 2.3 Linear Dependence and Independence 2.4 Basis and Dimension 2.5 Exercises", " Chapter 2 Vector Spaces 2.1 The Vector Space \\(\\mathbb{R}^n\\) Were used to thinking of \\(\\mathbb{R}^2\\) as the set of all points in the plane (Fig. 2.1). \\[\\mathbb{R}^2=\\{(x_1,x_2): x_1,x_2\\in \\mathbb{R}\\}.\\] Figure 2.1: A point in the plane Its also helpful to think of \\(\\mathbb{R}^2\\) as a collection of vectors. If we place a vector with its tail at the origin, its tip is at a point \\((x_1,x_2)\\). We name the vector by this point. This picture represents the (column) vector \\(\\mathbf{v}=\\begin{bmatrix}x_1\\\\x_2 \\end{bmatrix}\\). (Fig. 2.2) Figure 2.2: A vector in \\(\\mathbb{R}^2\\) Vector addition and scalar multiplication We know how to add vectors because we know how to add matrices. If \\(\\mathbf{u}=\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix}v_1\\\\v_2\\end{bmatrix}\\), then \\[\\mathbf{u}+\\mathbf{v}=\\begin{bmatrix}u_1+v_1\\\\u_2+v_2\\end{bmatrix}.\\] We also know how to multiply a vector by a scalar: \\[r\\mathbf{u}=\\begin{bmatrix} ru_1\\\\ru_2\\end{bmatrix}.\\] For example, if \\(\\mathbf{u}=\\begin{bmatrix}3\\\\5\\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix}2\\\\-1\\end{bmatrix}\\), then \\[\\mathbf{u}+\\mathbf{v}=\\begin{bmatrix}3+2\\\\5+(-1)\\end{bmatrix}=\\begin{bmatrix}5\\\\4\\end{bmatrix}.\\] Also, \\[2\\mathbf{u}=\\begin{bmatrix}2\\cdot 3\\\\2\\cdot 5\\end{bmatrix}=\\begin{bmatrix}6\\\\10\\end{bmatrix},\\] while \\[-2\\mathbf{u}=\\begin{bmatrix}-2\\cdot 3\\\\-2\\cdot 5\\end{bmatrix}=\\begin{bmatrix}-6\\\\-10\\end{bmatrix}.\\] Vector addition and scalar multiplication have nice geometric interpretations (Figures 2.3,2.4). Figure 2.3: Vector Addition Figure 2.4: Scalar Multiplication We cant forget the special vector \\[\\mathbf{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.\\] Unlike the others, the zero vector has 0 magnitude and has no well-defined direction. Speaking of magnitude, the magnitude of a vector \\(\\mathbf{x}\\), denoted \\(|\\mathbf{x}|\\) (or frequently \\(\\Vert \\mathbf{x}\\Vert\\)) is the distance from its tail to its tip. Using the Pythagorean theorem, if \\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix},\\] then \\[|\\mathbf{x}|=\\sqrt{x_1^2+x_2^2}.\\] For instance, if \\(\\mathbf{x}=\\begin{bmatrix}2\\\\3\\end{bmatrix}\\), then \\(|\\mathbf{x}|=\\sqrt{2^2+3^2}=\\sqrt{13}\\approx 3.6\\). With vector addition and scalar multiplication defined as above, we see that the following properties hold: If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are vectors in \\(\\mathbb{R}^2\\), then \\(\\mathbf{u}+\\mathbf{v}\\) is also a vector in \\(\\mathbb{R}^2\\). This property is called closure under addition. If \\(\\mathbf{u}\\) is a vector in \\(\\mathbb{R}^2\\) and \\(r\\) is any real number, then \\(r\\mathbf{u}\\) is also in \\(\\mathbb{R}^2\\). This property is called closure under scalar multiplication. Sets of things that satisfy these properties (and some others that we wont get into) are called vector spaces. Linear dependence and independence Two vectors in \\(\\mathbb{R}^2\\) are linearly dependent if one is a constant multiple of the other. Otherwise, they are linearly independent. For example, \\(\\mathbf{u}=\\begin{bmatrix}2\\\\-3\\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix}-6\\\\9\\end{bmatrix}\\) are linearly dependent, since \\(\\mathbf{v}=-3\\mathbf{u}\\). \\(\\mathbf{u}=\\begin{bmatrix}2\\\\-3\\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix}6\\\\9\\end{bmatrix}\\) are linearly independent, since \\(v_1=3u_1\\), but \\(v_2=-3u_2\\). The zero vector \\(\\mathbf{0}\\) is linearly dependent with every vector since \\(0\\mathbf{u}=\\mathbf{0}\\). Heres an alternative definition of linear dependence that we will be able to extend to larger collections of vectors. Definition 2.1 Two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly dependent if there exist scalars \\(a\\) and \\(b\\), not both zero, such that \\[a\\mathbf{u}+b\\mathbf{v}=\\mathbf{0}.\\] If \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are not linearly dependent, then they are linearly independent. For example, \\(\\mathbf{u}=\\begin{bmatrix}2\\\\-3\\end{bmatrix}\\) and \\(\\mathbf{v}=\\begin{bmatrix}-6\\\\9\\end{bmatrix}\\) are linearly dependent, since \\(3\\mathbf{u}+1\\mathbf{v}=\\mathbf{0}.\\) Linear Combinations Any sum of the form \\[a\\mathbf{u}+b\\mathbf{v}\\] is called a linear combination of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). An alternative way of stating Definition 2.1 is that two vectors are linearly dependent if there is a nontrivial linear combination that results in the zero vector. (Trivial in this context means that all coefficients are 0.) \\(\\mathbb{R}^3\\) and \\(\\mathbb{R}^n\\) We can think of \\(\\mathbb{R}^3\\) as a vector space, as well. \\[\\mathbb{R}^3=\\left\\{\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}:x_1,x_2,x_3\\in\\mathbb{R}\\right\\}.\\] And, while we cant easily visualize higher dimensional vectors, we can define \\(\\mathbb{R}^n\\) for any positive integer \\(n\\) as a vector space: \\[\\mathbb{R}^n=\\left\\{\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots \\\\x_n\\end{bmatrix}: x_1,x_2,\\dots, x_n\\in\\mathbb{R}\\right\\}.\\] (Note: you can write these vectors horizontally: \\((x_1,x_2,\\dots,x_n)\\) if necessary, to save space.) The properties for vectors in \\(\\mathbb{R}^2\\) have analogous properties in \\(\\mathbb{R}^n\\). The magnitude of a vector is defined this way: if \\[\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots \\\\x_n\\end{bmatrix},\\] then \\[|\\mathbf{x}|=\\sqrt{x_1^2+x_2^2+\\cdots+x_n^2}.\\] 2.2 Subspaces For what we will be doing in the future, subspaces will be very important to us. Heres the definition. Definition 2.2 Let \\(W\\) be a nonempty subset of a vector space \\(V\\). Then \\(W\\) is a subspace of \\(V\\) if For all vectors \\(\\mathbf{w}_1\\) and \\(\\mathbf{w}_2\\) in \\(W\\), \\(\\mathbf{w}_1+\\mathbf{w}_2\\) is also in \\(W\\) (i.e. \\(W\\) is closed under addition), and For vectors \\(\\mathbf{w}\\) in \\(W\\) and all scalars \\(r\\), \\(r\\mathbf{w}\\) is also in \\(W\\) (i.e. \\(W\\) is closed under scalar multiplication). These two can be combined into a single condition: a subset of a vector space is a subspace if all linear combinations of vectors from the subset are also in the subset. Essentially, a subspace is a subset of vector space that is itself a vector space. Example 2.1 Let \\(W\\) be the set of all vectors in \\(\\mathbb{R}^3\\) of the form \\[\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\end{bmatrix}\\] such that \\(w_1+w_2+w_3=0\\). For instance, \\(\\mathbf{w}=\\begin{bmatrix} 1\\\\2\\\\-3\\end{bmatrix}\\) is in \\(W\\), since \\(1+2+(-3)=0\\), but \\(\\mathbf{v}=\\begin{bmatrix} 1\\\\2\\\\-4\\end{bmatrix}\\) is not in \\(W\\), since \\(1+2+(-4)=-1\\neq 0\\). Is \\(W\\) a subspace of \\(\\mathbb{R}^3\\)? We need to check closure under addition and closure under scalar multiplication. Closure under addition: let \\(\\mathbf{w}=(w_1,w_2,w_3)\\) and let \\(\\mathbf{v}=(v_1,v_2,v_3)\\) be any two vectors in \\(W\\). (This means that \\(w_1+w_2+w_3=0\\) and \\(v_1+v_2+v_3=0\\).) Then \\(\\mathbf{w}+\\mathbf{v}=(w_1+v_1,w_2+v_2,w_3+v_3)\\). We need to show that \\(\\mathbf{w}+\\mathbf{v}\\) is in \\(W\\), which means showing that the sum of the components of \\(\\mathbf{w}+\\mathbf{v}\\) is 0. We have \\[\\begin{align*} (w_1+v_1)+(w_2+v_2)+(w_3+v_3)&amp;=(w_1+w_2+w_3)+(v_1+v_2+v_3)\\\\ &amp;=0+0 \\text{ since $\\mathbf{w}$ and $\\mathbf{v}$ are in $W$}\\\\ &amp;=0.\\checkmark \\end{align*}\\] The sum of two vectors in \\(W\\) is also in \\(W\\). Closure under scalar multiplication: Let \\(\\mathbf{w}=(w_1,w_2,w_3)\\) be a vector in \\(W\\), and let \\(r\\) be any real number. Then \\(r\\mathbf{w}=(rw_1,rw_2,rw_3)\\), and \\[rw_1+rw_2+rw_3=r(w_1+w_2+w_3)=r\\cdot 0=0.\\checkmark\\] Since \\(W\\) is closed under addition and scalar multiplication, it is a subspace of \\(\\mathbb{R}^3\\). Example 2.2 Let \\(W\\) be the set of all vectors in \\(\\mathbb{R}^2\\) of the form \\(\\mathbf{w}=(w_1,1)\\). (All vectors with a second component of 1.) Is this \\(W\\) a subspace? Nope. If \\(\\mathbf{w}=(w_1,1)\\), then \\(2\\mathbf{w}=(2w_1,2)\\), which is not in \\(W\\). Since \\(W\\) is not closed under scalar multiplication, it is not a subspace of \\(\\mathbb{R}^2\\). Its also easy to show that \\(W\\) is not closed under addition. Suppose that \\(W\\) is a subspace, and let \\(\\mathbf{w}\\) be any vector in \\(W\\). Then the scalar product \\(0\\mathbf{w}=\\mathbf{0}\\) must also be in \\(W\\). In other words, any subspace (and any vector space, for that matter) must contain a zero vector. If a subset of a vector space does not contain the zero vector, it cannot be a subspace. In our last example, \\(W\\) did not contain the zero vector, since \\(\\mathbf{0}=(0,0)\\) is not of the form \\((w_1,1)\\). We could have immediately determined that \\(W\\) was not a subspace. Lets state this fact about the zero vector as a theorem. Theorem 2.1 Let \\(W\\) be a subspace of a vector space \\(V\\). Then \\(\\mathbf{0}\\in W.\\) Here is one last example to close out the section. Example 2.3 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix, and let \\(W\\) be the set of all vectors \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\) such that \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\). Is \\(W\\) a subspace of \\(\\mathbb{R}^n\\)? Closure under addition: let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be two vectors in \\(W\\). Then \\[\\begin{align*} \\mathbf{A}\\left(\\mathbf{x}+\\mathbf{y}\\right)&amp;=\\mathbf{A}\\mathbf{x}+\\mathbf{A}\\mathbf{y}\\\\ &amp;=\\mathbf{0}+\\mathbf{0}=\\mathbf{0}.\\checkmark \\end{align*}\\] Closure under scalar multiplication: if \\(\\mathbf{x}\\) is in \\(W\\) and \\(r\\) is any real number, then \\[\\mathbf{A}(r\\mathbf{x})=r(\\mathbf{A}\\mathbf{x})=r\\mathbf{0}=\\mathbf{0}.\\checkmark\\] So \\(W\\) is a subspace of \\(\\mathbb{R}^n\\), and an important one, called the nullspace of \\(\\mathbf{A}\\). Well see more of it in the future. 2.3 Linear Dependence and Independence In Section 2.1, we talked about a linear combination of two vectors. Lets generalize the notion. Definition 2.3 Let \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\}\\) be a collection of vectors in a vector space \\(V\\). A linear combination of \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\}\\) is a vector \\[\\mathbf{v}=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_r\\mathbf{v}_r,\\] where \\(c_1,c_2,\\dots,c_r\\) are real numbers. Example 2.4 Let \\[\\mathbf{v}_1=\\begin{bmatrix} 1\\\\2\\\\1\\end{bmatrix},\\mathbf{v}_2=\\begin{bmatrix} 2\\\\3\\\\-1\\end{bmatrix}, \\text{ and }\\mathbf{v}_3=\\begin{bmatrix}4\\\\1\\\\2\\end{bmatrix}.\\] Then \\(\\mathbf{v}=2\\mathbf{v}_1+\\mathbf{v}_2-3\\mathbf{v}_3\\) is \\[\\begin{align*}\\mathbf{v}=2\\begin{bmatrix} 1\\\\2\\\\1\\end{bmatrix}+1\\begin{bmatrix} 2\\\\3\\\\-1\\end{bmatrix}-3\\begin{bmatrix}4\\\\1\\\\2\\end{bmatrix}&amp;=\\begin{bmatrix}2\\cdot 1+1\\cdot 2-3\\cdot 4\\\\2\\cdot 2+1\\cdot 3-3\\cdot 1\\\\2\\cdot 1+1\\cdot(-1)-3\\cdot 2\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}-8\\\\4\\\\-5\\end{bmatrix}. \\end{align*}\\] This type of calculation might seem familiar. Consider the matrix-vector product \\(\\mathbf{A}\\mathbf{x}\\): \\[\\mathbf{A}\\mathbf{x}=\\begin{bmatrix}1 &amp; 2 &amp; 4\\\\2 &amp; 3 &amp; 1\\\\1 &amp; -1 &amp; 2\\end{bmatrix}\\begin{bmatrix}2\\\\1\\\\-3\\end{bmatrix}=\\begin{bmatrix}-8\\\\4\\\\-5\\end{bmatrix}.\\] Note that the columns of \\(\\mathbf{A}\\) are the vectors \\(\\mathbf{v}_1,\\mathbf{v}_2,\\) and \\(\\mathbf{v}_3\\) from the last example, while the entries in \\(\\mathbf{x}\\) are the coefficients in the linear combination from the example. When you multiply a vector by a matrix, youre really just taking a linear combination of the columns of the matrix! This is an important and useful idea. Here it is in more general form: \\[\\begin{bmatrix}a_{11} &amp; a_{12} &amp;\\cdots &amp; a_{1n}\\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn}\\end{bmatrix} \\begin{bmatrix} c_1\\\\c_2\\\\ \\vdots\\\\c_n\\end{bmatrix}= c_1\\mathbf{a}_1+c_2\\mathbf{a}_2+\\cdots+c_n\\mathbf{a}_n,\\] where \\(\\mathbf{a}_i\\) is the \\(i\\)th column vector. (Here is some useful notation: the matrix with columns \\(\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\) is often written \\(\\begin{bmatrix}\\mathbf{a}_1&amp;\\mathbf{a}_2&amp;\\cdots&amp;\\mathbf{a}_n\\end{bmatrix}\\). Note the bold typeface indicating vectors.) One important result of this is that the equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) can be written as a linear combination of the columns of \\(\\mathbf{A}\\). Example 2.5 Can \\(\\mathbf{b}=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\) be written as a linear combination of \\(\\mathbf{a}_1=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix}4\\\\7\\end{bmatrix}\\)? This is equivalent to solving \\(\\begin{bmatrix}\\mathbf{a}_1 &amp; \\mathbf{a}_2\\end{bmatrix}\\begin{bmatrix}c_1\\\\c_2\\end{bmatrix}=\\mathbf{b}\\) To solve this, well create the matrix \\(\\mathbf{A}\\) with \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) as its columns, augment it with \\(\\mathbf{b}\\), and reduce:1 \\[\\left[ \\begin{array}{ll|l} 1 &amp; 4 &amp; 2\\\\2 &amp; 7 &amp; 5\\end{array}\\right]\\leadsto \\left[ \\begin{array}{ll|l} 1 &amp; 0 &amp; 6\\\\0 &amp; 1 &amp; -1\\end{array}\\right]\\] The answer is yes, with the combination \\(\\mathbf{b}=6\\mathbf{a}_1-\\mathbf{a}_2\\). To verify this, we have \\[6\\mathbf{a}_1-\\mathbf{a}_2=6\\begin{bmatrix}1\\\\2\\end{bmatrix}-\\begin{bmatrix}4\\\\7\\end{bmatrix}=\\begin{bmatrix}6\\cdot 1-1\\cdot 4\\\\6\\cdot 2-1\\cdot 7\\end{bmatrix}=\\begin{bmatrix}2\\\\5\\end{bmatrix}=\\mathbf{b}.\\] The span of a set of vectors is an important concept. Definition 2.4 Let \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\}\\) be a set of vectors in a vector space \\(V\\). The span of the set, \\(\\text{span}(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r)\\), is the set of all possible linear combinations of the vectors. The following theorem is important and easy to prove. Theorem 2.2 Let \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\}\\) be a set of vectors in a vector space \\(V\\). Then \\(W=\\textrm{span}(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r)\\) is a subspace of \\(V\\). Proof. Let \\(\\mathbf{u}\\) and \\(\\mathbf{w}\\) be in \\(W\\). There exist constants \\(a_1,\\dots,a_r\\) and \\(b_1,\\dots,b_r\\) such that \\(\\mathbf{u}=a_1\\mathbf{v}_1+\\cdots + a_r\\mathbf{v}_r\\) and \\(\\mathbf{w}=b_1\\mathbf{v}_1+\\cdots + b_r\\mathbf{v}_r.\\) Then \\(\\mathbf{u}+\\mathbf{w}=(a_1+b_1)\\mathbf{v}_1+\\cdots+(a_r+b_r)\\mathbf{v}_r\\), which is another linear combination of \\(\\mathbf{v}_1\\dots,\\mathbf{v}_r,\\) so it is in \\(W\\). Similarly, for any scalar \\(c,c\\mathbf{u}=ca_1\\mathbf{v}_1+\\cdots+ca_r\\mathbf{v}_r.\\) Since \\(W\\) is closed under addition and scalar multiplication, it is a subspace of \\(V\\). Linear dependence revisited Lets now generalize the definitions of linear dependence and independence. Definition 2.5 Let \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\) be vectors in a vector space \\(V\\). The vectors are linearly dependent if there exist constants \\(c_1,c_2,\\dots,c_r\\), not all zero, such that \\[c_1\\mathbf{v}_1+c_2\\mathbf{v}_2+\\cdots+c_r\\mathbf{v}_r=\\mathbf{0}.\\] If the set is not linearly dependent, then it is linearly independent. Note that any collection that includes the zero vector will be linearly dependent. Why? Suppose \\(\\{\\mathbf{v}_1,\\dots \\mathbf{v}_r, \\mathbf{0}\\}\\) is the set in question. Then \\[0\\mathbf{v}_1+\\cdots+0\\mathbf{v}_r+1\\cdot \\mathbf{0}=\\mathbf{0},\\] a nontrivial linear combination yielding the zero vector. Also note that if a collection of vectors is linearly dependent, than at least one can be written as a linear combination of the others. Example 2.6 Let \\(\\mathbf{v}_1=(1,2,-1),\\mathbf{v}_2=(2,-1,4)\\), and \\(\\mathbf{v}_3=(3,-4,9)\\). Are these vectors dependent or independent? To answer this question, we can put the vectors as columns in a matrix \\(\\mathbf{A}\\) and see if the equation \\(\\mathbf{A}\\mathbf{c}=\\mathbf{0}\\) has a nontrivial solution. (Again, remember that \\(\\mathbf{A}\\mathbf{c}\\) is a linear combination of the columns of \\(\\mathbf{A}\\).) \\[\\left[\\begin{array}{rrr|r}1 &amp; 2 &amp; 3 &amp; 0\\\\2 &amp; -1 &amp; -4 &amp; 0\\\\-1 &amp; 4 &amp; 9 &amp; 0\\end{array}\\right]\\leadsto \\left[\\begin{array}{rrr|r} 1 &amp; 0 &amp; -1 &amp; 0\\\\0 &amp; 1 &amp; 2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\] Since the reduced row echelon form of the coefficient matrix has a free variable, there will actually be infinitely many solutions. If we let \\(c_3=t\\), then \\(c_1-t=0\\) or \\(c_1=t\\), and \\(c_2+2t=0\\) or \\(c_2=-2t\\). In particular, if \\(t=1\\), then \\(c_1=1,c_2=-2,\\) and \\(c_3=1\\) will work. Lets check. \\[1\\begin{bmatrix}1\\\\2\\\\-1\\end{bmatrix}+-2\\begin{bmatrix}2\\\\-1\\\\4\\end{bmatrix}+1\\begin{bmatrix}3\\\\-4\\\\9\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\checkmark.\\] Since weve found a nontrivial linear combination adding up to the zero vector, the three vectors are linearly dependent. Example 2.7 Let \\(\\mathbf{v}_1=(1,2,-1),\\mathbf{v}_2=(2,3,4)\\), and \\(\\mathbf{v}_3=(3,5,4)\\). Are these vectors linearly dependent or independent? \\[\\left[\\begin{array}{rrr|r}1 &amp; 2 &amp; 3 &amp; 0\\\\2 &amp; 3 &amp; 5 &amp; 0\\\\-1 &amp; 4 &amp; 4 &amp; 0\\end{array}\\right]\\leadsto \\left[\\begin{array}{rrr|r}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\end{array}\\right]\\] In this case, the only solution is the trivial one: \\(c_1=c_2=c_3=0\\). The vectors are linearly independent. In both of these examples, we were dealing with three vectors in \\(\\mathbb{R}^3\\). The process will still work with any number of vectors in any dimension of vector space, but there is a nice connection when the number of vectors is equal to the dimension of the vector space. First, a quick definition. Definition 2.6 Two \\(m\\times n\\) matrices are row equivalent if one can be transformed into the other via a sequence of elementary row operations. Note that since row operations are reversible, if you can transform \\(\\mathbf{A}\\) to \\(\\mathbf{B}\\), you can transform \\(\\mathbf{B}\\) to \\(\\mathbf{A}\\) by reversing the steps. In Example 2.6, when we reduced the matrix, we had a row of zeros at the bottom. In that case, the vectors were dependent. In Example 2.7, the coefficient matrix reduced to the identity matrix. Heres a theorem. Theorem 2.3 Let \\(\\left\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots\\mathbf{v}_n\\right\\}\\) be a collection of \\(n\\) vectors in \\(\\mathbb{R}^n\\). Let \\(\\mathbf{A}\\) be the matrix whose columns are \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots\\mathbf{v}_n\\). Then \\(\\left\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots\\mathbf{v}_n\\right\\}\\) are linearly independent if and only if \\(\\mathbf{A}\\) is row equivalent to the identity matrix. Equivalently, \\(\\left\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots\\mathbf{v}_n\\right\\}\\) are linearly dependent if and only if \\(\\det\\mathbf{A}=0\\) if and only if \\(\\mathbf{A}\\) is invertible. The key thing here is that the number of vectors matches the dimension. There are only zeros to the right of the augmentation bar, and when you perform any row operations, there will still be only zeros there. If the coefficient part reduces to the identity matrix, you will have only the trivial solution. Otherwise, there will be a free variable and infinitely many solutions. Note that if \\(\\left\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_n\\right\\}\\) is a linearly independent collection of vectors in \\(\\mathbb{R}^n\\) and \\(\\mathbf{v}\\) is any vector in \\(\\mathbb{R}^n\\), then we can write \\(\\mathbf{v}\\) as a linear combination of \\(\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_n\\). Why? Since \\[\\mathbf{A}=\\begin{bmatrix} \\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{v}_n\\end{bmatrix}\\] is invertible, \\[\\mathbf{A}\\mathbf{x}=\\mathbf{v}\\] has a solution, and the entries in \\(\\mathbf{x}\\) give us the coefficients in the linear combination. We say that a linearly independent collection of \\(n\\) vectors in \\(\\mathbb{R}^n\\) spans \\(\\mathbb{R}^n\\). What happens when you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\)? Heres a quick example with 3 vectors in \\(\\mathbb{R}^2\\). (Well put them in the columns.) \\[\\left[\\begin{array}{rrr|r} 1 &amp; 2 &amp; 3 &amp; 0\\\\ 2 &amp; 4 &amp; 1 &amp; 0\\end{array}\\right]\\leadsto \\left[\\begin{array}{rrr|r}1 &amp; 2 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\end{array}\\right]\\] Since theres a free variable, \\(c_2\\) (the second coefficient) in this case, there will be infinitely many solutions. Whenever there are more columns than rows in the coefficient matrix there will be a free variable. If the number of vectors is greater than the dimension of the vector space, the set will always be linearly dependent. Lets make that a theorem. Theorem 2.4 Let \\(\\left\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\right\\}\\) be a set of \\(n\\) vectors in \\(\\mathbb{R}^m\\), with \\(n&gt;m\\). Then \\(\\left\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\right\\}\\) is a linearly dependent set. 2.4 Basis and Dimension Let \\(\\mathbf{u}=\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}\\) be any vector in \\(\\mathbb{R}^2\\), and let \\(\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}\\) and \\(\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\). (You may have seen \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) called \\(\\mathbf{i}\\) and \\(\\mathbf{j},\\) respectively, but we will be generalizing them soon, and we run out of letters quickly.) The key observation is that \\(\\mathbf{u}\\) can be written as a linear combination of \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) this way: \\[\\mathbf{u}=\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}=\\begin{bmatrix}u_1\\\\0\\end{bmatrix}+\\begin{bmatrix}0\\\\u_2\\end{bmatrix}=u_1\\begin{bmatrix}1\\\\0\\end{bmatrix}+u_2\\begin{bmatrix}0\\\\1\\end{bmatrix}=u_1\\mathbf{e}_1+u_2\\mathbf{e}_2.\\] The geometry of this linear combination looks like this (Fig. 2.5). Figure 2.5: Writing a vector in terms of \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) The vectors \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) have the following properties: \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are linearly independent. Any vector in \\(\\mathbb{R}^2\\) can be written as a linear combination of \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\). That is, they span \\(\\mathbb{R}^2\\). These properties define a basis for a vector space. Definition 2.7 A basis for a vector space \\(V\\) is a collection of linearly independent vectors \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_r\\}\\) whose span is all of \\(V\\). While \\(\\{\\mathbf{e}_1,\\mathbf{e}_2\\}\\) is a basis for \\(\\mathbb{R}^2\\), it is not the only one. In fact, as we learned in the last section, any linearly independent pair of vectors in \\(\\mathbb{R}^2\\) will span \\(\\mathbb{R}^2\\). We do, however, have the following fact about bases which we wont prove. Theorem 2.5 Let \\(\\{\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_r\\}\\) and \\(\\{\\mathbf{v}_1,\\mathbf{v}_2\\dots,\\mathbf{v}_s\\}\\) be two bases for a vector space \\(V\\). Then \\(r=s\\). This common value is the dimension of \\(V\\). Note: there are such things as infinite dimensional vector spaces, but we wont be dealing with them in this book. The standard basis for \\(\\mathbb{R}^n\\) The standard basis for \\(\\mathbb{R}^3\\) is \\[\\mathbf{e}_1=\\begin{bmatrix} 1\\\\0\\\\0\\end{bmatrix},\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix},\\mathbf{e}_3=\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}.\\] (You might know them better as \\(\\mathbf{i},\\) \\(\\mathbf{j},\\) and \\(\\mathbf{k}.\\)) The standard basis for \\(\\mathbb{R}^n\\) is the set of vectors \\(\\{\\mathbf{e}_i,i=1\\dots n\\}\\), where \\(\\mathbf{e}_i\\) is the \\(n\\)-dimensional vector with a 1 in the \\(i\\)th entry, and zeros everywhere else. Remember that any collection of \\(n\\) linearly independent vectors in \\(\\mathbb{R}^n\\) will span \\(\\mathbb{R}^n\\) and, thus, form a basis for it. The dimension of a subspace Since a subspace of a finite dimensional vector space is a vector space in its own right, it will have its own dimension. If we can find a basis, we will know its dimension. The dimension of the Nullspace We talked about the nullspace of an \\(m\\times n\\) matrix \\(\\mathbf{A}\\) in Example 2.3 . Its the set of all vectors \\(\\mathbf{x}\\) in \\(\\mathbb{R}^n\\) such that \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}.\\) Lets call it \\(\\text{Null}(\\mathbf{A})\\). If we can find a basis for \\(\\text{Null}(\\mathbf{A})\\), we can find its dimension as a subspace of \\(\\mathbb{R}^n\\). Example 2.8 Find a basis for \\(\\text{Null}(\\mathbf{A})\\), where \\[\\mathbf{A}=\\begin{bmatrix}5 &amp; 2 &amp; -15 &amp; 8\\\\2 &amp; 1 &amp; -6 &amp; 3\\\\-3 &amp; -1 &amp; 9 &amp; -5\\end{bmatrix}.\\] To solve \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\), we augment \\(\\mathbf{A}\\) with the zero vector and put in reduced row echelon form. \\[\\left[\\begin{array}{rrrr|r}5 &amp; 2 &amp; -15 &amp; 8 &amp; 0\\\\2 &amp; 1 &amp; -6 &amp; 3&amp; 0\\\\-3 &amp; -1 &amp; 9 &amp; -5&amp; 0\\end{array}\\right]\\leadsto \\left[\\begin{array}{rrrr|r} 1 &amp; 0 &amp; -3 &amp; 2 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]\\] With the columns representing \\(x_1,x_2,x_3,\\) and \\(x_4\\), we have \\(x_3\\) and \\(x_4\\) as free variables. If we let \\(s=x_3\\) and \\(t=x_4\\), then \\(x_1=3s-2t\\) and \\(x_2=t\\). We can write the solution to \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) in parametric form as \\[\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}3s-2t\\\\t\\\\s\\\\t\\end{bmatrix}=s\\begin{bmatrix}3\\\\0\\\\1\\\\0\\end{bmatrix}+t\\begin{bmatrix}-2\\\\1\\\\0\\\\1\\end{bmatrix}.\\] The vectors \\[\\mathbf{v}_1=\\begin{bmatrix}3\\\\0\\\\1\\\\0\\end{bmatrix} \\text{ and } \\mathbf{v}_2=\\begin{bmatrix}-2\\\\1\\\\0\\\\1\\end{bmatrix}\\] form our basis for \\(\\text{Null}(\\mathbf{A})\\), which is a 2-dimensional subspace of \\(\\mathbb{R}^4\\). Note that the dimension corresponds to the number of free variables in the reduced row echelon form of \\(\\mathbf{A}.\\) This number is sometimes called the nullity of \\(\\mathbf{A}.\\) As a check, note that \\[\\mathbf{A}\\mathbf{v}_1=\\begin{bmatrix}5 &amp; 2 &amp; -15 &amp; 8\\\\2 &amp; 1 &amp; -6 &amp; 3\\\\-3 &amp; -1 &amp; 9 &amp; -5\\end{bmatrix}\\begin{bmatrix}3\\\\0\\\\1\\\\0\\end{bmatrix}=3\\begin{bmatrix}5\\\\2\\\\-3\\end{bmatrix}+\\begin{bmatrix}-15\\\\-6\\\\9\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}=\\mathbf{0}.\\] This calculation shows that \\(\\mathbf{v}_1\\) is in the nullspace of \\(\\mathbf{A}\\). A similar calculation will show that \\(\\mathbf{v}_2\\) is also in it. One last note: if \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x}=\\mathbf{0}\\), then \\(\\text{Null}(\\mathbf{A})\\) is said to have dimension 0. The column space of a matrix Definition 2.8 Let \\(\\mathbf{A}=\\begin{bmatrix}\\mathbf{a}_1 &amp; \\mathbf{a}_2&amp;\\cdots &amp; \\mathbf{a}_n\\end{bmatrix}\\) be an \\(m\\times n\\) matrix. (Recall that the columns of \\(\\mathbf{A},\\) \\(\\{\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\}\\) are vectors in \\(\\mathbb{R}^m\\).) The column space of \\(\\mathbf{A},\\) denoted \\(\\text{Col}(\\mathbf{A})\\), is the span of \\(\\{\\mathbf{a}_1,\\mathbf{a}_2,\\dots,\\mathbf{a}_n\\}\\). Note that \\(\\text{Col}(\\mathbf{A})\\) is a subspace of \\(\\mathbb{R}^m\\). Recall that the equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is a linear combination of the columns of \\(\\mathbf{A},\\) which means that \\(\\mathbf{b}\\) has to be in the column space of \\(\\mathbf{A}.\\) Theorem 2.6 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix, and let \\(\\mathbf{b}\\) be a vector in \\(\\mathbb{R}^m\\). Then the equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) has a solution if and only if \\(\\mathbf{b}\\) is in the column space of \\(\\mathbf{A}.\\) Example 2.9 It turns out, that the process of finding a basis for the nullspace of \\(\\mathbf{A}\\) will also give you the basis for the column space. The reasoning is a bit tricky, but the process is simple. Recall the last example we did, leaving off the augmented vector. \\[\\mathbf{A}=\\begin{bmatrix}5 &amp; 2 &amp; -15 &amp; 8 \\\\2 &amp; 1 &amp; -6 &amp; 3\\\\-3 &amp; -1 &amp; 9 &amp; -5\\end{bmatrix}\\leadsto \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 2 \\\\0 &amp; 1 &amp; 0 &amp; -1 \\\\0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] The reduced row echelon form of \\(\\mathbf{A}\\) has two pivot columns (columns with leading 1s): column 1 and column 2. The corresponding columns of \\(\\mathbf{A}\\) will form a basis for \\(\\text{Col}(\\mathbf{A})\\). Our basis is \\[\\left\\{\\left[\\begin{array}{rrr}5\\\\2\\\\-3\\end{array}\\right],\\left[\\begin{array}{rrr} 2\\\\1\\\\-1\\end{array}\\right]\\right\\}.\\] This is the procedure for identifying a basis for the column space of a matrix \\(\\mathbf{A}\\). Put \\(\\mathbf{A}\\) in to reduced row echelon form, call it \\(\\mathbf{R}.\\) (Actually, just row echelon form is sufficient.) Identify the pivot columns of \\(\\mathbf{R}.\\) These are the columns with leading ones. The columns of \\(\\mathbf{A}\\) corresponding to the pivot columns of \\(\\mathbf{R}\\) form a basis for \\(\\text{Col}(\\mathbf{A}).\\) Basically, elementary row operations preserve column dependencies, though they do not preserve the column space itself. The pivot columns of \\(\\mathbf{R}\\) form a basis for its column space, so the corresponding columns of \\(\\mathbf{A}\\) form a basis for its column space. Now that we know how to find a basis for the column space of a matrix, we know what the dimension of the space is. Definition 2.9 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. The column rank of \\(\\mathbf{A}\\) is the dimension of the column space of \\(\\mathbf{A}.\\) In Example 2.9, the column rank is 2, since the column space has two basis vectors. Note that the column rank of a matrix is equal to the number of pivot columns. The nullity of a matrix is equal to the number of free variables, which is the number of columns that arent pivot columns. When you add these two numbers together, you get the total number of columns in the matrix. This leads to the following result. Theorem 2.7 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. Then \\[\\text{dim}\\, \\text{Col}(\\mathbf{A)}+\\text{dim}\\, \\text{Null}(\\mathbf{A})=n.\\] For example, if \\(\\mathbf{A}\\) is a \\(3\\times 5\\) matrix with a nullity of 2, then the column rank is \\(5-2=3\\). The row space The row space of a matrix \\(\\mathbf{A}\\), denoted \\(\\text{Row}(\\mathbf{A}),\\) is the span of the row vectors of \\(\\mathbf{A}.\\) It turns out that we can find a basis for the row space at the same time were finding a basis for the column space and the nullspace. Again, we row reduce. Theorem 2.8 The nonzero rows of the row reduced coefficient matrix form a basis for the row space. Lets go back to our example. \\[\\mathbf{A}=\\begin{bmatrix}5 &amp; 2 &amp; -15 &amp; 8 \\\\2 &amp; 1 &amp; -6 &amp; 3\\\\-3 &amp; -1 &amp; 9 &amp; -5\\end{bmatrix}\\leadsto \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 2 \\\\0 &amp; 1 &amp; 0 &amp; -1 \\\\0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] Our basis is \\[\\left\\{\\begin{bmatrix}1 &amp;0 &amp; -3 &amp; 2\\end{bmatrix},\\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; -1\\end{bmatrix}\\right\\}.\\] This one is easier to explain. Elementary row operations at most produce vectors that are still in the row space. Adding a multiple of one row to another: \\(c\\mathbf{r}_i+\\mathbf{r}_j\\) is in \\(\\text{Row}(\\mathbf{A}).\\) Multiplying a row by a nonzero constant: \\(c\\mathbf{r}_i\\) is in \\(\\text{Row}(\\mathbf{A}).\\) Interchanging two rows just rearranges the vectors. This means that the rows of the reduced matrix, call it \\(\\mathbf{R},\\) are in the span of the rows of \\(\\mathbf{A}.\\) Since row operations are reversible, the rows of \\(\\mathbf{A}\\) are also linear combinations of the rows of \\(\\mathbf{R}.\\) Moreover, the nonzero rows of a row echelon matrix are always independent. (Think about why this is true.) They are a linearly independent set of vectors that span the row space of \\(\\mathbf{A}\\) so they form a basis for it. A word of warning: for the column space, we take our basis vectors from the original matrix. It would be tempting to do that for the row space basis too, but issues can arise. Example 2.10 Consider the following matrix and its reduced form. \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\2 &amp; 4 &amp; 6\\\\1 &amp; 5 &amp; 9\\end{bmatrix}\\leadsto \\begin{bmatrix}1 &amp; 0 &amp; -1\\\\0 &amp; 1 &amp; 2\\\\0 &amp; 0 &amp; 0\\end{bmatrix}\\] The first two rows of the reduced matrix are our basis: \\[\\left\\{\\begin{bmatrix}1 &amp; 0 &amp; -1\\end{bmatrix},\\begin{bmatrix}0 &amp; 1 &amp; 2\\end{bmatrix}\\right\\}.\\] Note that the second row of \\(\\mathbf{A}\\) are multiples of each other, so they cant form a basis for \\(\\text{Row}(\\mathbf{A}).\\) The row rank of a matrix is the dimension of its row space. Since its the number of nonzero rows in the reduced form of the matrix, its the number of rows with leading ones. But hey, the number of rows with leading ones is the same as the number of columns with leading ones in them, which is the column rank. Theorem 2.9 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. Then \\[\\text{dim}\\,\\text{Col}(\\mathbf{A})=\\text{dim}\\,\\text{Row}(\\mathbf{A})\\overset{\\text{def}}{=}\\text{rank} (\\mathbf{A}).\\] That is, the rank of a matrix is the row rank and the column rank, which are always the same number. Note that the column rank cant exceed the number of columns, while the row rank cant exceed the number of rows. Thus, for an \\(m\\times n\\) matrix \\(\\mathbf{A},\\) \\[\\text{rank}(\\mathbf{A})\\leq \\min\\{m,n\\}.\\] For example, a \\(3\\times 5\\) matrix and a \\(5\\times 3\\) matrix both can have a maximum rank of 3. If a matrix has the maximum rank possible for its size, it is called full rank. Otherwise, it is rank deficient. Example 2.11 Lets take another matrix and find the bases for all the subspaces weve seen. Since the augmented column is just a bunch of zeros, well leave it off. \\[\\mathbf{A}=\\begin{bmatrix}2 &amp; -1 &amp; 1 &amp; 5\\\\-4 &amp; -4 &amp; -4 &amp; -12\\\\6 &amp; -15 &amp; -1 &amp; 11\\\\0 &amp; -24 &amp; -8 &amp; -8\\end{bmatrix}\\leadsto \\begin{bmatrix}1&amp;0 &amp; 2/3 &amp; 8/3\\\\0 &amp; 1 &amp; 1/3 &amp; 1/3\\\\0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}.\\] From this, we can read off the following. A basis for \\(\\text{Col}(\\mathbf{A})\\) is \\(\\{(2,-4,6,0),(-1,-4,-15,-24)\\}\\). A basis for \\(\\text{Row}(\\mathbf{A})\\) is \\(\\{(1,0,2/3,8/3),(0,1,1/3,1/3)\\}\\). The rank is 2. The reduced row echelon form of the matrix is \\[\\begin{bmatrix}1&amp;0 &amp; 2/3 &amp; 8/3\\\\0 &amp; 1 &amp; 1/3 &amp; 1/3\\\\0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}.\\] Well assign parameters to the free variables: \\(x_3=s,x_4=t\\). Then \\[x_1+\\frac{2}{3}s+\\frac{8}{3}t=0\\to x_1=-\\frac{2}{3}s-\\frac{8}{3}t\\] and \\[x_2+\\frac13s+\\frac13t=0\\to x_2=x_2=-\\frac13s-\\frac13t.\\] so \\[\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\end{bmatrix}=\\begin{bmatrix}-\\frac{2}{3}s-\\frac{8}{3}t\\\\-\\frac13s-\\frac13t\\\\s\\\\t\\end{bmatrix}=s\\begin{bmatrix}-\\frac23\\\\-\\frac13\\\\1\\\\0\\end{bmatrix}+t\\begin{bmatrix}-\\frac83\\\\-\\frac13\\\\0\\\\1\\end{bmatrix}.\\] Our basis for \\(\\text{Null}(\\mathbf{A})\\) is \\[\\left\\{\\begin{bmatrix}-\\frac23\\\\-\\frac13\\\\1\\\\0\\end{bmatrix},\\begin{bmatrix}-\\frac83\\\\-\\frac13\\\\0\\\\1\\end{bmatrix}\\right\\}.\\] Note: we can multiply both vectors by 3 to get rid of the fractions. 2.5 Exercises Let \\[\\mathbf{u}=\\begin{bmatrix} 2\\\\4\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix} 3\\\\-1\\end{bmatrix}.\\] Find and graph the following. \\(3\\mathbf{u}\\) \\(-2\\mathbf{v}\\) \\(2\\mathbf{u}+3\\mathbf{v}\\) \\(2\\mathbf{u}-3\\mathbf{v}\\) Let \\[\\mathbf{u}=\\begin{bmatrix} 3\\\\1\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix} 4\\\\-2\\end{bmatrix}.\\] Find and graph the following. \\(2\\mathbf{u}\\) \\(-\\mathbf{v}\\) \\(3\\mathbf{u}+2\\mathbf{v}\\) \\(3\\mathbf{u}-2\\mathbf{v}\\) For \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) from Problem 1, find \\(|\\mathbf{u}|\\) and \\(|\\mathbf{v}|.\\) Find \\(|\\mathbf{u}|\\) if \\[\\mathbf{u}=\\begin{bmatrix} 1\\\\-1\\\\2\\end{bmatrix}.\\] Let \\[\\mathbf{u}=\\begin{bmatrix} 2\\\\1\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix} 4\\\\ d \\end{bmatrix}.\\] Find \\(d\\) so that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly dependent. Let \\[\\mathbf{u}=\\begin{bmatrix} 3\\\\2\\end{bmatrix}, \\mathbf{v}=\\begin{bmatrix} -4\\\\ d \\end{bmatrix}.\\] Find \\(d\\) so that \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly dependent. Let \\(W\\) be the set of all solutions to the equation \\[\\begin{bmatrix} 3 &amp; -1\\\\2 &amp; 5\\end{bmatrix}\\mathbf{w}=\\begin{bmatrix}1\\\\1\\end{bmatrix}.\\] Is \\(W\\) a subspace of \\(\\mathbb{R}^2\\)? Let \\(W\\) be the set of all solutions to the equation \\[\\begin{bmatrix} 2 &amp; -2\\\\1 &amp; 4\\end{bmatrix}\\mathbf{w}=\\begin{bmatrix}2\\\\8\\end{bmatrix}.\\] Is \\(W\\) a subspace of \\(\\mathbb{R}^2\\)? Let \\(W\\) be the set of all vectors \\(\\mathbf{w}\\) of the form \\(\\mathbf{w}=\\begin{bmatrix}w\\\\w^2\\end{bmatrix}, w\\in \\mathbb{R}.\\) Is \\(W\\) a subspace of \\(\\mathbb{R}^2\\)? Let \\(W\\) be the set of all vectors \\(\\mathbf{w}\\) of the form \\(\\mathbf{w}=\\begin{bmatrix}w\\\\w^2\\\\w^3\\end{bmatrix}, w\\in \\mathbb{R}.\\) Is \\(W\\) a subspace of \\(\\mathbb{R}^3\\)? Let \\(W\\) be the set of all vectors \\(\\mathbf{w}\\) of the form \\[\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\end{bmatrix}\\] that satisfy the equation \\(w_1+2w_2+3w_3=0.\\) is \\(W\\) a subspace of \\(\\mathbb{R}^3?\\) Let \\(W\\) be the set of all vectors \\(\\mathbf{w}\\) of the form \\[\\mathbf{w}=\\begin{bmatrix}w_1\\\\w_2\\\\w_3\\end{bmatrix}\\] that satisfy the equation \\(4w_1+5w_2-2w_3=0.\\) is \\(W\\) a subspace of \\(\\mathbb{R}^3?\\) Write the following product as a linear combination of vectors. Do not simplify. \\[\\begin{bmatrix} 1 &amp; 3 &amp; -2 \\\\2 &amp; 4 &amp; 5\\\\8&amp; 1 &amp; 2\\end{bmatrix} \\begin{bmatrix} 2\\\\-1\\\\4\\end{bmatrix}\\] Write the following product as a linear combination of vectors. Do not simplify. \\[\\begin{bmatrix} 2 &amp; 1 &amp; -4 \\\\-1 &amp; 3 &amp; 2\\\\ 6 &amp; 2 &amp; 3\\end{bmatrix} \\begin{bmatrix} 1\\\\-4\\\\2\\end{bmatrix}\\] Write the linear combination \\[3\\begin{bmatrix} 2\\\\-1\\end{bmatrix}+(-2)\\begin{bmatrix}1\\\\4\\end{bmatrix}\\] as a product of a matrix and a vector. Write the linear combination \\[9\\begin{bmatrix} 3\\\\-3\\end{bmatrix}+(-5)\\begin{bmatrix}2\\\\3\\end{bmatrix}\\] as a product of a matrix and a vector. Can the vector \\(\\mathbf{b}=\\begin{bmatrix}1\\\\4\\end{bmatrix}\\) be written as a linear combination of \\(\\mathbf{a}_1=\\begin{bmatrix}2\\\\-3\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix}5\\\\1\\end{bmatrix}?\\) If so, find the coefficients. Can the vector \\(\\mathbf{b}=\\begin{bmatrix}9\\\\4\\end{bmatrix}\\) be written as a linear combination of \\(\\mathbf{a}_1=\\begin{bmatrix}2\\\\5\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix}6\\\\1\\end{bmatrix}?\\) If so, find the coefficients. Can the vector \\(\\mathbf{b}=\\begin{bmatrix}1\\\\4\\\\3\\end{bmatrix}\\) be written as a linear combination of \\(\\mathbf{a}_1=\\begin{bmatrix}2\\\\-3\\\\5\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix}5\\\\1\\\\2\\end{bmatrix}?\\) If so, find the coefficients. Can the vector \\(\\mathbf{b}=\\begin{bmatrix}1\\\\1\\\\5\\end{bmatrix}\\) be written as a linear combination of \\(\\mathbf{a}_1=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix}1\\\\4\\\\-1\\end{bmatrix}?\\) If so, find the coefficients. Let \\(\\mathbf{a}_1=\\begin{bmatrix}2\\\\1\\\\3\\end{bmatrix},\\mathbf{a}_2=\\begin{bmatrix}2\\\\-1\\\\4\\end{bmatrix},\\) and \\(\\mathbf{a}_3=\\begin{bmatrix} 4\\\\0\\\\7\\end{bmatrix}.\\) Are \\(\\mathbf{a}_1,\\mathbf{a}_2,\\) and \\(\\mathbf{a}_3\\) linearly independent or linearly dependent? Let \\(\\mathbf{a}_1=\\begin{bmatrix}5\\\\2\\\\1\\end{bmatrix},\\mathbf{a}_2=\\begin{bmatrix}-1\\\\5\\\\3\\end{bmatrix},\\) and \\(\\mathbf{a}_3=\\begin{bmatrix} 2\\\\5\\\\2\\end{bmatrix}.\\) Are \\(\\mathbf{a}_1,\\mathbf{a}_2,\\) and \\(\\mathbf{a}_3\\) linearly independent or linearly dependent? Write the standard basis vector \\(\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}\\) as a linear combination of vectors in the basis \\(\\left\\{\\begin{bmatrix}1\\\\3\\end{bmatrix},\\begin{bmatrix}1\\\\4\\end{bmatrix}\\right\\}.\\) Write the standard basis vector \\(\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}\\) as a linear combination of vectors in the basis \\(\\left\\{\\begin{bmatrix}2\\\\7\\end{bmatrix},\\begin{bmatrix}1\\\\4\\end{bmatrix}\\right\\}.\\) The set of all \\(2\\times 2\\) matrices is a vector space. Find its dimension by finding a basis. The set of all \\(2\\times 3\\) matrices is a vector space. Find its dimension by finding a basis. Show that the set of all \\(2\\times 2\\) upper triangular matrices is a subspace of the vector space of all \\(2\\times 2\\) matrices. Find its dimension by finding a basis. Show that the set of all \\(3\\times 3\\) diagonal matrices is a subspace of the vector space of all \\(2\\times 2\\) matrices. Find its dimension by finding a basis. Let \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 3 &amp; 1 &amp; 2\\\\2 &amp; 1 &amp; 2 &amp; 3\\\\1 &amp; 2 &amp; 1 &amp; 4\\end{bmatrix}.\\] Find A basis for \\(\\text{Col}(\\mathbf{A}).\\) A basis for \\(\\text{Row}(\\mathbf{A}).\\) A basis for \\(\\text{Null}(\\mathbf{A}).\\) The rank of \\(\\mathbf{A}.\\) Let \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 2 &amp; 2 &amp; 1\\\\3 &amp; 3 &amp; 6 &amp; 1\\\\1 &amp; 2 &amp; 2 &amp; 4\\end{bmatrix}.\\] Find A basis for \\(\\text{Col}(\\mathbf{A}).\\) A basis for \\(\\text{Row}(\\mathbf{A}).\\) A basis for \\(\\text{Null}(\\mathbf{A}).\\) The rank of \\(\\mathbf{A}.\\) Let \\(\\mathbf{A}=\\begin{bmatrix} 1 &amp; 3 &amp; -4\\end{bmatrix}.\\) Find a basis for the nullspace of \\(\\mathbf{A}.\\) Let \\(\\mathbf{A}=\\begin{bmatrix} 2 &amp; 4 &amp; 1\\end{bmatrix}.\\) Find a basis for the nullspace of \\(\\mathbf{A}.\\) Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix and \\(\\mathbf{b}\\) be a vector in \\(\\mathbb{R}^m.\\) Let \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) be two solutions to \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}.\\) Show that \\(\\mathbf{x}_1-\\mathbf{x}_2\\) is in the nullspace of \\(\\mathbf{A}.\\) Of course, we could also use inverses or technology. "],["eigenvalues-and-eigenvectors.html", "Chapter 3 Eigenvalues and Eigenvectors 3.1 Eigenvalues and Eigenvectors 3.2 Special Cases and Complications 3.3 Application: The Leslie Matrix 3.4 Graphs and Adjacency Matrices 3.5 Random surfers and Stochastic Matrices 3.6 The Markov and Oracle Ranking Methods 3.7 Exercises", " Chapter 3 Eigenvalues and Eigenvectors Eigenvalues and eigenvectors play a critical role in many matrix applications. In this chapter, we will introduce the paired concepts and see some applications of them. 3.1 Eigenvalues and Eigenvectors When a vector is multiplied (on the left) by an appropriately sized matrix, you get another vector. This is sometimes referred to as a linear transformation. For instance if \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2 \\\\-1 &amp; 3\\end{bmatrix}\\] and \\[\\mathbf{x}=\\begin{bmatrix} 2\\\\1\\end{bmatrix},\\] then \\[\\mathbf{A}\\mathbf{x}=\\begin{bmatrix} 1 &amp; 2 \\\\-1 &amp; 3 \\end{bmatrix}\\begin{bmatrix} 2\\\\1\\end{bmatrix}=\\begin{bmatrix}4\\\\1\\end{bmatrix}\\] Multiplication by \\(\\mathbf{A}\\) transforms the vector \\(\\mathbf{x}=(2,1)\\) into the vector \\((4,1)\\) (Fig. 3.1). Figure 3.1: A linear transformation Note that \\[\\mathbf{A}\\mathbf{e}_1=\\begin{bmatrix} 1 &amp; 2 \\\\-1 &amp; 3\\end{bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix}=\\begin{bmatrix}1\\\\-1\\end{bmatrix},\\] which is the first column of \\(\\mathbf{A},\\) while \\[\\mathbf{A}\\mathbf{e}_2=\\begin{bmatrix} 1 &amp; 2 \\\\-1 &amp; 3\\end{bmatrix}\\begin{bmatrix}0\\\\1\\end{bmatrix}=\\begin{bmatrix}2\\\\3\\end{bmatrix},\\] which is the second column of \\(\\mathbf{A}.\\) In general, if \\(\\mathbf{A}\\) is any matrix and \\(\\mathbf{e}_i\\) is the standard basis vector of appropriate length, then \\(\\mathbf{A}\\mathbf{e}_i=\\mathbf{a}_i,\\) the \\(i\\)th column of \\(\\mathbf{A}\\)! Now consider the product \\[\\mathbf{A}\\mathbf{x}=\\begin{bmatrix}1&amp; -1\\\\2&amp;4\\end{bmatrix}\\begin{bmatrix}-1\\\\2\\end{bmatrix}=\\begin{bmatrix}-3\\\\6\\end{bmatrix}.\\] For this matrix/vector pair, we have \\(\\mathbf{A}\\mathbf{x}=3\\mathbf{x}.\\) Multiplication by \\(\\mathbf{A}\\) stretches this particular vector by a factor of \\(3\\). This doesnt happen with every vector. Indeed, \\[\\mathbf{A}\\mathbf{e}_1=\\begin{bmatrix}1\\\\2\\end{bmatrix}.\\] The vector \\(\\mathbf{x}=\\begin{bmatrix}-1\\\\2\\end{bmatrix}\\) is what we call an eigenvector for the matrix \\(\\mathbf{A},\\) and the scaling factor 3 is called an eigenvalue of \\(\\mathbf{A}.\\) Definition 3.1 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) (square) matrix, and let \\(\\mathbf{x}\\) be a nonzero vector such that \\(\\mathbf{A}\\mathbf{x}=\\lambda \\mathbf{x}\\) for some scalar \\(\\lambda\\). Then \\(\\mathbf{x}\\) is an eigenvector of \\(\\mathbf{A}\\) with corresponding eigenvalue \\(\\lambda\\). Some notes: While the zero vector satisfies \\(\\mathbf{A0}=\\mathbf{0}\\) for matrices \\(\\mathbf{A},\\) we never call it an eigenvector. Eigenvectors arent unique. For instance, if \\(\\mathbf{A}\\mathbf{x}=\\lambda \\mathbf{x}\\), and \\(r\\) is any nonzero constant, then \\[\\mathbf{A}(r\\mathbf{x})=r\\mathbf{A}\\mathbf{x}=r\\lambda \\mathbf{x}=\\lambda(r\\mathbf{x}),\\] so any nonzero multiple of an eigenvector is also an eigenvector for the same eigenvalue. Some applications use eigenvectors of magnitude 1, while others use eigenvalues whose entries add up to 1. Now that weve defined eigenvectors and eigenvalues, we need to figure out how to find them. Eigenvectors and eigenvalues satisfy the equation \\[\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x}.\\] With a little rearrangement, and using the trick \\(\\mathbf{I}\\mathbf{x}=\\mathbf{x}\\), we get the following: \\[\\begin{align*} \\mathbf{A}\\mathbf{x}&amp;=\\lambda \\mathbf{x}\\\\ \\mathbf{A}\\mathbf{x}-\\lambda\\mathbf{x}&amp;=\\mathbf{0}\\\\ \\mathbf{A}\\mathbf{x}-\\lambda \\mathbf{I}\\mathbf{x}&amp;=\\mathbf{0}\\\\ (\\mathbf{A}-\\lambda \\mathbf{I})\\mathbf{x}&amp;=\\mathbf{0}. \\end{align*}\\] If \\(\\mathbf{A}\\) is a square \\(n\\times n\\) matrix, then \\(\\mathbf{B}=\\mathbf{A}-\\lambda\\mathbf{I}\\) is also an \\(n\\times n\\) matrix. Remember that for a square matrix \\(\\mathbf{B},\\) \\[\\mathbf{B}\\mathbf{x}=\\mathbf{0}\\] has a nontrivial solution if and only if \\(\\det \\mathbf{B}=0.\\) The matrix \\(\\mathbf{A}\\) will have the eigenvalue \\(\\lambda\\) if and only if \\[ \\det(\\mathbf{A}-\\lambda\\mathbf{I})=0. \\tag{3.1} \\] This is the characteristic equation for the matrix \\(\\mathbf{A}.\\) Any nontrivial solution to the characteristic equation will be an eigenvector corresponding to the eigenvalue \\(\\lambda.\\) To sum up, we have the following. Theorem 3.1 The value \\(\\lambda\\) is an eigenvalue for the \\(n\\times n\\) matrix \\(\\mathbf{A}\\) if and only if \\[\\det(\\mathbf{A}-\\lambda\\mathbf{I})=0.\\] Corresponding eigenvectors are nonzero solutions to \\[(\\mathbf{A}-\\lambda \\mathbf{I})\\mathbf{x}=\\mathbf{0}.\\] For a given matrix \\(\\mathbf{A},\\) \\(\\det(\\mathbf{A}-\\lambda\\mathbf{I})\\) is a polynomial in \\(\\lambda\\), called the characteristic polynomial of \\(\\mathbf{A}.\\) Example 3.1 Lets find eigenvalues and eigenvectors for the matrix \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; -1\\\\2 &amp; 4\\end{bmatrix}.\\] We have \\[\\mathbf{A}-\\lambda \\mathbf{I}=\\begin{bmatrix}1 &amp; -1\\\\2 &amp; 4\\end{bmatrix}-\\begin{bmatrix}\\lambda &amp; 0\\\\0 &amp; \\lambda\\end{bmatrix}=\\begin{bmatrix}1-\\lambda &amp; -1\\\\2 &amp; 4-\\lambda\\end{bmatrix}.\\] Then \\[\\begin{align*} \\det(\\mathbf{A}-\\lambda \\mathbf{I})&amp;=(1-\\lambda)(4-\\lambda)-(-1)(2) \\\\ &amp;=4-\\lambda-4\\lambda+\\lambda^2+2\\\\ &amp;=\\lambda^2-5\\lambda+6=(\\lambda-2)(\\lambda-3). \\end{align*}\\] The characteristic equation \\(\\det(\\mathbf{A}-\\lambda \\mathbf{I})=0\\) for this matrix is \\[(\\lambda-2)(\\lambda-3)=0.\\] The eigenvalues are \\(\\lambda_1=3\\) and \\(\\lambda_2=2\\). (Lets write them in decreasing order.) Now that we know the eigenvalues, we can solve for the corresponding eigenvectors. We will start with \\(\\lambda_1=3\\). A corresponding eigenvector must satisfy \\[(\\mathbf{A}-3\\mathbf{I})\\mathbf{x}=\\mathbf{0}.\\] With \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; -1\\\\2 &amp; 4\\end{bmatrix},\\] \\[\\mathbf{A}-3\\mathbf{I}=\\begin{bmatrix}1-3 &amp; -1 \\\\2 &amp; 4-3\\end{bmatrix}=\\begin{bmatrix}-2 &amp; -1 \\\\2 &amp; 1\\end{bmatrix}.\\] To solve \\((\\mathbf{A}-3\\mathbf{I})\\mathbf{x}=\\mathbf{0}\\), we reduce: \\[\\left[\\begin{array}{rr|r}-2 &amp; -1 &amp; 0\\\\2 &amp; 1 &amp; 0\\end{array}\\right] \\leadsto \\left[\\begin{array}{rr|r} 1 &amp; 1/2 &amp; 0\\\\0 &amp; 0 &amp; 0\\end{array}\\right].\\] Any vector \\(\\mathbf{x}=(x_1,x_2)\\) that satisfies \\(x_1=-\\frac{1}{2}x_2\\) will be an eigenvector. For instance, \\(\\mathbf{x}=(-1,2)\\) is the one we saw in the example. Well let \\(\\mathbf{x}=\\mathbf{x}_1=(-1,2)\\) be our eigenvector corresponding to \\(\\lambda_1=3.\\) Note that in \\[\\mathbf{A}-3\\mathbf{I}=\\begin{bmatrix}-2 &amp; -1 \\\\2 &amp; 1\\end{bmatrix},\\] the rows are multiples of each other. We can actually use either row to find a relationship between the coordinates, without having to row reduce. (This only works for \\(2\\times 2\\) matrices.) The second row tells us that \\(2x_1+x_2=0\\), or \\(x_2=-2x_1\\). If we let \\(x_1=1\\), we get \\(x_2=-2\\), for an eigenvector of \\(\\mathbf{x}_1=(1,-2)\\), which is a multiple of the one we observed earlier. To sum up, for \\(2\\times 2\\) matrices, once weve found \\(\\mathbf{A}-\\lambda \\mathbf{I}\\), we can jump straight to the eigenvector. Now lets find an eigenvector corresponding to \\(\\lambda_2=2\\). We have \\[\\mathbf{A}-2\\mathbf{I}=\\begin{bmatrix}1-2 &amp; -1 \\\\2 &amp; 4-2\\end{bmatrix}=\\begin{bmatrix}-1 &amp; -1 \\\\2 &amp; 2\\end{bmatrix}.\\] The first row tells us that \\(-x_1-x_2=0,\\) or \\(x_2=-x_1\\). The vector \\(\\mathbf{x}_2=(1,-1)\\) works. Lets check: \\[\\mathbf{A}\\mathbf{x}_2=\\begin{bmatrix}1 &amp; -1\\\\2 &amp; 4\\end{bmatrix}\\begin{bmatrix}1\\\\-1\\end{bmatrix}=\\begin{bmatrix}2\\\\-2\\end{bmatrix}=2\\mathbf{x}_2\\checkmark.\\] Eigenvalues and Eigenvectors in R We can use R to find eigenvalues and eigenvectors via the eigen function from the base package. If they are all real numbers, the eigenvalues will be listed in descending order. The corresponding eigenvectors are listed in the same order. The eigenvectors are scaled so that the have magnitude 1. A &lt;- matrix(c(1,2,-1,4), nrow = 2) A ## [,1] [,2] ## [1,] 1 -1 ## [2,] 2 4 eigen(A) ## eigen() decomposition ## $values ## [1] 3 2 ## ## $vectors ## [,1] [,2] ## [1,] 0.4472136 -0.7071068 ## [2,] -0.8944272 0.7071068 If you give a name to the eigen output, it makes it a little easier to extract the eigenvectors. A.ev &lt;- eigen(A) evectors &lt;- A.ev$vectors evectors ## [,1] [,2] ## [1,] 0.4472136 -0.7071068 ## [2,] -0.8944272 0.7071068 evectors[,1] ## [1] 0.4472136 -0.8944272 evectors[,2] ## [1] -0.7071068 0.7071068 Another option for finding eigenvalues and eigenvectors for small matrices is WolframAlpha.com. After you enter a matrix, the engine will spit out several of its properties. As of this writing, the Desmos.com matrix calculator does not do eigenvalue/eigenvector computations. Higher dimensional matrices The characteristic polynomial for a \\(2\\times 2\\) matrix is a quadratic polynomial. The characteristic polynomial for an \\(n\\times n\\) matrix will be an \\(n\\)th degree polynomial, which makes it harder or impossible to do by hand. Example 3.2 Find the eigenvalues and eigenvectors for \\[\\mathbf{A}=\\begin{bmatrix}-1 &amp; 2 &amp; 0\\\\2 &amp; 0 &amp; 2\\\\0 &amp; 2 &amp; 1\\end{bmatrix}.\\] We have \\[\\begin{align*}\\det(\\mathbf{A}-\\lambda\\mathbf{I})&amp;=\\begin{vmatrix}-1-\\lambda &amp; 2 &amp; 0\\\\2 &amp; -\\lambda &amp; 2\\\\0 &amp; 2 &amp; 1-\\lambda\\end{vmatrix}\\\\ &amp;=-\\lambda^3+9\\lambda\\\\ &amp;=-\\lambda(\\lambda^2-9)\\\\ &amp;=-\\lambda(\\lambda+3)(\\lambda-3) \\end{align*}\\] The eigenvalues are \\(\\lambda_1=3,\\lambda_2=0,\\) and \\(\\lambda_3=-3.\\) For \\(\\lambda_1=3,\\) we reduce. \\[\\begin{align*} \\left[\\begin{array}{ccc|c}-1-3 &amp; 2 &amp; 0&amp;0\\\\2 &amp; 0-3 &amp; 2&amp;0\\\\0 &amp; 2 &amp; 1-3&amp;0\\end{array}\\right]&amp;=\\left[\\begin{array}{ccc|c} -4 &amp; 2 &amp; 0&amp;0\\\\2 &amp; -3 &amp; 2&amp;0\\\\0 &amp; 2 &amp; -2&amp;0\\end{array}\\right] \\\\ &amp;\\leadsto \\left[\\begin{array}{ccc|c} 1 &amp; 0 &amp; -1/2 &amp;0\\\\0 &amp; 1 &amp; -1&amp;0\\\\0 &amp; 0 &amp; 0&amp;0\\end{array}\\right] \\end{align*}\\] We get that \\(x_3\\) is free, \\(x_1=\\frac{1}{2}x_3\\) and \\(x_2=x_3\\). Setting \\(x_3=2\\) gives us \\(\\mathbf{x}_1=(1,2,2).\\) For \\(\\lambda_2=0,\\) we get the following. \\[ \\left[\\begin{array}{ccc|c}-1 &amp; 2 &amp; 0&amp;0\\\\2 &amp; 0 &amp; 2&amp;0\\\\0 &amp; 2 &amp; 1&amp;0\\end{array}\\right]\\leadsto \\left[\\begin{array}{ccc|c} 1 &amp; 0 &amp; 1 &amp;0\\\\0 &amp; 1 &amp; 1/2&amp;0\\\\0 &amp; 0 &amp; 0&amp;0\\end{array}\\right] \\] Again, with \\(x_3\\) free, we get \\(x_1=-x_3\\) and \\(x_2=-\\frac{1}{2}x_3.\\) Setting \\(x_3=2\\) gives us an eigenvector of \\(\\mathbf{x}_2=(-2,1,2)\\). For \\(\\lambda_3=-3,\\) we have this. \\[\\begin{align*} \\left[\\begin{array}{ccc|c}-1+3 &amp; 2 &amp; 0&amp;0\\\\2 &amp; 0+3 &amp; 2&amp;0\\\\0 &amp; 2 &amp; 1+3&amp;0\\end{array}\\right]&amp;=\\left[\\begin{array}{ccc|c} 2 &amp; 2 &amp; 0&amp;0\\\\2 &amp; 3 &amp; 2&amp;0\\\\0 &amp; 2 &amp; 4&amp;0\\end{array}\\right] \\\\ &amp;\\leadsto \\left[\\begin{array}{ccc|c} 1 &amp; 0 &amp; -2 &amp;0\\\\0 &amp; 1 &amp; 2&amp;0\\\\0 &amp; 0 &amp; 0&amp;0\\end{array}\\right] \\end{align*}\\] The relationships are \\(x_1=2x_3\\) and \\(x_2=-2x_3\\), so letting \\(x_3=2\\) gives us our final eigenvector of \\(\\mathbf{x}_3=(2,-2,1).\\) 3.2 Special Cases and Complications There are some matrices for which it is easy to find eigenvalues. Diagonal and triangular matrices We talked about diagonal and triangular matrices in Section 1.6. It is easy to find their eigenvalues. Example 3.3 Find the eigenvalues and eigenvectors for \\(\\mathbf{A},\\) where \\[\\mathbf{A}=\\begin{bmatrix}2 &amp; 3\\\\0 &amp; 5\\end{bmatrix}.\\] Note that \\[\\det(\\mathbf{A}-\\lambda \\mathbf{I})=\\begin{vmatrix}2-\\lambda &amp; 3\\\\0 &amp; 5-\\lambda\\end{vmatrix}=(2-\\lambda)(5-\\lambda),\\] so the eigenvalues are 2 and 5, the entries on the diagonal. When we subtract \\(\\lambda\\) from the diagonal, a triangular or diagonal matrix stays triangular or diagonal, and the determinant is the product of the diagonal entries. This gives us the following theorem. Theorem 3.2 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) diagonal, upper triangular, or lower triangular matrix. The eigenvalues of \\(\\mathbf{A}\\) are the entries on the diagonal of \\(\\mathbf{A}.\\) This means that we can immediately read off the eigenvalues for these special matrices. Returning to Example 3.3, we should find the eigenvectors. Well start with \\(\\lambda_2=2.\\) First, remember that the columns of a matrix tell us what happen to the standard basis vectors. Note that \\[\\mathbf{A}\\begin{bmatrix}1 \\\\0\\end{bmatrix}=\\begin{bmatrix}2\\\\0\\end{bmatrix},\\] so \\(\\mathbf{x}_2=\\begin{bmatrix}1\\\\0\\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda_2=2.\\) We still need to find the eigenvector corresponding to \\(\\lambda_1=5.\\) This one takes a little more work. \\[\\left[\\begin{array}{cc|c} 2-5 &amp; 3 &amp; 0\\\\0 &amp; 5-5 &amp; 0\\end{array}\\right]=\\left[\\begin{array}{rr|r}-3 &amp; 3&amp; 0\\\\0 &amp; 0 &amp; 0\\end{array}\\right].\\] The first row tells us that \\(3x_2=3x_1\\) or \\(x_2=x_1\\). Thus \\(\\mathbf{x}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) is an eigenvector corresponding to \\(\\lambda_1=5.\\) Note that the standard basis vectors are all eigenvectors for diagonal matrices. For example, \\[\\begin{bmatrix} 2 &amp; 0 &amp; 0\\\\ 0 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 5\\end{bmatrix}\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}=\\begin{bmatrix}0 \\\\3\\\\0\\end{bmatrix}.\\] The trace For a generic \\(2\\times 2\\) matrix \\(\\mathbf{A},\\) the characteristic polynomial is: \\[\\begin{align*} \\begin{vmatrix} a-\\lambda &amp; b\\\\c &amp; d-\\lambda\\end{vmatrix}&amp;=(a-\\lambda)(d-\\lambda)-bc\\\\ &amp;=ad-a\\lambda-d\\lambda+\\lambda^2-bc\\\\ &amp;=\\lambda^2-(a+d)\\lambda+(ad-bc). \\end{align*}\\] Suppose the eigenvalues are \\(\\lambda_1\\) and \\(\\lambda_2\\). Then the characteristic polynomial is \\[(\\lambda-\\lambda_1)(\\lambda-\\lambda_2)=\\lambda^2-(\\lambda_1+\\lambda_2)\\lambda+\\lambda_1\\lambda_2.\\] Matching coefficients gives us the following: \\[\\lambda_1+\\lambda_2=a+d\\text{ and }\\lambda_1\\lambda_2=ad-bc.\\] First note that \\(\\lambda_1\\lambda_2=ad-bc=\\det \\mathbf{A}\\). The determinant of a \\(2\\times 2\\) matrix is the product of the eigenvalues. The \\(\\lambda\\)-coefficient is \\(-(\\lambda_1+\\lambda_2)=-(a+d)\\), and \\(a+d\\) is the sum of the diagonal entries. This is called the trace of a matrix. Definition 3.2 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) matrix. The trace of \\(\\mathbf{A},\\) denoted \\(\\text{tr }\\mathbf{A}\\), is the sum of the diagonal entries of \\(\\mathbf{A}.\\) For a \\(2\\times 2\\) matrix, the trace is the sum of the eigenvalues. This relationship  and the relationship between the eigenvalues and the determinant  also hold for larger matrices Theorem 3.3 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) matrix with eigenvalues \\(\\lambda_1,\\lambda_2,\\dots,\\lambda_n\\), including repetitions. Then \\[\\det \\mathbf{A}= \\lambda_1\\lambda_2\\cdots \\lambda_n,\\] and \\[\\text{tr } \\mathbf{A}= \\lambda_1+\\lambda_2+\\cdots+\\lambda_n.\\] Note that these are somewhat obvious for diagonal and triangular matrices, but are also true for all square matrices. Example 3.4 Find a nondiagonal/nontriangular \\(2\\times 2\\) matrix with eigenvalues \\(\\lambda_1=5\\) and \\(\\lambda_2=-1\\). Let \\(\\mathbf{A}=\\begin{bmatrix}a &amp; b\\\\c&amp; d\\end{bmatrix}.\\) We know that \\(\\text{tr }\\mathbf{A}=\\lambda_1+\\lambda_2=5+-1=4.\\) We can pick any pair of diagonal entries that add up to 4 (other than 5 and \\(-1\\)). Lets choose \\(a=2\\) and \\(d=2\\). The determinant is \\(\\lambda_1\\lambda_2=5(-1)=-5.\\) With \\(a=d=2, \\det\\mathbf{A}=4-bc=-5,\\) so \\(bc=9.\\) We can choose any pair of numbers whose product is 9. Lets let \\(b=c=3.\\) Then our matrix is \\[\\mathbf{A}=\\begin{bmatrix}2&amp; 3\\\\3 &amp; 2\\end{bmatrix}.\\] Complications So far, all of our examples have involved matrices with real, distinct eigenvalues. Because we are dealing with solutions to polynomial equations, things can become messier. As an example, note that \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 0\\\\0 &amp; 2\\end{bmatrix}\\) and \\(\\mathbf{B}=\\begin{bmatrix}2 &amp; 3\\\\0 &amp; 2\\end{bmatrix}\\) both have the repeated eigenvalue 2. Since \\(\\mathbf{A}\\) is diagonal, \\(\\mathbf{e}_1\\) and \\(\\mathbf{e}_2\\) are two linearly independent eigenvectors for it. (In fact, any nonzero vector in \\(\\mathbb{R}^2\\) is an eigenvector, since \\(\\mathbf{A}=2\\mathbf{I}\\).) For \\(\\mathbf{B},\\) \\(\\mathbf{e}_1\\) is also an eigenvector, but \\(\\mathbf{e}_2\\) isnt. If we try to solve for one in the normal way, \\[\\left[\\begin{array}{cc|c}2-2 &amp; 3 &amp; 0\\\\0 &amp; 2-2 &amp; 0\\end{array}\\right]=\\left[\\begin{array}{cc|c}0 &amp; 3&amp; 0\\\\0 &amp; 0 &amp; 0\\end{array}\\right],\\] we get that \\(x_2=0\\), so we only have a single linearly independent eigenvector \\(\\mathbf{e}_1=(1,0)\\). In this case, \\(\\lambda=2\\) is what we call a deficient eigenvalue. Its also possible for a matrix to have complex-valued eigenvalues. For example, with \\(\\text{tr }\\mathbf{A}=-3,\\det \\mathbf{A}=5,\\) \\[\\mathbf{A}=\\begin{bmatrix} -1 &amp; -3\\\\1 &amp; -2\\end{bmatrix}\\] has characteristic equation \\[\\lambda^2+3\\lambda+5=0,\\] so the eigenvalues are \\[\\lambda=\\frac{-3\\pm \\sqrt{3^2-4\\cdot 1\\cdot 5}}{2}=\\frac{-3\\pm \\sqrt{-11}}{2}=-\\frac{3}{2}\\pm i\\frac{\\sqrt{11}}{2}.\\] Fortunately, for our applications, the matrices will either only have all real eigenvalues or well only need the real eigenvalues. Eigenvalues of a transpose How do the eigenvalues of a matrix and its transpose compare? Note that, since \\(\\mathbf{I}^T=\\mathbf{I},\\) \\[\\begin{align*} \\det (\\mathbf{A}^T-\\lambda \\mathbf{I})&amp;=\\det (\\mathbf{A}^T-\\lambda \\mathbf{I}^T)\\\\ &amp;=\\det (\\mathbf{A}-\\lambda \\mathbf{I})^T\\\\ &amp;=\\det (\\mathbf{A}-\\lambda \\mathbf{I}), \\end{align*}\\] so \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\) have the same characteristic polynomial. Since they have the same characteristic polynomial, they have the same eigenvalues. They dont necessarily have the same eigenvectors, however. Theorem 3.4 Let \\(\\mathbf{A}\\) be a square matrix. Then \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\) have the same eigenvalues. Matrix powers Suppose that \\(\\mathbf{A}\\) is an \\(n\\times n\\) matrix, with eigenvalue \\(\\lambda\\) and corresponding eigenvector \\(\\mathbf{x}\\). Then \\[\\begin{align*} \\mathbf{A}\\mathbf{x}&amp;=\\lambda \\mathbf{x}\\\\ \\mathbf{A}^2\\mathbf{x}&amp;=\\mathbf{A}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}(\\lambda \\mathbf{x})=\\lambda(\\mathbf{A}\\mathbf{x})=\\lambda(\\lambda \\mathbf{x})=\\lambda^2\\mathbf{x}\\\\ \\vdots &amp; \\\\ \\mathbf{A}^n\\mathbf{x}&amp;=\\lambda^n \\mathbf{x}. \\end{align*}\\] So: Eigenvectors of \\(\\mathbf{A}\\) are eigenvectors of \\(\\mathbf{A}^n\\) with eigenvalues \\(\\lambda^n, n=2,3,\\dots\\) This makes it easy to calculate \\(\\mathbf{A}^n\\mathbf{x}\\) for eigenvectors. Well take advantage of this in upcoming sections. Example 3.5 Let \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; -1\\\\2 &amp; 4\\end{bmatrix}.\\] We know from Example 3.1 that \\(\\mathbf{x}=\\begin{bmatrix}-1 \\\\2\\end{bmatrix}\\) is an eigenvector of \\(\\mathbf{A}\\) with corresponding eigenvalue \\(\\lambda=3\\). Then \\[\\mathbf{A}^4\\mathbf{x}=3^4\\mathbf{x}=81\\begin{bmatrix}-1\\\\2\\end{bmatrix}=\\begin{bmatrix}-81\\\\162\\end{bmatrix}.\\] 3.3 Application: The Leslie Matrix In this section, we will be studying an application of eigenvalues and eigenvectors to population modeling. Patrick Leslie described his matrix method for studying discrete time, age-structured population models in 1945 [10]. The model includes year-to-year survival rates as well as reproduction rates. We will start with an example. Example 3.6 Suppose a species lives at most two years, reproduces each year, and dies. We will consider the zero-year-olds, one-year-olds, and two-year-olds separately. Each age 0 individual has a probability of surviving to age 1 of 0.2, and produces on average 2 offspring. Each age 1 individual has a probability of surviving to age 2 of 0.5, and produces an average of 4 offspring. Each age 2 individual has a survival probability of 0, but produces an average of 3 offspring. The Leslie matrix for this population looks like this: \\[\\mathbf{L}=\\begin{bmatrix} 2 &amp; 4 &amp; 3\\\\ 0.2 &amp; 0 &amp; 0\\\\ 0 &amp; 0.5 &amp; 0\\end{bmatrix}.\\] Before we explain what we do with the matrix, we need to talk about population vectors. Let \\(N_0(t)\\) represent the number of age 0 individuals at year \\(t\\), \\(N_1(t)\\) represent the number of one-year-olds, and \\(N_2(t)\\) is the number of two-year-olds. We can combine these populations into a single vector: \\[\\mathbf{N}(t)=\\begin{bmatrix}N_0(t)\\\\N_1(t)\\\\N_2(t)\\end{bmatrix}.\\] For example, if we start with 10 age-zero, 20 age-1 and 20 age-2 individuals, then \\[\\mathbf{N}(0)=\\begin{bmatrix}10\\\\20\\\\20\\end{bmatrix}.\\] To find the population distribution for the next year, we multiply \\(\\mathbf{N}(t)\\) by \\(\\mathbf{L}\\): \\[\\mathbf{N}(1)=\\mathbf{L}\\,\\mathbf{N}(0),\\] \\[\\mathbf{N}(2)=\\mathbf{L}\\,\\mathbf{N}(1),\\] and so on. In general, \\[\\mathbf{N}(t+1)=\\mathbf{L}\\,\\mathbf{N}(t).\\] With our initial distribution of \\(\\mathbf{N}(0)=(10,20,20)\\), the distribution one year later will be \\[\\mathbf{N}(1)=\\begin{bmatrix}2 &amp; 4 &amp; 3\\\\ 0.2 &amp; 0 &amp; 0\\\\0 &amp; 0.5 &amp; 0\\end{bmatrix}\\begin{bmatrix}10\\\\20\\\\20\\end{bmatrix}=\\begin{bmatrix}160\\\\2\\\\10\\end{bmatrix}.\\] Is this right? We have 20% of the age-0 individuals surviving to age 1. Fifty percent of the 20 age-one population (10) survive to age 2. The new age-zero population is \\[2\\cdot 10+4\\cdot 20+3\\cdot 20=20+80+60=160.\\] How it works: Remember that the columns of a matrix tell you what happens to the standard basis vector under multiplication. For instance, \\[\\mathbf{L}\\,\\mathbf{e}_1=\\begin{bmatrix}2&amp;4&amp;3\\\\0.2&amp;0 &amp; 0\\\\0 &amp; 0.5 &amp; 0\\end{bmatrix}\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}=\\begin{bmatrix}2\\\\0.2\\\\0\\end{bmatrix}.\\] A single age-0 individual will produce (on average) 2 offspring, has a probability of 0.2 of surviving to age 1, and does not contribute to the age-2 population. We can analyze the other columns in a similar manner. We can keep going. \\[\\mathbf{N}(2)=\\mathbf{L}\\mathbf{N}(1)=\\begin{bmatrix}2&amp;4&amp;3\\\\0.2&amp;0&amp;0\\\\0&amp;0.5&amp;0\\end{bmatrix}\\begin{bmatrix}160\\\\2\\\\10\\end{bmatrix}=\\begin{bmatrix}358\\\\32\\\\1\\end{bmatrix}.\\] Note that \\[\\mathbf{N}(2)=\\mathbf{L}\\,\\mathbf{N}(1)=\\mathbf{L}(\\mathbf{L}\\,\\mathbf{N}(0))=\\mathbf{L}^2\\,\\mathbf{N}(0).\\] In general, \\(\\mathbf{N}(t)=\\mathbf{L}^t\\,\\mathbf{N}(0)\\). Figure 3.2 shows a plot of the age group populations over time. Figure 3.2: Population growth If we plot the age groups share of the total population over time, something interesting happens (Fig. 3.3). Figure 3.3: Proportion of total population Note that after 1 year, the age-1 population is roughly 1.16% of the total population. Also note that the proportions seem to level off. Well investigate this more later. Lets talk about the general form of the Leslie matrix. Suppose a population can live to age \\(m\\). Then its Leslie matrix will be an \\((m+1)\\times(m+1)\\) matrix of the form \\[\\mathbf{L}=\\begin{bmatrix} F_0 &amp; F_1 &amp; F_2 &amp; \\cdots &amp; F_{m-1} &amp; F_m\\\\P_0 &amp; 0 &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; 0\\\\ 0 &amp; P_1 &amp; 0 &amp;\\cdots &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; \\vdots\\\\ 0 &amp; \\cdots &amp; \\cdots &amp; 0 &amp; P_{m-1} &amp; 0\\end{bmatrix},\\] where \\(F_i\\) is the average number of offspring that an age-\\(i\\) individual produces. (\\(F\\) is for fecundity), and \\(P_i\\) is the probability that an age-\\(i\\) individual survives to age \\(i+1\\). Suppose a Leslie matrix \\(L\\) has a positive eigenvalue \\(\\lambda\\), with a corresponding eigenvector \\(\\mathbf{v}\\) that has nonnegative entries. Then \\[\\mathbf{L}\\mathbf{v}=\\lambda \\mathbf{v}.\\] If \\(\\mathbf{v}\\) represents a population distribution, then in the next year, each age groups population is multiplied by the same constant, \\(\\lambda\\). That is, the proportions will stay constant, and \\(\\lambda\\) represents the growth rate. Well illustrate this with a \\(2\\times 2\\) example. Example 3.7 Let \\[\\mathbf{L}=\\begin{bmatrix}1.4 &amp; 3\\\\0.05 &amp; 0\\end{bmatrix}.\\] With \\(\\text{tr }\\mathbf{L}=1.4\\) and \\(\\det \\mathbf{L}=-0.15\\), the characteristic equation is \\[\\lambda^2-1.4\\lambda-0.15=0,\\] and the eigenvalues are \\(\\lambda_1=1.5\\) and \\(\\lambda_2=-0.1\\). Lets find an eigenvector corresponding to \\(\\lambda_1=1.6\\): \\[\\mathbf{L}-1.5\\mathbf{I}=\\begin{bmatrix} 1.4-1.5 &amp; 3\\\\0.05 &amp; 0-1.5\\end{bmatrix}=\\begin{bmatrix}-0.1 &amp; 3\\\\0.05 &amp; -1.5\\end{bmatrix}.\\] The first row tells us that \\(-0.1v_1+3v_2=0\\) or \\(v_1=30v_2\\). With \\(v_2=1, v_1=30\\). Our eigenvector is \\(\\mathbf{v}=(30,1)\\). Lets check: \\[\\mathbf{L}\\mathbf{v}=\\begin{bmatrix}1.4 &amp; 3\\\\0.05 &amp; 0\\end{bmatrix}\\begin{bmatrix}20\\\\1\\end{bmatrix}=\\begin{bmatrix}45\\\\1.5\\end{bmatrix}=1.5\\begin{bmatrix}30\\\\1\\end{bmatrix}.\\] We didnt find the other eigenvector, and we dont really need to know what it is. Lets just call it \\(\\mathbf{v}_2\\). We do know that it and \\(\\mathbf{v}_1\\) are linearly independent (why?), so they form a basis for \\(\\mathbb{R}^2\\). Let \\(\\mathbf{N}(0)\\) be any initial population distribution. Then for some constants \\(c_1\\) and \\(c_2\\), \\[\\mathbf{N}(0)=c_1\\mathbf{v}_1+c_2\\mathbf{v}_2.\\] Now lets iterate: \\[\\mathbf{N}(t)=\\mathbf{L}^t \\mathbf{N}(0)=\\mathbf{L}^t(c_1 \\mathbf{v}_1+c_2\\mathbf{v}_2)=c_1\\mathbf{L}^t\\mathbf{v}_1+c_2\\mathbf{L}^t\\mathbf{v}_2=c_1(1.6)^t\\mathbf{v}_1+c_2(-0.1)^t\\mathbf{v}_2.\\] As \\(t\\to\\infty\\), the \\(\\mathbf{v}_2\\) term will vanish, and \\(\\mathbf{N}(t)\\approx c_1\\lambda^t \\mathbf{v}_1\\). The populations will approach a 30:1 ratio in the long run. (Were glossing over some details here.) Will we always have this situation? For \\(2\\times 2\\) Leslie matrices, provided a positive fraction of zero-year-olds survive, and that some group produces offspring, there will always be a real, positive eigenvalue. For \\(2\\times 2\\) Leslie matrices, we have the following theorem.[4] Theorem 3.5 Let \\(\\mathbf{L}\\) be a \\(2\\times 2\\) Leslie matrix with eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\). The larger eigenvalue determines the growth rate. The eigenvector corresponding to the larger eigenvalue is a stable age distribution. With some added conditions that we wont get into here, the analogous result holds for larger Leslie matrices. Lets return to Example 3.6 with \\[\\mathbf{L}=\\begin{bmatrix} 2 &amp; 4 &amp; 3\\\\ 0.2 &amp; 0 &amp; 0\\\\ 0 &amp; 0.5 &amp; 0\\end{bmatrix}.\\] L &lt;- matrix(c(2,0.2,0,4,0,0.5,3,0,0), nrow = 3) L ## [,1] [,2] [,3] ## [1,] 2.0 4.0 3 ## [2,] 0.2 0.0 0 ## [3,] 0.0 0.5 0 ev &lt;- eigen(L) ev ## eigen() decomposition ## $values ## [1] 2.3876762+0.0000000i -0.1938381+0.2967692i -0.1938381-0.2967692i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.99635800+0i 0.7157844+0.0000000i 0.7157844+0.0000000i ## [2,] 0.08345839+0i -0.2208541-0.3381312i -0.2208541+0.3381312i ## [3,] 0.01747691+0i -0.2289662+0.5216492i -0.2289662-0.5216492i Note the complex eigenvalues and eigenvectors. The first eigenvalue is purely real, as are the components of the first eigenvector. We can use the Re function to strip off the 0I terms from that eigenvector. We will also rescale it so that the terms sum to 1. Then the components will represent the long term population proportions for each age group. ev1 &lt;- Re(ev$vectors[,1]) ev1 &lt;- ev1/sum(ev1) ev1 ## [1] 0.90801430 0.07605841 0.01592729 In the long run, the percentages of 0-, 1-, and 2-year-olds approach 90.8%, 7.6%, and 1.6%, respectively. 3.4 Graphs and Adjacency Matrices Were going to see a few more applications of eigenvalues and eigenvectors, but before we get to them, we need to talk about graphs. A graph involves a collection of points, called vertices, and lines connecting them, called edges. Note that it can be hard to draw a graph in two dimensions without the edges intersecting. These intersections are not part of the graph, in the sense that one cant switch from one edge to another without going through a vertex. Figure 3.4: A graph The following code produces the graph in 3.4. It uses the igraph package. One note: there is a bit of randomness involved in the plot, so if you want a reproducible graph, you might want to set the seed first. See Appendix B for more sample graph code. library(igraph) edges &lt;- c(1,2,1,3,1,5,2,3,2,4,4,1,2,5) g &lt; -graph(edges, directed = F) plot(g, vertex.size = 30, vertex.color = &quot;lightblue&quot;) Graphs can be used to describe road and other types of networks, and even the results of tournaments (Figs. 3.5,3.6). Figure 3.5: A fictional bus network Figure 3.6: A tournament graph Sometimes there is an edge from a vertex to itself, called a loop. Sometimes pairs of vertices are connected by more than one edge (Fig. 3.7). Figure 3.7: Loops and multiple edges Sometimes the connection only goes one way. For example, if the vertices represent websites, there might be a link from one to another. In these cases, we use a directed edge, with the direction indicated by an arrow. Figure 3.8: A directed graph Adjacency matrices An adjacency matrix is a matrix that contains information about the edges and vertices of a graph. Definition 3.3 For an (undirected) graph with \\(n\\) vertices, the corresponding adjacency matrix is an \\(n\\times n\\) matrix \\(\\mathbf{A}=[a_{ij}]\\), whose \\(ij\\)-th entry is the number of edges connecting vertex \\(i\\) and vertex \\(j\\). Here is the adjacency matrix for the graph in Figure 3.4. \\[\\mathbf{A}=\\begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] The graph in Figure 3.7 has the following adjacency matrix. There are two edges between vertex 1 and vertex 2, so \\(a_{12}=a_{21}=2\\), and the loop from vertex 5 to itself gives us \\(a_{55}=1.\\) \\[\\begin{bmatrix} 0 &amp; 2 &amp; 1 &amp; 1 &amp; 1\\\\ 2 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] Example 3.8 (Bus Routes) We saw this bus network graph before. The two edges connecting Austin and San Antonio indicate two different routes. The adjacency matrix for this bus network looks like this. The vertices are numbered in alphabetical order (1 = Austin, 2 = Dallas, 3 = Houston, 4 = San Antonio). \\[\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 2\\\\ 0 &amp; 0 &amp; 1 &amp; 1\\\\ 0 &amp; 1 &amp; 0 &amp; 1\\\\ 2 &amp; 1 &amp; 1 &amp; 0 \\end{bmatrix}\\] Positive integer powers of the adjacency matrix have a very nice property. Theorem 3.6 Let \\(\\mathbf{A}\\) be an adjacency matrix for a graph. If \\(r\\) is a positive integer, then the \\(ij\\)-th entry of \\(\\mathbf{A}^r\\) is the number of routes from vertex \\(i\\) to vertex \\(j\\) that take exactly \\(r\\) steps. For the bus network, we have \\[\\mathbf{A}^2=\\begin{bmatrix} 4 &amp; 2 &amp; 2 &amp; 0\\\\ 2 &amp; 2 &amp; 1 &amp; 1\\\\ 2 &amp; 1 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; 1 &amp; 6 \\end{bmatrix}\\] There are two 2-step routes between Austin and Dallas, and none between Austin and San Antonio. What about three steps? \\[ \\mathbf{A}^3=\\begin{bmatrix} 0 &amp; 2 &amp; 2 &amp; 12\\\\ 2 &amp; 2 &amp; 3 &amp; 7\\\\ 2 &amp; 3 &amp; 2 &amp; 7\\\\ 12 &amp; 7 &amp; 7 &amp; 2 \\end{bmatrix}\\] Why does this work? Consider the \\(ij\\)-th entry of \\(\\mathbf{A}^2\\). \\[\\mathbf{A}^2=\\begin{bmatrix}&amp;&amp;&amp;\\\\&amp;&amp;&amp;\\\\a_{i1} &amp; a_{i2} &amp;\\cdots &amp; a_{in}\\\\ &amp;&amp;&amp;\\end{bmatrix}\\begin{bmatrix}&amp;&amp;a_{1j}&amp;&amp;\\\\&amp;&amp;a_{2j}&amp;&amp;\\\\&amp;&amp;&amp;&amp;\\\\&amp;&amp;a_{nj}&amp;&amp;\\end{bmatrix},\\] so \\[\\mathbf{A}^2_{ij}=a_{i1}a_{1j}+a_{i2}a_{2j}+\\cdots+\\underbrace{a_{ik}a_{kj}}_{\\text{ number of two-step routes through vertex } k}+\\cdots+a_{in}a_{nj}.\\] This sum represents the total number of 2-step routes from vertex \\(i\\) to vertex \\(j\\). Directed adjacency matrices The adjacency matrix for a directed graph is a little different. Definition 3.4 For a directed graph with \\(n\\) vertices, the corresponding directed adjacency matrix is an \\(n\\times n\\) matrix \\(\\mathbf{A}=[a_{ij}]\\), whose \\(ij\\)-th entry is the number of directed edges from vertex \\(i\\) to vertex \\(j\\). Unlike the adjacency matrix for an undirected matrix, the adjacency matrix for a directed matrix is not necessarily symmetric. Here is an example of a directed adjacency matrix. This goes with the graph in Figure 3.8. \\[\\mathbf{A}=\\begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\] As was the case with undirected adjacency matrices, positive integer powers of directed adjacency matrices display the number of multi-step paths between vertices. For our directed graph, \\[\\mathbf{A}^2=\\begin{bmatrix}0 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\\\1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 1 &amp; 0 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}.\\] The directed graph of a tournament For a tournament, there is an edge from Team \\(i\\) to Team \\(j\\) if Team \\(j\\) beats Team \\(i\\). For our toy 5-team example (1.23), the directed adjacency matrix (see the graph in Figure 3.6) looks like this. \\[\\mathbf{A}=\\begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\end{bmatrix}\\] For example, the 1 in row 1, column 2 indicates that Team 1 (the Aardvarks) lost to Team 2 (the Beagles). A teams wins are indicated by the sum of the entries in its column, while its losses are indicated by the sum of the entries in its row. The column sums: Team Aardvarks Beagles Crocs Donkeys Egrets Wins 1 1 4 2 0 And the row sums: Team Aardvarks Beagles Crocs Donkeys Egrets Losses 2 2 0 1 3 We can use the adjacency matrix find the Colley ratings. library(igraph) Teams &lt;- c(&quot;Aardvarks&quot;, &quot;Beagles&quot;, &quot;Crocs&quot;, &quot;Donkeys&quot;, &quot;Egrets&quot;) edges &lt;- c(1,2,1,3,5,1,2,3,2,4,4,3,5,3,5,4) dg &lt;- graph(edges, directed = TRUE) DM&lt;-as.matrix(as_adjacency_matrix(dg)) #Directed adjacency matrix DM ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 1 0 0 ## [2,] 0 0 1 1 0 ## [3,] 0 0 0 0 0 ## [4,] 0 0 1 0 0 ## [5,] 1 0 1 1 0 This is the directed adjacency matrix. We can get the undirected adjacency matrix, which just matches up teams that have played, if we add the directed matrix and its transpose. UM &lt;- DM + t(DM) UM ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 1 0 1 ## [2,] 1 0 1 1 0 ## [3,] 1 1 0 1 1 ## [4,] 0 1 1 0 1 ## [5,] 1 0 1 1 0 If we compare this to the Colley matrix from Example 1.24, we see some similarities and differences. The diagonal entries are missing from this matrix, but note that the rows (and columns) tell us the number of games each team has played. We can add two to these numbers and put them on the diagonal. We can fix the off-diagonal entries just by changing their signs. d &lt;- rowSums(UM) #creates a vector with each team&#39;s total games # This next line does two things: creates a diagonal matrix by # adding 2 to each of the values in d and applying the diag # function. Then it subtracts the undirected matrix from it. This # will make the off-diagonal signs negative (when not 0). CM &lt;- diag(d+2)-UM CM ## [,1] [,2] [,3] [,4] [,5] ## [1,] 5 -1 -1 0 -1 ## [2,] -1 5 -1 -1 0 ## [3,] -1 -1 6 -1 -1 ## [4,] 0 -1 -1 5 -1 ## [5,] -1 0 -1 -1 5 Now we just need to create the \\(\\mathbf{b}\\) vector and solve the system of equations to get our ratings. Wins &lt;- colSums(DM) Losses&lt;- rowSums(DM) b &lt;- 1 + (Wins - Losses)/2 r &lt;- solve(CM, b) #solves CM*r=b to give ratings vector # This is for creating and printing the standings. standings &lt;- data.frame(Team=Teams,Wins,Losses, Rating=r) kable(standings,digits=c(0,0,0,3), booktabs = TRUE, longtable = TRUE) %&gt;% kable_styling(position = &quot;center&quot;) Team Wins Losses Rating Aardvarks 1 2 0.400 Beagles 1 2 0.457 Crocs 4 0 0.786 Donkeys 2 1 0.600 Egrets 0 3 0.257 3.5 Random surfers and Stochastic Matrices Random surfer methods use the behavior of random web surfers to rank websites by importance. Lets consider the following small-scale example with five sites (Fig. 3.9). Figure 3.9: A small network A random surfer on Site 1 has two links to choose, Site 2 or Site 3. Suppose they choose randomly; that is, they choose each site with probability 1/2 (Fig. 3.10). Figure 3.10: A random surfer on the network A random surfer at Site 2 can go to any of the other four sites, each with probability 1/4 (Fig. 3.11). Figure 3.11: The choices from Site 2 We can put these probabilities into the columns of a matrix called a transition matrix. Sites that arent linked to get the probability 0 assigned. \\[\\mathbf{S}=\\begin{bmatrix}0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.5\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0.5\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0\\end{bmatrix} \\] The \\(s_{ij}\\) entry is the probability of going to vertex \\(i\\) from vertex \\(j\\). For example \\(s_{42}=0.25=P(2\\to 4)\\). Important note: The row and columns in this transition matrix have the opposite roles from what they had in a directed adjacency matrix. There is a reason for doing it this way that we will get to soon. Stochastic matrices The matrix \\(\\mathbf{S}\\) is what we call a (column) stochastic matrix: all the entries are between 0 and 1. all the columns add up to 1. Another important note: It is somewhat traditional to have the rows of a stochastic matrix adding up to 1 (i.e., row stochastic). We will soon be using eigenvalues and eigenvectors, and to be consistent with how we use them, we want the columns to add to 1 (column stochastic). Powers of stochastic matrices have a property that is similar to a property of adjacency matrices. \\[\\mathbf{S}^2=\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.5\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ \\mathbf{0} &amp; \\mathbf{0.25} &amp; \\mathbf{0.5} &amp; \\mathbf{0} &amp; \\mathbf{0.5}\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} 0 &amp; \\mathbf{0.25} &amp; 0 &amp; 0 &amp; 0.5\\\\ 0.5 &amp; \\mathbf{0} &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; \\mathbf{0.25} &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\mathbf{0.25} &amp; 0.5 &amp; 0 &amp; 0.5\\\\ 0 &amp; \\mathbf{0.25} &amp; 0.5 &amp; 1 &amp; 0 \\end{bmatrix}\\] The product of the fourth row of \\(\\mathbf{S}\\) and the second column of \\(\\mathbf{S}\\) is \\[0\\cdot 0.25+0.25\\cdot 0+\\underbrace{0.5\\cdot 0.25}_{P(3\\to 4)P(2\\to 3)}+0\\cdot 0.25+0.5\\cdot0.25= 0.25\\] The highlighted product is the probability of going from Site 2 to Site 3 and then from Site 3 to Site 4. The sum is the probability of going from Site 2 to Site 4 in two clicks, the \\(\\mathbf{S}^2_{42}\\) entry. The columns of \\(\\mathbf{S}^2\\) give the probabilities of going from one site to another in exactly two clicks. The columns of \\(\\mathbf{S}^r\\) will give the probabilities of going from one site to another in exactly \\(r\\) clicks. In our example, \\[\\mathbf{S}^2=\\begin{bmatrix}0.125 &amp; 0.125 &amp; 0.25 &amp; 0.5 &amp; 0\\\\ 0 &amp; 0.125 &amp; 0 &amp; 0 &amp; 0.25\\\\ 0.125 &amp; 0.125 &amp; 0 &amp; 0 &amp; 0.25\\\\ 0.375 &amp; 0.25 &amp; 0.25 &amp; 0.5 &amp; 0\\\\ 0.375 &amp; 0.375 &amp; 0.5 &amp; 0 &amp; 0.5\\end{bmatrix}.\\] As an example, the probability of going from Site 2 to Site 5 in two clicks is 0.375. You cant go from Site 3 to Site 2 in two clicks. You can only get to Sites 1, 4, or 5 (Column 3). Note that \\(\\mathbf{S}^2\\) is also column stochastic, as every positive integer power of \\(\\mathbf{S}\\) will be. That is a consequence of the following theorem. Theorem 3.7 Let \\(\\mathbf{S}\\) and \\(\\mathbf{T}\\) be two \\(n\\times n\\) column stochastic matrices. Then the product \\(\\mathbf{ST}\\) is also column stochastic. Proof. First note that, since all the entries in \\(\\mathbf{S}\\) and \\(\\mathbf{T}\\) are nonnegative, the entries in \\(\\mathbf{ST}\\) will also be nonnegative. Let \\(\\mathbf{v}\\) be the \\(n\\)-dimensional row vector \\(\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}\\) and let \\(\\mathbf{a}=\\begin{bmatrix} a_1\\\\a_2\\\\ \\vdots \\\\ a_n\\end{bmatrix}\\) be an \\(n\\)-dimensional column vector. Then \\(\\mathbf{v}\\mathbf{a}=1\\cdot a_1+1\\cdot a_2+\\cdots+1\\cdot a_n=a_1+a_2+\\cdots a_n\\). That is, multiplication on the left by \\(\\mathbf{v}\\) sums the entries in that vector. Now note that the product \\[\\begin{align*} \\mathbf{v}\\mathbf{S}&amp;=\\mathbf{v}\\begin{bmatrix}\\mathbf{s}_1 &amp; \\mathbf{s}_2 &amp; \\cdots &amp; \\mathbf{s}_n\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}\\mathbf{v}\\mathbf{s}_1 &amp; \\mathbf{v}\\mathbf{s}_2 &amp; \\cdots &amp; \\mathbf{v}\\mathbf{s}_n\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}, \\end{align*}\\] since all of the \\(\\mathbf{s}_i\\) have entries that sum to 1. The product \\(\\mathbf{v}(\\mathbf{ST})\\) will be a row vector consisting of the column sums of \\(\\mathbf{ST}\\). And \\[\\mathbf{v}(\\mathbf{ST})=(\\mathbf{v}\\mathbf{S})\\mathbf{T}=\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}\\mathbf{T}=\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix},\\] because the columns of \\(\\mathbf{T}\\) also sum to 1. Since \\(\\mathbf{ST}\\) is a matrix with nonnegative entries whose columns sum to 1, it is column stochastic. Now lets consider what happens when we unleash an army of surfers on the network. Lets start with them evenly spread out among the sites. They can be represented by the vector \\[\\mathbf{v}=\\begin{bmatrix}0.2\\\\0.2\\\\0.2\\\\0.2\\\\0.2\\end{bmatrix}.\\] If the surfers all click at the same time, their distribution after that click is given by \\[\\mathbf{S}\\mathbf{v}=\\begin{bmatrix}0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.5\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0.5\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0\\end{bmatrix}\\begin{bmatrix}0.2\\\\0.2\\\\0.2\\\\0.2\\\\0.2\\end{bmatrix}=\\begin{bmatrix} 0.15 \\\\ 0.10 \\\\ 0.15 \\\\ 0.25 \\\\ 0.35\\end{bmatrix}.\\] After two and three clicks, the distributions are \\[\\mathbf{S}(\\mathbf{S}\\mathbf{v})=\\mathbf{S}^2\\mathbf{v}=\\begin{bmatrix}0.2\\\\0.075\\\\0.1\\\\0.275\\\\0.35\\end{bmatrix} \\text{ and } \\mathbf{S}^3\\mathbf{v}=\\begin{bmatrix}0.19375\\\\0.1\\\\0.11875\\\\0.24375\\\\0.34375\\end{bmatrix}.\\] What happens in the long run? After a few more clicks, the distribution appears to stabilize (Fig. 3.12). Figure 3.12: Distribution of random surfers as a function of the number of steps After a lot of clicks, we have \\[\\mathbf{S}^{57}\\mathbf{v}=(0.19277,0.09638,0.12048,0.25301,0.33735)\\] and \\[\\mathbf{S}^{58}\\mathbf{v}=\\mathbf{S}(\\mathbf{S}^{57}\\mathbf{v})=(0.19277,0.09638,0.12048,0.25301,0.33735)\\] Some notes: We can use these values to rank our websites. Site 5 seems to be the most popular, followed by Sites 4,1,3, and 2. This stable distribution is going to be an eigenvector for \\(\\mathbf{S}\\), with a corresponding eigenvalue of 1! It turns out that a stochastic matrix will always have the value 1 as an eigenvalue. It is actually the dominant (largest in absolute value) eigenvalue. And when we scale the corresponding eigenvector so that the terms add up to 1, we get our stable distribution! (Remember a similar phenomenon with the Leslie matrix?) So, this seems to be a good method for ranking websites based on links in and out. But there are a few problems that can come up. Issue 1: dangling nodes A dangling node on a graph is a vertex that has no edges leaving it. In this graph (Fig. 3.13), vertex 5 is dangling, and the corresponding column in the transition matrix has all 0s. It is substochastic. The surfers have nowhere to go. Figure 3.13: A dangling node Here is the transition matrix for the graph. \\[\\mathbf{S}=\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0\\\\0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0\\end{bmatrix}\\] Adding a loop at vertex 5 would cause all the traffic to get stuck there, leading to a stable distribution of \\((0,0,0,0,1)\\), which is not particularly useful for ratings. Issue 2: Disconnected graphs It is also possible that the graph consists of two or more distinct groups that have no connections between them, as in Figure 3.14. Figure 3.14: A disconnected graph In this case, \\(\\lambda=1\\) will actually be a double eigenvalue, and there wont be a unique stable solution. Issue 3: Cycling A third problem that comes up is the rock/paper/scissors issue. If a graph contains a cycle that cant be exited, as in Figure 3.15, there can be a stable solution, but it might not be achieved in any iteration process. Note: the cycle could be of any length greater than 1. Figure 3.15: a cycle There are some conditions on the stochastic matrix and underlying graph that will lead to a unique stable solution. For that, we need some definitions. Definition 3.5 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) matrix. \\(\\mathbf{A}\\) is nonnegative (\\(\\mathbf{A}\\geq 0\\)) if all entries in \\(\\mathbf{A}\\) are nonnegative. \\(\\mathbf{A}\\) is positive (\\(\\mathbf{A}&gt;0\\)) if all entries in \\(\\mathbf{A}\\) are positive. \\(\\mathbf{A}\\) is regular if \\(\\mathbf{A}\\geq 0\\) and there exists a positive integer \\(k\\) such that \\(\\mathbf{A}^k\\) is a positive matrix. Note that stochastic matrices are nonnegative. For a graph and its transition matrix, regular means that for some \\(k\\) it is possible to get from every vertex to every other vertex in exactly \\(k\\) steps. The Perron-Frobenius Theorem is key for graph-based ranking methods[11]. Theorem 3.8 (The Perron-Frobenius Theorem for Stochastic Matrices) Let \\(\\mathbf{A}\\) be a regular stochastic matrix. Then The matrix \\(\\mathbf{A}\\) has \\(\\lambda_1=1\\) as an eigenvalue of multiplicty \\(1.\\) The eigenvector \\(\\mathbf{v}_1\\) corresponding to \\(\\lambda_1\\) can be chosen so that it has all positive entries that sum to \\(1\\) (a positive probability vector). All other eigenvalues \\(\\lambda_j\\) have \\(|\\lambda_j|&lt;1\\). The corresponding eigenvectors have entries that sum to \\(0.\\) If \\(\\mathbf{p}\\) is any probability vector, then \\(\\mathbf{A}^n\\mathbf{p}\\to\\mathbf{v}_1\\) as \\(n\\to\\infty\\). In particular, the columns of \\(\\mathbf{A}^n\\) are all approaching \\(\\mathbf{v}_1\\). The stochastic version of the Perron-Frobenius Theorem is a special case of a more general theorem [12]. Markov Chains The graph/transition matrix pair is an example of a Markov chain. A Markov chain is a collection of states (for us, the vertices in a graph) with associated probabilities of moving from one state to another. The probabilities for moving from one state to another only depend on the current state and not on any past state or sequence of states. In addition to the application weve already seen, Markov chains can be used to simulate things such as baseball games or to create simple weather models. Here is an example of one of them. Suppose there are three kinds of days: sunny, cloudy, and rainy. The transition matrix from one type of day to another might look like this. Example 3.9 Today Sunny Cloudy Rainy Sunny 0.7 0.5 0.4 Tomorrow Cloudy 0.2 0.3 0.5 Rainy 0.1 0.2 0.1 If we square the transition matrix, we get the probabilities for two days out. \\[ \\mathbf{S}^2=\\begin{bmatrix} 0.63 &amp; 0.58 &amp; 0.57 \\\\ 0.25 &amp; 0.29 &amp; 0.28\\\\ 0.12 &amp; 0.13 &amp; 0.15 \\end{bmatrix} \\] If its sunny today, theres a 63% chance that it will be sunny in two days. If its cloudy today, theres a 13% chance that it will be rainy in two days. What will happen in the long run? Since this weather forecaster is a positive stochastic matrix, when we raise it to higher and higher powers, all three columns will approach the stable distribution. \\[ \\textbf{S}^{100}=\\begin{bmatrix} 0.6091954 &amp; 0.6091954 &amp; 0.6091954 \\\\ 0.2643678 &amp; 0.2643678 &amp; 0.2643678\\\\ 0.1264368 &amp; 0.1264368 &amp; 0.1264368 \\end{bmatrix} \\] If you think about it, what this is saying is that the weather today wont tell us much about the weather 100 days from today (ignoring seasons and all that). About 61% of the days will be sunny, 26% will be cloudy, and 13% will be rainy. Why is 1 always an eigenvalue? We close this section by taking care of some unfinished business. The Perron-Frobenius Theorem guarantees us that a stochastic matrix will have an eigenvalue of 1, but there is an easier way to see that. Weve already observed that \\[\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}\\mathbf{S}=\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}\\] for a stochastic matrix \\(\\mathbf{S}\\) and the appropriately sized row vector of ones. That means that \\[(\\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1\\end{bmatrix}\\mathbf{S})^T=\\begin{bmatrix}1\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix}\\] or \\[\\mathbf{S}^T\\begin{bmatrix}1\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix}=\\begin{bmatrix}1\\\\1\\\\ \\vdots \\\\ 1\\end{bmatrix}.\\] In other words, the column vector of ones is an eigenvector for \\(\\mathbf{S}^T\\) with corresponding eigenvalue 1. Since matrices and their transposes share the same eigenvalues, 1 is also an eigenvalue for \\(S\\). 3.6 The Markov and Oracle Ranking Methods There are several Markov chain-based ranking methods. We will see two in this section. The Markov method, which is modeled on the PageRank algorithm [13], was developed by Amy N. Langville and Carl D. Meyer [11]. The Oracle method was developed by E. Cabral Balreira, Brian Miceli, and T. Tegtmeyer [14]. The Markov method The key step in any Markov chain-based method is to create a regular matrix, so that we can take advantage of the Perron-Frobenious theorem. Recall the graph we saw in Figure 3.13. The corresponding transition matrix is here \\[\\mathbf{T}=\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0\\\\0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0\\end{bmatrix}.\\] The first step is to replace the column of all zeros with a column whose entries are all \\(1/n\\), where \\(n\\) is the number of vertices or teams. \\[\\mathbf{T}=\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0\\\\0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0\\end{bmatrix}\\to \\mathbf{S}=\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0.2\\\\0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0.2\\end{bmatrix}\\] This is like replacing a site with no outgoing links with one that links to all of the other sites, including itself. We still need to do something to ensure regularity of the transition matrix. The Langville-Meyer/PageRank solution is brutally simple. It introduces a teleportation matrix. A certain fraction of the time, say \\(\\alpha\\), the random surfers will follow the original network. The remainder of the time, the surfers will magically teleport to any other site in the network, with equal probability. The \\(n\\times n\\) teleportation matrix looks like this: \\[\\mathbf{E}=\\begin{bmatrix} 1/n &amp; 1/n &amp; \\cdots &amp; 1/n\\\\ 1/n &amp; 1/n &amp; \\cdots &amp; 1/n\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1/n &amp; 1/n &amp; \\cdots &amp; 1/n \\end{bmatrix}.\\] When we combine the transition matrix and the teleportation matrix, we get a new stochastic matrix \\[\\textbf{G}=\\alpha \\textbf{S}\\textbf{}+(1-\\alpha)\\textbf{E}\\] that is regular (indeed, positive) by design. The probability eigenvector corresponding to the eigenvalue 1 will be our ratings vector. We can experiment with the value of \\(\\alpha\\), but \\(\\alpha = 0.85\\) is a common value. For our dangling node example, \\[\\begin{align*} \\mathbf{G}&amp;=0.85\\begin{bmatrix} 0 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0.2\\\\ 0 &amp; 0.25 &amp; 0.5 &amp; 0 &amp; 0.2\\\\0 &amp; 0.25 &amp; 0.5 &amp; 1 &amp; 0.2\\end{bmatrix}+0.15\\begin{bmatrix} 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2\\\\ 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2\\\\ 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2\\\\ 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2\\\\ 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 &amp; 0.2 \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} 0.03 &amp; 0.2425 &amp; 0.03 &amp; 0.03 &amp; 0.2\\\\ 0.455 &amp; 0.03 &amp; 0.03 &amp; 0.03 &amp; 0.2\\\\ 0.455 &amp; 0.2425 &amp; 0.03 &amp; 0.03 &amp; 0.2\\\\ 0.03 &amp; 0.2425 &amp; 0.455 &amp; 0.03 &amp; 0.2\\\\ 0.03 &amp; 0.2425 &amp; 0.455 &amp; 0.88 &amp; 0.2 \\end{bmatrix}. \\end{align*}\\] The ratings eigenvector is \\[\\mathbf{v}=(0.1223049, 0.1437397, 0.1742844, 0.1963758, 0.3632952).\\] Example 3.10 Here is one way to implement the Markov method in R, demonstrated on our toy example. library(igraph) library(expm) teams &lt;- c(&quot;Aardvarks&quot;, &quot;Beagles&quot;, &quot;Crocs&quot;, &quot;Donkeys&quot;, &quot;Egrets&quot;) results &lt;-c (1,2,1,3,5,1,2,3,2,4,4,3,5,3,5,4) gtoy &lt;- graph(results) coords &lt;- layout_in_circle(gtoy) plot(gtoy, vertex.label = teams, vertex.size = 50, vertex.color = &quot;lightblue&quot;, layout = coords) # We take the transpose of the adjacency matrix T &lt;- t(as.matrix(as_adjacency_matrix(gtoy))) T ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 1 ## [2,] 1 0 0 0 0 ## [3,] 1 1 0 1 1 ## [4,] 0 1 0 0 1 ## [5,] 0 0 0 0 0 s &lt;- nrow(T) # Next, we identify the dangling nodes, replace their # columns with ones, and then make the transition # matrix by scaling the column sums. D &lt;- colSums(T) dangling &lt;- which(D == 0) T[, dangling] &lt;- 1 T ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 1 0 1 ## [2,] 1 0 1 0 0 ## [3,] 1 1 1 1 1 ## [4,] 0 1 1 0 1 ## [5,] 0 0 1 0 0 scale &lt;- diag(1/colSums(T)) S &lt;- T%*%scale #multiplication on right acts on the columns S ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.0 0.0 0.2 0 0.3333333 ## [2,] 0.5 0.0 0.2 0 0.0000000 ## [3,] 0.5 0.5 0.2 1 0.3333333 ## [4,] 0.0 0.5 0.2 0 0.3333333 ## [5,] 0.0 0.0 0.2 0 0.0000000 # Now we add the teleportation matrix to get the G matrix G &lt;- 0.85*S + 0.15*matrix(1/s, nrow = s, ncol = s) G ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.030 0.030 0.2 0.03 0.3133333 ## [2,] 0.455 0.030 0.2 0.03 0.0300000 ## [3,] 0.455 0.455 0.2 0.88 0.3133333 ## [4,] 0.030 0.455 0.2 0.03 0.3133333 ## [5,] 0.030 0.030 0.2 0.03 0.0300000 # Now we raise G to a high power and extract our ratings # vector (G%^%50)[, 1] ## [1] 0.1295831 0.1560467 0.4174933 0.1959030 0.1009739 Here are the ratings. They are presented with the Colley ratings for comparison. Recall that the Colley ratings are not designed to add up to 1. The rankings are the same. Team W-L PCT Colley Rating Markov Rating Crocs 4-0 1.000 0.786 0.4175 Donkeys 2-1 0.667 0.600 0.1959 Beagles 1-2 0.333 0.457 0.1560 Aardvarks 1-2 0.333 0.400 0.1296 Egrets 0-3 0.000 0.257 0.1010 The Oracle method The Oracle method takes a different approach to induce regularity in the transition matrix. Figure 3.16: The Oracle By adjusting the number of edges up to and down from the Oracle, we can fine tune the ratings. The simplest case just has 1 edge in both directions. The original transposed adjacency matrix and Oracle adjacency matrix will look like this. \\[\\mathbf{A}=\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix},\\mathbf{A}_{Oracle}=\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\mathbf{1} \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\mathbf{1} \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; \\mathbf{1} \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; \\mathbf{1} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\mathbf{1} \\\\ \\mathbf{1} &amp; \\mathbf{1} &amp; \\mathbf{1} &amp; \\mathbf{1} &amp; \\mathbf{1} &amp; \\mathbf{0}\\end{bmatrix}\\] When we divide by the column sums, we get the transition matrix with the Oracle: \\[\\mathbf{S=}\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/4 &amp; 1/5 \\\\ 1/3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/5\\\\ 1/3 &amp; 1/3 &amp; 0 &amp; 1/2 &amp; 1/4 &amp; 1/5 \\\\ 0 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/4 &amp; 1/5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/5 \\\\ 1/3 &amp; 1/3 &amp; 1 &amp; 1/2 &amp; 1/4 &amp; 0\\end{bmatrix}\\] When we exponentiate \\(\\mathbf{S}\\), all of the columns look like this, where the last entry belongs to the Oracle. \\[(0.0933610, 0.1058091, 0.2240664, 0.1286307, 0.0746888, 0.3734440)\\] If we remove the Oracle and rescale the remaining ratings to add to 1 (not necessary, but otherwise the numbers can be quite small), we get the following ranking: Team W L Rating Rank Crocs 4 0 0.358 1 Donkeys 2 1 0.205 2 Beagles 1 2 0.169 3 Aardvarks 1 2 0.149 4 Egrets 0 3 0.119 5 Modifications The nice thing about the Oracle method is that the number of edges up to and down from the Oracle can be adjusted. Suppose the Oracle likes winners. The number of edges down to a team can be based on the number of wins they have. Because we need at least one edge down to every team, we add 1 to the win total. Here are the respective transposed adjacency matrix and transition matrix. \\[\\textbf{A}_{Oracle}=\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1 &amp; 5 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0\\end{bmatrix},\\textbf{S}=\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/4 &amp; 2/13 \\\\ 1/3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 2/13\\\\ 1/3 &amp; 1/3 &amp; 0 &amp; 1/2 &amp; 1/4 &amp; 5/13 \\\\ 0 &amp; 1/3 &amp; 0 &amp; 0 &amp; 1/4 &amp; 3/13 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1/13 \\\\ 1/3 &amp; 1/3 &amp; 1 &amp; 1/2 &amp; 1/4 &amp; 0\\end{bmatrix}\\] Here are the resulting ratings. Note that the Crocs rating has increased from 0.358 to 0.470, while the winless Egrets rating has gone down significantly. Team W L Rating Rank Crocs 4 0 0.470 1 Donkeys 2 1 0.217 2 Beagles 1 2 0.143 3 Aardvarks 1 2 0.117 4 Egrets 0 3 0.052 5 3.7 Exercises Let \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 2\\\\5 &amp; 4\\end{bmatrix}.\\] Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\) by hand. Let \\[\\mathbf{A}=\\begin{bmatrix} 3 &amp; 2\\\\5 &amp; 0\\end{bmatrix}.\\] Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\) by hand. Let \\[\\mathbf{A}=\\begin{bmatrix} 2 &amp; -2\\\\2 &amp; 6\\end{bmatrix}.\\] Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\) by hand. Let \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 4\\\\-1 &amp; 5\\end{bmatrix}.\\] Find the eigenvalues and eigenvectors of \\(\\mathbf{A}\\) by hand. The matrix \\[\\mathbf{A}=\\begin{bmatrix} -2 &amp; -3 &amp; -1\\\\1 &amp; 2 &amp; 1\\\\3 &amp; 3 &amp; 2\\end{bmatrix}\\] has eigenvalues \\(\\lambda_1=2, \\lambda_2=1,\\) and \\(\\lambda_3=-1.\\) Find the corresponding eigenvectors by hand. The matrix \\[\\mathbf{A}=\\begin{bmatrix} 2 &amp; -1 &amp; 1\\\\-1 &amp; 2 &amp; -1\\\\1 &amp; 1 &amp; 2\\end{bmatrix}\\] has eigenvalues \\(\\lambda_1=3, \\lambda_2=2,\\) and \\(\\lambda_3=-1.\\) Find the corresponding eigenvectors by hand. Find a nondiagonal, nontriangular \\(2\\times 2\\) matrix that has eigenvalues \\(\\lambda_1=5\\) and \\(\\lambda_2=-2.\\) Find a nondiagonal, nontriangular \\(2\\times 2\\) matrix that has eigenvalues \\(\\lambda_1=4\\) and \\(\\lambda_2=3.\\) For the matrix in Problem 1, show that the nonzero columns of \\(\\mathbf{A}-\\lambda_1\\mathbf{I}\\) are eigenvectors corresponding to \\(\\lambda_2\\) and that the nonzero columns of \\(\\mathbf{A}-\\lambda_2\\mathbf{I}\\) are eigenvectors corresponding to \\(\\lambda_1.\\) (This is a general result for \\(2\\times 2\\) matrices with non-repeating real eigenvalues.) For the matrix in Problem 2, show that the nonzero columns of \\(\\mathbf{A}-\\lambda_1\\mathbf{I}\\) are eigenvectors corresponding to \\(\\lambda_2\\) and that the nonzero columns of \\(\\mathbf{A}-\\lambda_2\\mathbf{I}\\) are eigenvectors corresponding to \\(\\lambda_1.\\) The matrix \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 3\\\\3&amp;2\\end{bmatrix}\\) found in Example 3.4 has the eigenvector \\(\\mathbf{x}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) corresponding to the eigenvalue \\(\\lambda_1=5.\\) Find \\(\\mathbf{A}^{3}\\mathbf{x}_1.\\) The matrix \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 3\\\\3&amp;2\\end{bmatrix}\\) found in Example 3.4 has the eigenvector \\(\\mathbf{x}_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\) corresponding to the eigenvalue \\(\\lambda_2=-1.\\) Find \\(\\mathbf{A}^{101}\\mathbf{x}_2.\\) Let \\(\\mathbf{L}=\\begin{bmatrix} 2.9 &amp; 3\\\\0.1 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}10 \\\\0\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1)\\) and \\(\\mathbf{N}(2).\\) Do not round. Find the eigenvalues of \\(\\mathbf{L}.\\) What is the long-run growth rate? What is the long-run ratio of age-zero to age-one individuals? Let \\(\\mathbf{L}=\\begin{bmatrix} 4.9 &amp; 10\\\\0.05 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}100 \\\\0\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1)\\) and \\(\\mathbf{N}(2).\\) Do not round. Find the eigenvalues of \\(\\mathbf{L}.\\) What is the long-run growth rate? What is the long-run ratio of age-zero to age-one individuals? Let \\(\\mathbf{L}=\\begin{bmatrix} 2 &amp; 4 &amp; 1\\\\0.05 &amp; 0 &amp; 0\\\\0 &amp; 0.4 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}10 \\\\10\\\\10\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1)\\) and \\(\\mathbf{N}(2).\\) Do not round. Find the eigenvalues of \\(\\mathbf{L}.\\) What is the long-run growth rate? In the long run, what will the percentages for each age group be? Let \\(\\mathbf{L}=\\begin{bmatrix} 5 &amp; 6 &amp; 2\\\\0.01 &amp; 0 &amp; 0\\\\0 &amp; 0.2 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}10 \\\\10\\\\10\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1)\\) and \\(\\mathbf{N}(2).\\) Do not round. Find the eigenvalues of \\(\\mathbf{L}.\\) What is the long-run growth rate? In the long run, what will the percentages for each age group be? Let \\(\\mathbf{L}=\\begin{bmatrix} 0 &amp; 4\\\\ 0.5 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}2\\\\0\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1), \\mathbf{N}(2), \\mathbf{N}(3),\\) and \\(\\mathbf{N}(4).\\) Will there be a stable population distribution? Let \\(\\mathbf{L}=\\begin{bmatrix} 0 &amp; 8\\\\ 0.25 &amp; 0\\end{bmatrix}\\) be a Leslie matrix. Suppose an initial population distribution of \\(\\mathbf{N}(0)=\\begin{bmatrix}0\\\\2\\end{bmatrix}.\\) Find \\(\\mathbf{N}(1), \\mathbf{N}(2), \\mathbf{N}(3),\\) and \\(\\mathbf{N}(4).\\) Will there be a stable population distribution? Sketch an undirected graph with adjacency matrix \\[\\mathbf{A}=\\begin{bmatrix} 1 &amp; 1 &amp; 2 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 1\\\\2 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\end{bmatrix}.\\] Sketch an undirected graph with adjacency matrix \\[\\mathbf{A}=\\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 1\\\\0 &amp; 1 &amp; 2 &amp; 0\\\\1 &amp; 2 &amp; 0 &amp; 2\\\\1 &amp; 0 &amp; 2 &amp; 0\\end{bmatrix}.\\] For the graph in Problem 19, how many ways are there to get from vertex 1 to vertex 2 in exactly four steps? For the graph in Problem 20, how many ways are there to get from vertex 1 to vertex 3 in exactly five steps? Find the adjacency matrix for the directed graph in Figure 3.17. Figure 3.17: Graph for Problem 23 Find the adjacency matrix for the directed graph in Figure 3.18. Figure 3.18: Graph for Problem 24 Find the column stochastic transition matrix for the graph in Figure 3.19. Figure 3.19: Graph for Problem 25 Find the column stochastic transition matrix for the graph in Figure 3.20. Figure 3.20: Graph for Problem 26 A switch has two positions: on and off. Every second, the position might change. If it is in the on position, it changes to off with probability 2/3, and stays on with probability 1/3. If it is in the off position, it switches to on with probability 1/2 and stays off with probability 1/2. Write out the column stochastic transition matrix for this switch. If the switch is on now, what is the probability that it is on two seconds from now? (That is, after two switching opportunities.) What percentage of the time is the switch in the on position? Professor X holds random office hours. If he holds office hours one day, the probability he holds them the next day is 1/10. If he does not hold them one day, the probability he holds them the next day is 1/2. Write out the column stochastic transition matrix for Professor Xs office hours. If Professor X holds office hours today, what is the probability he holds office hours two days from now? In the long run, what fraction of days does Professor X hold office hours? In the Chapter 1 Exercises, we found the Colley ratings based on some football results. We are now going to apply the Markov method to those same results. Based on the results below (Problem 39 from Chapter 1), perform the following. Write out the column stochastic transition matrix \\(\\mathbf{T}.\\) The Argos didnt lose. Adjust for this dangling node to get a new matrix \\(\\mathbf{S}.\\) Find \\(\\mathbf{G}=0.85\\mathbf{S}+0.15\\mathbf{E},\\) where \\(\\mathbf{E}\\) is the teleportation matrix. Find the ratings by finding the probability vector guaranteed by the Perron-Frobenius Theorem. Winner Score Loser Score Argos 20 Brahmas 13 Cobbers 31 Brahmas 10 Argos 24 Cobbers 21 Dukes 10 Brahmas 7 Brahmas 35 Dukes 3 Lets find the Markov ratings for the results in Problem 40 from Chapter 1. Perform the following. Write out the column stochastic transition matrix \\(\\mathbf{T}.\\) Adjust for any dangling nodes to get the matrix \\(\\mathbf{S}.\\) (If there are no dangling nodes, then \\(\\mathbf{S}=\\mathbf{T}\\).) Find \\(\\mathbf{G}=0.85\\mathbf{S}+0.15\\mathbf{E},\\) where \\(\\mathbf{E}\\) is the teleportation matrix. Find the ratings by finding the probability vector guaranteed by the Perron-Frobenius Theorem. Winner Score Loser Score Argos 28 Cobbers 14 Brahmas 21 Argos 13 Brahmas 17 Dukes 10 Dukes 31 Brahmas 28 Now, lets apply the Oracle method to the results in Problem 29. Find the transposed adjacency matrix \\(\\mathbf{A}.\\) Add a column for the Oracle by summing each row and adding 1. This will represent the number of wins for each team, increased by 1. Add a row for the Oracle at the bottom. This row will consist of four ones and a zero at the end. Make this new matrix column stochastic by dividing each column by its sum. Find the Oracle ratings by finding the probability eigenvector. Find the Oracle ratings for the results in Problem 30. Use the instructions in Problem 31. References 4. Neuhauser C (2004) Calculus for biology and medicine, 2nd ed., Pearson/Prentice Hall. 10. Leslie PH (1945) The use of matrices in certain population mathematics. Biometrika 33: 183212. 11. Langville AN, Meyer CD (2012) Whos #1? The science of rating and ranking, Princeton University Press. 12. Meyer CD (2023) Matrix analysis and applied linear algebra, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA. 13. Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems 33: 107117. 14. Balreira EC, Miceli BK, Tegtmeyer T (2014) An oracle method to predict NFL games. Journal of Quantitative Analysis in Sports 10: 183196. "],["orthogonality.html", "Chapter 4 Orthogonality 4.1 Inner Product, Length, and Orthogonality 4.2 Orthogonal Subspaces 4.3 Orthogonal and Orthonormal Sets 4.4 Orthogonal Projections 4.5 Orthogonalization 4.6 Exercises", " Chapter 4 Orthogonality Several of our future topics will rely heavily on a property called orthogonality. These topics include least squares, the singular value decomposition, and principal component analysis. In this chapter, we will lay the groundwork for those discussions. 4.1 Inner Product, Length, and Orthogonality Now we want to define the inner product of two vectors. Definition 4.1 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be vectors in \\(\\mathbb{R}^n\\), with \\[\\mathbf{u}=\\begin{bmatrix}u_1\\\\u_2\\\\\\vdots\\\\u_n\\end{bmatrix},\\mathbf{v}=\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix}.\\] Then the inner or dot product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), denoted \\(\\mathbf{u}\\cdot\\mathbf{v}\\) is \\[\\mathbf{u}\\cdot\\mathbf{v}=\\mathbf{u}^T\\mathbf{v}=\\begin{bmatrix}u_1 &amp; u_2 &amp; \\cdots &amp; u_n\\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{bmatrix}=u_1v_1+u_2v_2+\\cdots+u_nv_n.\\] Weve already mentioned the dot product when we defined matrix multiplication, but now we are going to see a few more of its properties. The ability to write it as a matrix product involving the transpose of one of the vectors is particularly useful. Example 4.1 With \\[\\mathbf{u}=\\begin{bmatrix}1\\\\-2\\\\3\\end{bmatrix},\\mathbf{v}=\\begin{bmatrix}4\\\\7\\\\5\\end{bmatrix},\\] we have \\[\\mathbf{u}\\cdot\\mathbf{v}=(1)(4)+(-2)(7)+(3)(5)=5.\\] Note that by commutativity of multiplication, \\(\\mathbf{u}\\cdot \\mathbf{v}=\\mathbf{v}\\cdot\\mathbf{u}\\). Here is a larger list of the properties. Theorem 4.1 Let \\(\\mathbf{u}, \\mathbf{v},\\) and \\(\\mathbf{w}\\) be vectors in \\(\\mathbb{R}^n\\), and let \\(c\\) be a scalar. Then \\(\\mathbf{u}\\cdot \\mathbf{v}=\\mathbf{v}\\cdot \\mathbf{u}\\) \\((\\mathbf{u}+\\mathbf{v})\\cdot \\mathbf{w}=\\mathbf{u}\\cdot \\mathbf{w}+\\mathbf{v}\\cdot \\mathbf{w}\\) \\((c\\mathbf{u})\\cdot \\mathbf{v}=c(\\mathbf{u}\\cdot \\mathbf{v})=\\mathbf{u}\\cdot(c\\mathbf{v})\\) \\(\\mathbf{u}\\cdot \\mathbf{u}\\geq 0\\), and \\(\\mathbf{u}\\cdot\\mathbf{u}=0\\) if and only if \\(\\mathbf{u}=\\mathbf{0}.\\) The last property holds because if \\(\\mathbf{u}=(u_1,u_2,\\dots,u_n)\\), then \\[\\mathbf{u}\\cdot \\mathbf{u}=u_1u_1+u_2u_2+\\cdots+u_nu_n=u_1^2+u_2^2+\\cdots+u_n^2,\\] which is never negative, and can only be zero if all the \\(u_i\\) are zero. You should recognize \\(\\mathbf{u}\\cdot \\mathbf{u}\\) as the magnitude of \\(\\mathbf{u}\\) squared. Theorem 4.2 Let \\(\\mathbf{v}\\) be a vector in \\(\\mathbb{R}^n\\). The magnitude or norm of \\(\\mathbf{v}\\) is \\[|\\mathbf{v}|=\\sqrt{\\mathbf{v}\\cdot \\mathbf{v}}=\\sqrt{v_1^2+v_2^2+\\cdots+v_n^2},\\] and \\[\\mathbf{v}\\cdot \\mathbf{v}=\\mathbf{v}^T\\mathbf{v}=|\\mathbf{v}|^2.\\] If we multiply a vector by a constant \\(c\\), how does its length change? \\[|c\\mathbf{v}|=\\sqrt{c^2v_1^2+c^2v_2^2+\\cdots+c^2v_n^2}=\\sqrt{c^2(v_1^2+v_2^2+\\cdots +v_n^2)}=|c||\\mathbf{v}|.\\] A unit vector is a vector whose length is 1. They are handy things to have. If \\(\\mathbf{v}\\) is a nonzero vector, then \\[\\mathbf{u}=\\frac{1}{|\\mathbf{v}|}\\mathbf{v}\\] is a unit vector in the direction of \\(\\mathbf{v}\\). Example 4.2 Let \\(\\mathbf{v}=(2,-1,3).\\) Find a unit vector in the direction of \\(\\mathbf{v}.\\) With \\(|\\mathbf{v}|=\\sqrt{2^2+(-1)^2+3^2}=\\sqrt{4+1+9}=\\sqrt{14}\\), we have \\[\\mathbf{u}=\\frac{1}{\\sqrt{14}}(2,-1,3)=\\left(\\frac{2}{\\sqrt{14}},-\\frac{1}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right).\\] Lets check: \\[|\\mathbf{u}|^2=\\frac{4}{14}+\\frac{1}{14}+\\frac{9}{14}=1,\\] so \\(|\\mathbf{u}|=1\\). Distance in \\(\\mathbb{R}^n\\) Let \\(\\mathbf{u}=(u_1,u_2,\\dots,u_n)\\) and \\(\\mathbf{v}=(v_1,v_2,\\dots,v_n)\\) be vectors in \\(\\mathbb{R}^n\\). Then the distance between (the tips of) \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is given by the distance formula: \\[\\begin{align*} \\text{dist}(\\mathbf{u},\\mathbf{v})&amp;=\\sqrt{(u_1-v_1)^2+(u_2-v_2)^2+\\cdots+(u_n-v_n)^2}\\\\ &amp;=\\sqrt{(\\mathbf{u}-\\mathbf{v})\\cdot (\\mathbf{u}-\\mathbf{v})}\\\\ &amp;=|\\mathbf{u}-\\mathbf{v}|. \\end{align*}\\] The inner product between two vectors tells us something about their relationship. The following theorem gives us that relationship. Theorem 4.3 Let \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) be two vectors in \\(\\mathbb{R}^n\\). Then \\[\\begin{equation} \\mathbf{u}\\cdot\\mathbf{v}=|\\mathbf{u}||\\mathbf{v}|\\cos\\theta, \\tag{4.1} \\end{equation}\\] where \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}.\\) Equation (4.1) is often called the Law of Cosines, and you can derive it from that trigonometric equation. Note that if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular, then \\(\\theta=90^\\circ, \\cos\\theta=0,\\) and \\(\\mathbf{u}\\cdot \\mathbf{v}=0.\\) Similarly, if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are nonzero vectors and \\(\\theta\\neq 90^\\circ,\\) then \\(\\mathbf{u}\\cdot \\mathbf{v}\\neq 0\\). Definition 4.2 Two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal if \\(\\mathbf{u}\\cdot\\mathbf{v}=0.\\) Here are some examples. Let \\(\\mathbf{u}=(1,3,2)\\) and \\(\\mathbf{v}=(2,4,1)\\). Then \\[\\mathbf{u}\\cdot \\mathbf{v}=2+12+2=16,\\] so \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are not orthogonal. Let \\(\\mathbf{u}=(1,3,2)\\) and \\(\\mathbf{v}=(2,4,-7)\\). Then \\[\\mathbf{u}\\cdot \\mathbf{v}=2+12-14=0\\] and \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal. Note that \\(\\mathbf{v}\\cdot\\mathbf{0}=0\\) for any vector \\(\\mathbf{v}\\), so the zero vector is orthogonal to every vector. 4.2 Orthogonal Subspaces Remember subspaces? Important examples included the column, row, and null space of a matrix \\(\\mathbf{A}\\). We can consider vectors that are orthogonal to these subspaces. Definition 4.3 Let \\(W\\) be a subspace of \\(\\mathbb{R}^n\\) and let \\(\\mathbf{v}\\) be a vector in \\(\\mathbb{R}^n\\). We say that \\(\\mathbf{v}\\) is orthogonal to \\(W\\) if \\(\\mathbf{v}\\) is orthogonal to every vector in \\(W\\). The orthogonal complement of \\(W\\) is the collection of all vectors orthogonal to \\(W\\), and is denoted \\(W^{\\perp}\\) (read as \\(W\\)-perp). The following theorems are useful. Theorem 4.4 Let \\(W\\) be a subspace of \\(\\mathbb{R}^n\\). A vector \\(\\mathbf{x}\\) is in \\(W^\\perp\\) if and only if \\(\\mathbf{x}\\) is orthogonal to every vector in a set that spans \\(W\\). This means we can check it against a basis or any convenient spanning set. The next one tells us the structure of \\(W^{\\perp}.\\) Theorem 4.5 Let \\(W\\) be a subspace of \\(\\mathbb{R}^n\\). Then \\(W^\\perp\\) is a subspace of \\(\\mathbb{R}^n\\). The row space and nullspace of a matrix We can actually write \\(\\mathbb{R}^n\\) in terms of some special orthogonal subspaces. Recall that the row space of an \\(m\\times n\\) matrix \\(\\mathbf{A}\\) is the span of its row vectors, while the nullspace of \\(\\mathbf{A}\\) is the set of all vectors \\(\\mathbf{v}\\) such that \\(\\mathbf{A}\\mathbf{v}=\\mathbf{0}\\). Suppose \\(\\mathbf{v}=(v_1,v_2,\\dots,v_n)\\) is in \\(\\text{Null}(\\mathbf{A})\\). Then, focusing on the second row of \\(\\mathbf{A}\\) as an example, \\[\\mathbf{A}\\mathbf{v}=\\begin{bmatrix} &amp; &amp; &amp; \\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n}\\\\ &amp;&amp;&amp;\\\\ &amp; &amp; &amp; \\end{bmatrix}\\begin{bmatrix}v_1\\\\v_2\\\\\\vdots \\\\v_n\\end{bmatrix}=\\begin{bmatrix}0\\\\\\mathbf{0}\\\\ \\vdots \\\\ 0\\end{bmatrix}.\\] The \\(i\\)th entry in the product is the product of the \\(i\\)th row of \\(\\mathbf{A}\\) with \\(\\mathbf{v}\\). This means that \\(\\mathbf{v}\\) is orthogonal to each row vector of \\(\\mathbf{A}\\). Since the rows of \\(\\mathbf{A}\\) span the row space of \\(\\mathbf{A}\\), every vector in \\(\\text{Null}(\\mathbf{A})\\) is in \\(\\text{Row}(\\mathbf{A})^{\\perp}\\). We get the following theorem. Theorem 4.6 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. Then \\[(\\textrm{Row}(\\mathbf{A}))^\\perp=\\textrm{Null}(\\mathbf{A})\\] and \\[(\\textrm{Col}(\\mathbf{A}))^{\\perp}=\\textrm{Null}(\\mathbf{A}^T).\\] The second statement is true because the columns of \\(\\mathbf{A}\\) become the rows of \\(\\mathbf{A}^T\\). These theorems seem a little abstract now, but they will come in handy in the future. 4.3 Orthogonal and Orthonormal Sets In Section 4.1, we defined what it meant for two vectors to be orthogonal. In this section, were going to talk about sets of mutually orthogonal vectors. First, a definition. Definition 4.4 A set of vectors \\(\\{\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_p\\}\\) in \\(\\mathbb{R}^n\\) is said to be an orthogonal set if each pair of distinct vectors from the set is orthogonal, that is, if \\(\\mathbf{u}_i\\cdot\\mathbf{u}_j=0\\) whenever \\(i\\neq j\\). Example 4.3 If \\[\\mathbf{u}_1=\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\mathbf{u}_2=\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix},\\text{ and }\\mathbf{u}_3=\\begin{bmatrix}-1\\\\-4\\\\7\\end{bmatrix},\\] we have \\[\\mathbf{u}_1\\cdot \\mathbf{u}_2=(3)(-1)+(1)(2)+(1)(1)=0,\\] \\[\\mathbf{u}_1\\cdot\\mathbf{u}_3=(3)(-1)+(1)(-4)+(1)(7)=0,\\] and \\[\\mathbf{u}_2\\cdot \\mathbf{u}_3=(-1)(-1)+(2)(-4)+(1)(7)=0.\\] It shouldnt be surprising that orthogonal sets are linearly independent. Heres a theorem. Theorem 4.7 If \\(S=\\{\\mathbf{u}_1,\\mathbf{u}_2,\\cdots,\\mathbf{u}_p\\}\\) is an orthogonal set of nonzero vectors in \\(\\mathbb{R}^n\\), then \\(S\\) is linearly independent and hence is a basis for the subspace spanned by \\(S\\). Proof. We want to show that any linear combination that yields the zero vector is trivial. Suppose that \\[\\mathbf{0}=c_1\\mathbf{u}_1+c_2\\mathbf{u}_2+\\cdots+c_p\\mathbf{u}_p.\\] Then \\[\\begin{align*} 0=\\mathbf{u}_1\\cdot \\mathbf{0}&amp;=\\mathbf{u}_1\\cdot(c_1\\mathbf{u}_1+c_2\\mathbf{u}_2+\\cdots+c_p\\mathbf{u}_p)\\\\ &amp;=c_1\\mathbf{u}_1\\cdot\\mathbf{u}_1+c_2\\mathbf{u}_1\\cdot\\mathbf{u}_2+\\cdots+c_p\\mathbf{u}_1\\cdot\\mathbf{u}_p\\\\ &amp;=c_1|\\mathbf{u}_1|^2+0+\\cdots+0\\\\ &amp;=c_1|\\mathbf{u}_1|^2. \\end{align*}\\] Since \\(\\mathbf{u}_1\\) is nonzero, its length is not zero, so \\(c_1=0\\). Similarly, \\(c_2=c_3=\\cdots=c_p=0.\\) The only linear combination of the orthogonal set that yields the zero vector is indeed the trivial one, meaning \\(S\\) is linearly independent. Because \\(S\\) is linearly independent, it forms a basis for its span. Orthogonal bases are nice things to have. Definition 4.5 An orthogonal basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\) is a basis for \\(W\\) that is also an orthogonal set. Note that the standard basis \\[\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\\\0\\\\\\vdots\\\\0\\end{bmatrix},\\mathbf{e}_2=\\begin{bmatrix}0 \\\\1\\\\0\\\\\\vdots\\\\0\\end{bmatrix},\\cdots,\\mathbf{e}_n=\\begin{bmatrix}0\\\\0\\\\0\\\\\\vdots\\\\1\\end{bmatrix}\\] is an orthogonal basis for \\(\\mathbb{R}^n\\). When a basis is orthogonal, its easy to find a linear combination of the basis vectors that yields any vector. Theorem 4.8 Let \\(\\{\\mathbf{u}_1,\\cdots,\\mathbf{u}_p\\}\\) be an orthogonal basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\). For each \\(\\mathbf{y}\\) in \\(W\\), the weights (coefficients) in the linear combination \\[\\mathbf{y}=c_1\\mathbf{u}_1+\\cdots+c_p\\mathbf{u}_p\\] are given by \\[c_j=\\frac{\\mathbf{y}\\cdot \\mathbf{u}_j}{\\mathbf{u}_j\\cdot \\mathbf{u}_j},j=1\\dots p.\\] The proof is similar to the last proof. The dot product of \\(\\mathbf{y}\\) with each \\(\\mathbf{u}_j\\) will tell you what each \\(c_j\\) has to be. Example 4.4 In Example 4.3, \\[S=\\{\\mathbf{u}_1,\\mathbf{u}_2,\\mathbf{u}_3\\}=\\left\\{\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix},\\begin{bmatrix}-1\\\\-4\\\\7\\end{bmatrix}\\right\\}\\] is an orthogonal basis for \\(\\mathbb{R}^3\\). If \\(\\mathbf{y}=(1,2,3)\\), \\[\\mathbf{y}\\cdot \\mathbf{u}_1=8,\\mathbf{y}\\cdot \\mathbf{u}_2=6, \\text{ and } \\mathbf{y}\\cdot \\mathbf{u}_3=12,\\] while \\[\\mathbf{u}_1\\cdot \\mathbf{u}_1=11,\\mathbf{u}_2\\cdot\\mathbf{u}_2=6,\\text{ and }\\mathbf{u}_3\\cdot\\mathbf{u}_3=66.\\] After simplifying fractions, we get \\[\\mathbf{y}=\\frac{8}{11}\\mathbf{u}_1+\\mathbf{u}_2+\\frac{2}{11}\\mathbf{u}_3.\\] Orthogonal projections Given a vector \\(\\mathbf{y}\\) and another vector \\(\\mathbf{u}\\), wed like to be able to write \\[\\mathbf{y}=\\hat{\\mathbf{y}}+\\mathbf{z},\\] where \\(\\hat{\\mathbf{y}}\\) is a vector in the same direction as \\(\\mathbf{u}\\), and \\(\\mathbf{z}\\) is a vector that is orthogonal to \\(\\mathbf{u}\\) (Fig. 4.1). Figure 4.1: A vector projection Our projection \\(\\hat{\\mathbf{y}}\\) will be some scalar multiple of \\(\\mathbf{u}\\), say \\(\\hat{\\mathbf{y}}=\\alpha \\mathbf{u}\\). Then, since \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}=\\mathbf{y}-\\alpha\\mathbf{u}\\) should be orthogonal to \\(\\mathbf{u}\\), we have \\[0=(\\mathbf{y}-\\alpha\\mathbf{u})\\cdot\\mathbf{u}=\\mathbf{y}\\cdot\\mathbf{u}-\\alpha\\mathbf{u}\\cdot \\mathbf{u}.\\] Solving for \\(\\alpha\\) gives us our formula: \\[\\begin{equation} \\hat{\\mathbf{y}}=\\text{proj}_L\\mathbf{y}=\\frac{\\mathbf{y}\\cdot\\mathbf{u}}{\\mathbf{u}\\cdot \\mathbf{u}}\\mathbf{u},\\tag{4.2} \\end{equation}\\] where \\(\\text{proj}_L\\mathbf{y}\\) refers to the projection of \\(\\mathbf{y}\\) onto the line \\(L\\) spanned by \\(\\mathbf{u}\\). Example 4.5 Let \\(\\mathbf{u}=\\begin{bmatrix}1\\\\3\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}4\\\\4\\end{bmatrix}\\). Then \\[\\hat{\\mathbf{y}}=\\frac{(4,4)\\cdot (1,3)}{(1,3)\\cdot(1,3)}\\begin{bmatrix}1\\\\3\\end{bmatrix}=\\frac{16}{10}\\begin{bmatrix}1\\\\3\\end{bmatrix}=\\begin{bmatrix}8/5\\\\24/5\\end{bmatrix}.\\] Also, we have \\[\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}=\\begin{bmatrix}4\\\\4\\end{bmatrix}-\\begin{bmatrix}8/5\\\\24/5\\end{bmatrix}=\\begin{bmatrix}12/5\\\\-4/5\\end{bmatrix}.\\] A quick check shows that \\(\\mathbf{z}\\perp \\mathbf{u}\\). A couple of notes. The point \\(\\hat{\\mathbf{y}}\\) is the closest point on the line to \\(\\mathbf{y}\\). Thats important for future things. The distance from \\(\\mathbf{y}\\) to \\(\\hat{\\mathbf{y}}\\) is equal to the magnitude of \\(\\mathbf{z}\\). This is also important. Orthogonal sets are nice, but orthonormal sets are better. Definition 4.6 A set \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) is an orthonormal set if it is an orthogonal set and if \\(|\\mathbf{u}_i|=1\\) for \\(i=1,\\dots,p\\). For an orthonormal set, since \\(\\mathbf{u}_i\\cdot\\mathbf{u}_i=|\\mathbf{u}_i|^2=1^2=1,\\) the projection formula (4.2) and the earlier weight formula (Theorem 4.8) are much nicer. An orthonormal set makes for a very nice basis. Definition 4.7 An orthonormal basis for a subspace \\(W\\) is a orthogonal basis for \\(W\\) whose basis vectors all have unit length. Note that the standard basis is an orthonormal basis for \\(\\mathbb{R}^n\\), but its not always the most useful one. Example 4.6 We can always turn an orthogonal basis into an orthonormal basis. From Example 4.3, \\[S=\\{\\mathbf{u}_1,\\mathbf{u}_2,\\mathbf{u}_3\\}=\\left\\{\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix},\\begin{bmatrix}-1\\\\-4\\\\7\\end{bmatrix}\\right\\}\\] we have \\(|\\mathbf{u}_1|=\\sqrt{11},|\\mathbf{u}_2|=\\sqrt{6},\\) and \\(|\\mathbf{u}_3|=\\sqrt{66}\\), so \\[\\mathbf{v}_1=\\begin{bmatrix}3/\\sqrt{11}\\\\1/\\sqrt{11}\\\\1/\\sqrt{11}\\end{bmatrix},\\mathbf{v}_2=\\begin{bmatrix}-1/\\sqrt{6}\\\\2/\\sqrt{6}\\\\1/\\sqrt{6}\\end{bmatrix},\\mathbf{v}_3=\\begin{bmatrix}-1/\\sqrt{66}\\\\-4/\\sqrt{66}\\\\7/\\sqrt{66}\\end{bmatrix}\\] is an orthonormal basis for \\(\\mathbb{R}^3\\). Orthogonal matrices Matrices whose columns are orthonormal vectors have some nice properties. Consider the matrix \\(\\mathbf{U}\\) whose columns are the orthonormal vectors from our last example. \\[\\mathbf{U}=\\begin{bmatrix} 3/\\sqrt{11} &amp; -1/\\sqrt{6} &amp; -1/\\sqrt{66}\\\\1/\\sqrt{11} &amp; 2/\\sqrt{6} &amp; -4/\\sqrt{66}\\\\1/\\sqrt{11}&amp;1/\\sqrt{6}&amp;7/\\sqrt{66}\\end{bmatrix}.\\] If we consider the product \\(\\mathbf{U}^T\\mathbf{U}\\), we have \\[\\begin{bmatrix}3/\\sqrt{11} &amp; 1/\\sqrt{11} &amp; 1/\\sqrt{11}\\\\ -1/\\sqrt{6} &amp; 2/\\sqrt{6} &amp; 1/\\sqrt{6}\\\\-1/\\sqrt{66} &amp; -4/\\sqrt{66} &amp; 7/\\sqrt{66}\\end{bmatrix}\\begin{bmatrix} 3/\\sqrt{11} &amp; -1/\\sqrt{6} &amp; -1/\\sqrt{66}\\\\1/\\sqrt{11} &amp; 2/\\sqrt{6} &amp; -4/\\sqrt{66}\\\\1/\\sqrt{11}&amp;1/\\sqrt{6}&amp;7/\\sqrt{66}\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp;0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\] Since \\[\\mathbf{u}_i^T\\mathbf{u}_j=\\begin{cases} 1, &amp; i=j\\\\0, &amp; i\\neq j\\end{cases},\\] we get ones on the diagonal and zeros off of it. This can be a quick way to identify an orthonormal set of vectors using technology. Theorem 4.9 An \\(m\\times n\\) matrix \\(\\mathbf{U}\\) has orthonormal columns if and only if \\(\\mathbf{U}^T\\mathbf{U}=\\mathbf{I}\\). An \\(n\\times n\\) matrix \\(\\mathbf{U}\\) with orthonormal columns is called an orthogonal matrix (Dont ask why its not called orthonormal.) Theyre nice matrices. In particular, thanks to Theorem 4.9, if \\(\\mathbf{U}\\) is an orthogonal matrix, then \\(\\mathbf{U}^{-1}=\\mathbf{U}^T.\\) In general, Matrices with orthonormal columns have some useful properties, collected in the following theorem. Theorem 4.10 Let \\(\\mathbf{U}\\) be an \\(m\\times n\\) matrix with orthonormal columns, and let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be vectors in \\(\\mathbb{R}^n\\). Then \\(|\\mathbf{U}\\mathbf{x}|=|\\mathbf{x}|\\). (Multiplication by \\(\\mathbf{U}\\) preserves lengths.) \\((\\mathbf{U}\\mathbf{x})\\cdot(\\mathbf{U}\\mathbf{y})=\\mathbf{x}\\cdot\\mathbf{y}\\). (Multiplication of two vectors by \\(\\mathbf{U}\\) preserves inner products and angles.) \\((\\mathbf{U}\\mathbf{x})\\cdot(\\mathbf{U}\\mathbf{y})=0\\) if and only if \\(\\mathbf{x}\\cdot\\mathbf{y}=0\\). (Multiplication by \\(\\mathbf{U}\\) preserves orthogonality. Proof. The proofs of all three parts are simple and share the same flavor. For 1, note that \\[|\\mathbf{U}\\mathbf{x}|^2=(\\mathbf{U}\\mathbf{x})^T(\\mathbf{U}\\mathbf{x})=\\mathbf{x}^T\\underbrace{\\mathbf{U}^T\\mathbf{U}}_{\\mathbf{I}}\\mathbf{x}=\\mathbf{x}^T\\mathbf{x}=|\\mathbf{x}|^2.\\] For 2, \\[(\\mathbf{U}\\mathbf{x})\\cdot(\\mathbf{U}\\mathbf{y})=(\\mathbf{U}\\mathbf{x})^T(\\mathbf{U}\\mathbf{y})=\\mathbf{x}^T\\mathbf{U}^T\\mathbf{U}\\mathbf{y}=\\mathbf{x}^T\\mathbf{y}=\\mathbf{x}\\cdot\\mathbf{y}.\\] Property 3 is a simple consequence of 2. This is one of many instances where the realization of a dot product of two vectors as a matrix product of the transpose of one with the other comes in handy. Example 4.7 Let \\(\\mathbf{U}=\\begin{bmatrix}\\frac{3}{5}&amp;\\frac{4}{5}\\\\\\frac{4}{5}&amp;-\\frac{3}{5}\\end{bmatrix},\\) and let \\(\\mathbf{x}=\\begin{bmatrix}5\\\\12\\end{bmatrix}\\). Its easy to see that the columns of \\(\\mathbf{U}\\) are orthonormal. Also, \\(|\\mathbf{x}|=\\sqrt{5^2+12^2}=\\sqrt{169}=13\\). When we multiply, we get \\[\\mathbf{U}\\mathbf{x}=\\begin{bmatrix}\\frac{3}{5}&amp;\\frac{4}{5}\\\\\\frac{4}{5}&amp;-\\frac{3}{5}\\end{bmatrix}\\begin{bmatrix}5\\\\12\\end{bmatrix}=\\begin{bmatrix}63/5\\\\-16/5\\end{bmatrix},\\] and \\(|\\mathbf{U}\\mathbf{x}|=\\sqrt{(63/5)^2+(-16/5)^2}=\\sqrt{4225/25}=\\sqrt{169}=13.\\) 4.4 Orthogonal Projections In the last section, we learned how to project a vector onto the line spanned by another vector. In this section, we want to learn how to project a vector onto a more general subspace. Figure 4.2: A vector projection onto a subspace Suppose we have a basis for \\(\\mathbb{R}^n\\), like \\(\\{\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_n\\}\\). Then for any vector \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\), we can write \\(\\mathbf{y}=\\mathbf{z}_1+\\mathbf{z}_2\\), where \\(\\mathbf{z}_1\\) is in the span of some of the \\(\\mathbf{u}_i\\), while \\(\\mathbf{z}_2\\) is whats left over. For example, suppose that \\(\\{\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_5\\}\\) is an orthogonal basis for \\(\\mathbb{R}^5\\), and \\(W\\) is the subspace spanned by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). Then \\[\\mathbf{y}=\\underbrace{c_1\\mathbf{u}_1+c_2\\mathbf{u}_2}_{\\mathbf{z}_1}+\\underbrace{c_3\\mathbf{u}_3+c_4\\mathbf{u}_4+c_5\\mathbf{u}_5}_{\\mathbf{z}_2}.\\] Note that \\(\\mathbf{z}_1\\) is in the span of \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\), so it is in \\(W\\). It should be clear that \\(\\mathbf{z}_2\\) is in \\(W^\\perp\\), since \\(\\mathbf{u}_3,\\mathbf{u}_4,\\) and \\(\\mathbf{u}_5\\) are orthogonal to \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). This decomposition of a vector into a sum of one vector from a subspace \\(W\\) and another one from \\(W^\\perp\\) can always be done. Theorem 4.11 Let \\(W\\) be a subspace of \\(\\mathbb{R}^n\\). Then each \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\) can be written uniquely in the form \\[\\mathbf{y}=\\hat{\\mathbf{y}}+\\mathbf{z},\\] where \\(\\hat{\\mathbf{y}}\\) is in \\(W\\) and \\(\\mathbf{z}\\) is in \\(W^\\perp\\). In fact, if \\(\\{\\mathbf{u}_1,\\dots \\mathbf{u}_p\\}\\) is any orthogonal basis for \\(W\\), then \\[\\hat{\\mathbf{y}}=\\frac{\\mathbf{y}\\cdot\\mathbf{u}_1}{\\mathbf{u}_1\\cdot \\mathbf{u}_1}\\mathbf{u}_1+\\cdots+\\frac{\\mathbf{y}\\cdot \\mathbf{u}_p}{\\mathbf{u}_p\\cdot \\mathbf{u}_p}\\mathbf{u}_p,\\] and \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\). The vector \\(\\hat{\\mathbf{y}}\\) is the orthogonal projection of \\(\\mathbf{y}\\) onto the subspace \\(W\\), denoted \\(\\text{proj}_W\\mathbf{y}.\\) Example 4.8 Let \\[\\mathbf{u}_1=\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix},\\text{ and } \\mathbf{u}_2=\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix}.\\] If \\(\\mathbf{y}=(1,2,3)\\), find the projection of \\(\\mathbf{y}\\) onto the subspace \\(W\\) spanned by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). (Note that \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\) are orthogonal.) We have \\[\\begin{align*} \\hat{\\mathbf{y}}&amp;=\\frac{\\mathbf{y}\\cdot\\mathbf{u}_1}{\\mathbf{u}_1\\cdot \\mathbf{u}_1}\\mathbf{u}_1+\\frac{\\mathbf{y}\\cdot\\mathbf{u}_2}{\\mathbf{u}_2\\cdot \\mathbf{u}_2}\\mathbf{u}_2\\\\ &amp;=\\frac{8}{11}\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix}+\\frac{6}{6}\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix}=\\begin{bmatrix}13/11\\\\30/11\\\\19/11\\end{bmatrix}. \\end{align*}\\] We should see if \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\) is in \\(W^\\perp\\). We have \\[\\mathbf{z}=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}-\\begin{bmatrix}13/11\\\\30/11\\\\19/11\\end{bmatrix}=\\begin{bmatrix}-2/11\\\\-8/11\\\\14/11\\end{bmatrix}.\\] Since \\[\\mathbf{z}\\cdot \\mathbf{u}_1=\\begin{bmatrix}-2/11\\\\-8/11\\\\14/11\\end{bmatrix}\\cdot\\begin{bmatrix}3\\\\1\\\\1\\end{bmatrix}=0,\\text{ and }\\mathbf{z}\\cdot \\mathbf{u}_2=\\begin{bmatrix}-2/11\\\\-8/11\\\\14/11\\end{bmatrix}\\cdot\\begin{bmatrix}-1\\\\2\\\\1\\end{bmatrix}=0,\\] we see that \\(\\mathbf{z}\\) is in \\(W^\\perp\\). We mentioned in the last section that the projection of a vector \\(\\mathbf{y}\\) onto a line spanned by another vector is the closest point on that line to \\(\\mathbf{y}\\). The analogous result is true for projections onto more general subspaces. Theorem 4.12 Let \\(W\\) be a subspace of \\(\\mathbb{R}^n\\), let \\(\\mathbf{y}\\) be any vector in \\(\\mathbb{R}^n\\), and let \\(\\hat{\\mathbf{y}}\\) be the orthogonal projection of \\(\\mathbf{y}\\) onto \\(W\\). Then \\(\\hat{\\mathbf{y}}\\) is the closest point in \\(W\\) to \\(\\mathbf{y}\\) in the sense that \\[|\\mathbf{y}-\\hat{\\mathbf{y}}|&lt; |\\mathbf{y}-\\mathbf{v}|\\] for all \\(\\mathbf{v}\\) in \\(W\\) distinct from \\(\\hat{\\mathbf{y}}\\). Proof. We can write \\(\\mathbf{y}-\\mathbf{v}=(\\mathbf{y}-\\hat{\\mathbf{y}})+(\\hat{\\mathbf{y}}-\\mathbf{v})\\). Since \\(\\mathbf{v}\\) is in \\(W\\), \\(\\hat{\\mathbf{y}}-\\mathbf{v}\\) is too, and it is orthogonal to \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}},\\) so we can use the Pythagorean Theorem with \\(\\mathbf{y}-\\mathbf{v}\\) as the hypotenuse (see Fig. 4.3): \\[|\\mathbf{y}-\\mathbf{v}|^2=|\\mathbf{y}-\\hat{\\mathbf{y}}|^2+|\\hat{\\mathbf{y}}-\\mathbf{v}|^2.\\] Since \\(\\hat{\\mathbf{y}}\\) and \\(\\mathbf{v}\\) are distinct, \\(|\\hat{\\mathbf{y}}-\\mathbf{v}|^2&gt;0\\) and \\(|\\mathbf{y}-\\mathbf{v}|^2&gt;|\\mathbf{y}-\\hat{\\mathbf{y}}|^2.\\) Figure 4.3: Projection onto a subspace Things are again cleaner if we are working with an orthonormal basis. Theorem 4.13 If \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) is an orthonormal basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\), then \\[\\text{proj}_W\\mathbf{y}=(\\mathbf{y}\\cdot\\mathbf{u}_1)\\mathbf{u}_1+\\cdots+(\\mathbf{y}\\cdot\\mathbf{u}_p)\\mathbf{u}_p.\\] If \\(\\mathbf{U}=\\begin{bmatrix}\\mathbf{u}_1 &amp; \\mathbf{u}_2 &amp; \\cdots &amp; \\mathbf{u}_p\\end{bmatrix}\\), then \\[\\text{proj}_W\\mathbf{y}=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}\\] for all \\(\\mathbf{y}\\) in \\(\\mathbb{R}^n\\). The first part of this theorem is just the previous projection theorem with \\(\\mathbf{u}_i\\cdot\\mathbf{u}_i=1\\) for all \\(i\\). The second part comes from the fact that the weights are \\(\\mathbf{u}_1^T\\mathbf{y},\\mathbf{u}_2^T\\mathbf{y},\\dots \\mathbf{u}_p^T\\mathbf{y}\\) and can be written in a vector as \\(\\mathbf{U}^T\\mathbf{y}\\), which, when multiplied by \\(\\mathbf{U}\\) on the left gives us the desired linear combination. Example 4.9 Let \\(W\\) be the subspace of \\(\\mathbb{R}^3\\) spanned by the orthonormal vectors \\[\\{\\mathbf{u}_1,\\mathbf{u}_2\\}=\\left\\{\\begin{bmatrix}1/3\\\\2/3\\\\2/3\\end{bmatrix},\\begin{bmatrix} 2/3\\\\ 1/3\\\\-2/3\\end{bmatrix}\\right\\}.\\] Find the projection matrix \\(\\mathbf{U}\\mathbf{U}^T\\) and use it to project the vector \\(\\mathbf{y}=(1,2,3)\\) onto \\(W\\). With \\[\\mathbf{U}=\\begin{bmatrix} 1/3 &amp; 2/3 \\\\ 2/3 &amp; 1/3\\\\ 2/3 &amp; -2/3\\end{bmatrix},\\] we have \\[\\mathbf{U}\\mathbf{U}^T=\\begin{bmatrix} 1/3 &amp; 2/3 \\\\ 2/3 &amp; 1/3\\\\ 2/3 &amp; -2/3\\end{bmatrix}\\begin{bmatrix} 1/3 &amp; 2/3 &amp; 2/3\\\\2/3 &amp; 1/3 &amp; -2/3\\end{bmatrix} =\\begin{bmatrix} 5/9 &amp; 4/9 &amp; -2/9\\\\ 4/9 &amp; 5/9 &amp; 2/9\\\\ -2/9 &amp; 2/9 &amp; 8/9\\end{bmatrix}.\\] (Remember that \\(\\mathbf{U}^T\\mathbf{U}=\\mathbf{I}\\), the \\(2\\times 2\\) identity matrix.) This gives us \\[\\begin{align*} \\hat{\\mathbf{y}}&amp;=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}\\\\ &amp;=\\begin{bmatrix} 5/9 &amp; 4/9 &amp; -2/9\\\\ 4/9 &amp; 5/9 &amp; 2/9\\\\ -2/9 &amp; 2/9 &amp; 8/9\\end{bmatrix}\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}7/9\\\\20/9\\\\26/9\\end{bmatrix}. \\end{align*}\\] You can check to see that \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}=(2/9,-2/9,1/9)\\) is orthogonal to \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2\\). 4.5 Orthogonalization We dont always start out with an orthogonal basis for a subspace. There is a process for taking any basis and turning it into an orthogonal one, called the Gram-Schmidt process. Heres how we do it. Suppose \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_p\\}\\) is a basis for a subspace \\(W\\) of \\(\\mathbb{R}^n\\). Define \\[\\begin{align} \\mathbf{v}_1&amp;=\\mathbf{x}_1\\\\ \\nonumber \\mathbf{v}_2&amp;=\\mathbf{x}_2-\\frac{\\mathbf{x}_2\\cdot \\mathbf{v}_1}{\\mathbf{v}_1\\cdot\\mathbf{v}_1}\\mathbf{v}_1\\\\ \\nonumber \\mathbf{v}_3&amp;=\\mathbf{x}_3-\\frac{\\mathbf{x}_3\\cdot \\mathbf{v}_1}{\\mathbf{v}_1\\cdot\\mathbf{v}_1}\\mathbf{v}_1-\\frac{\\mathbf{x}_3\\cdot \\mathbf{v}_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2\\\\ \\nonumber &amp;\\vdots\\\\ \\nonumber \\mathbf{v}_p&amp;=\\mathbf{x}_p-\\frac{\\mathbf{x}_p\\cdot \\mathbf{v}_1}{\\mathbf{v}_1\\cdot\\mathbf{v}_1}\\mathbf{v}_1-\\frac{\\mathbf{x}_p\\cdot \\mathbf{v}_2}{\\mathbf{v}_2\\cdot \\mathbf{v}_2}\\mathbf{v}_2-\\cdots -\\frac{\\mathbf{x}_p\\cdot \\mathbf{v}_{p-1}}{\\mathbf{v}_{p-1}\\cdot \\mathbf{v}_{p-1}}\\mathbf{v}_{p-1}. \\tag{4.3} \\end{align}\\] Then \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\dots,\\mathbf{v}_p\\}\\) is an orthogonal basis for \\(W\\). We can then get an orthonormal basis \\(\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_p\\}\\) for \\(W\\) with \\(\\mathbf{u}_i=\\mathbf{v}_i/|\\mathbf{v}_i|\\) for \\(i=1,\\dots,p.\\) How it works: At each step, were removing the part of the vector that is in the span of the previous ones, leaving us with a vector that is orthogonal to its predecessors. In practice, this can be tedious, and we can use technology to perform the process. We will see a small example. Example 4.10 Let \\(\\mathbf{x}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}\\) and let \\(\\mathbf{x}_2=\\begin{bmatrix}1\\\\2\\end{bmatrix}\\). Note that \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) are linearly independent, so they span \\(\\mathbb{R}^2\\). The first step is simple: \\[\\mathbf{v}_1=\\mathbf{x}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix}.\\] Then \\[\\begin{align*} \\mathbf{v}_2&amp;=\\mathbf{x}_2-\\frac{\\mathbf{x}_2\\cdot \\mathbf{v}_1}{\\mathbf{v}_1\\cdot\\mathbf{v}_1}\\mathbf{v}_1\\\\ &amp;=\\begin{bmatrix}1\\\\2\\end{bmatrix}-\\frac{(1,2)\\cdot(1,1)}{(1,1)\\cdot(1,1)}\\begin{bmatrix}1\\\\1\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}1\\\\2\\end{bmatrix}-\\frac{3}{2}\\begin{bmatrix}1\\\\1\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}-1/2\\\\1/2\\end{bmatrix}. \\end{align*}\\] Now that we have an orthogonal basis, we can turn it into an orthonormal basis: \\[\\{\\mathbf{u}_1,\\mathbf{u}_2\\}=\\left\\{\\frac{1}{|\\mathbf{v}_1|}\\mathbf{v}_1,\\frac{1}{|\\mathbf{v}_2|}\\mathbf{v}_2\\right\\}.\\] With \\(|\\mathbf{v}_1|=\\sqrt{1^2+1^2}=\\sqrt{2}\\) and \\(|\\mathbf{v}_2|=\\sqrt{(-1/2)^2+(1/2)^2}=\\sqrt{1/2}=1/\\sqrt{2}\\), we have \\[\\{\\mathbf{u}_1,\\mathbf{u}_2\\}=\\left\\{\\begin{bmatrix}1/\\sqrt{2}\\\\1/\\sqrt{2}\\end{bmatrix},\\begin{bmatrix}-1/\\sqrt{2}\\\\1/\\sqrt{2}\\end{bmatrix}\\right\\}.\\] QR-factorization One bonus feature of the Gram-Schmidt orthonormalization procedure is that it leads to an important factorization for certain matrices. Suppose that \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix with independent columns \\(\\{\\mathbf{x}_1,\\mathbf{x}_2,\\dots,\\mathbf{x}_n\\}\\) (so we must have \\(n\\leq m\\)). By the Gram-Schmidt process, we can create an orthonormal collection of vectors using the columns of \\(\\mathbf{A}\\), \\(\\{\\mathbf{u}_1 ,\\mathbf{u}_2 ,\\dots , \\mathbf{u}_n\\}.\\) Let \\(\\mathbf{Q}=\\begin{bmatrix}\\mathbf{u}_1 &amp; \\mathbf{u}_2 &amp; \\cdots &amp; \\mathbf{u}_n\\end{bmatrix}\\) be the \\(m\\times n\\) matrix created using this orthonormal basis as the columns. For \\(j=1\\dots n,\\mathbf{x}_j\\) is in \\(\\text{span}(\\mathbf{x}_1,\\dots,\\mathbf{x}_j)\\), which is the same as \\(\\text{span}(\\mathbf{u}_1,\\cdots \\mathbf{u}_j)\\), so we can write \\[\\mathbf{x}_j=r_{1j}\\mathbf{u}_1+\\cdots+r_{jj}\\mathbf{u}_j+0\\mathbf{u}_{j+1}+\\cdots+0\\mathbf{u}_n.\\] We can store these weights in the \\(n\\)-dimensional vector \\[\\mathbf{r}_j=\\begin{bmatrix}r_{1j}\\\\r_{2j}\\\\ \\vdots \\\\r_{jj}\\\\0\\\\ \\vdots\\\\0\\end{bmatrix},\\] and \\(\\mathbf{x}_j=\\mathbf{Q}\\mathbf{r}_j\\), for \\(j=1\\dots n\\). If we define \\(\\mathbf{R}=\\begin{bmatrix}\\mathbf{r}_1 &amp; \\cdots &amp; \\mathbf{r}_n\\end{bmatrix}\\), then \\[\\mathbf{A}=\\begin{bmatrix}\\mathbf{x}_1 &amp; \\cdots &amp; \\mathbf{x}_n\\end{bmatrix}=\\mathbf{Q}\\mathbf{R}.\\] What weve just shown is that any \\(m\\times n\\) matrix \\(\\mathbf{A}\\) with linearly independent columns can be written as \\[\\mathbf{A}=\\mathbf{Q}\\mathbf{R},\\] where \\(\\mathbf{Q}\\) is an \\(m\\times n\\) matrix with orthonormal columns that form a basis for the column space of \\(\\mathbf{A},\\) and \\(\\mathbf{R}\\) is an upper triangular matrix. This is the QR-factorization of \\(\\mathbf{A}.\\) Note that \\(\\mathbf{Q}\\) comes from the Gram-Schmidt procedure, and we can calculate \\(\\mathbf{R}\\) using the fact that \\[\\mathbf{Q}^T\\mathbf{A}=\\mathbf{Q}^T\\mathbf{Q}\\mathbf{R}=\\mathbf{I}\\mathbf{R}=\\mathbf{R},\\] so \\(\\mathbf{R}=\\mathbf{Q}^T\\mathbf{A}\\). Example 4.11 Lets use our work in Example 4.10 to find a QR-factorization. Let \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 1\\\\1 &amp; 2\\end{bmatrix},\\] the matrix whose columns are \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) from before. Our \\(\\mathbf{Q}\\) matrix will be \\[\\mathbf{Q}=\\begin{bmatrix}1/\\sqrt{2}&amp;-1/\\sqrt{2}\\\\1/\\sqrt{2} &amp; 1/\\sqrt{2}\\end{bmatrix}\\] and we can calculate \\(\\mathbf{R}:\\) \\[\\mathbf{R}=\\mathbf{Q}^T\\mathbf{A}=\\begin{bmatrix}1/\\sqrt{2} &amp;1/\\sqrt{2}\\\\-1/\\sqrt{2} &amp; 1/\\sqrt{2}\\end{bmatrix}\\begin{bmatrix}1 &amp; 1\\\\1 &amp; 2\\end{bmatrix}=\\begin{bmatrix}\\sqrt{2} &amp; 3/\\sqrt{2}\\\\0 &amp; 1/\\sqrt{2}\\end{bmatrix}.\\] Lets check: \\[\\mathbf{Q}\\mathbf{R}=\\begin{bmatrix}1/\\sqrt{2}&amp;-1/\\sqrt{2}\\\\1/\\sqrt{2} &amp; 1/\\sqrt{2}\\end{bmatrix}\\begin{bmatrix}\\sqrt{2} &amp; 3/\\sqrt{2}\\\\0 &amp; 1/\\sqrt{2}\\end{bmatrix}=\\begin{bmatrix}1 &amp; 1\\\\1 &amp; 2\\end{bmatrix}\\checkmark.\\] Why is this factorization useful? Suppose we are trying to solve \\(\\mathbf{A}\\mathbf{x}=\\mathbf{y}\\), and we have the QR-factorization for \\(\\mathbf{A}\\). Then \\[\\begin{align*} \\mathbf{A}\\mathbf{x}&amp;=\\mathbf{y}\\\\ \\mathbf{Q}\\mathbf{R}\\mathbf{x}&amp;=\\mathbf{y}\\\\ \\mathbf{Q}^T\\mathbf{Q}\\mathbf{R}\\mathbf{x}&amp;=\\mathbf{Q}^T\\mathbf{y}\\\\ \\mathbf{R}\\mathbf{x}&amp;=\\mathbf{Q}^T\\mathbf{y} \\end{align*}\\] Finding a matrix inverse, especially for a large matrix, can be numerically challenging. Since \\(\\mathbf{Q}\\) is orthogonal, its inverse is just its transpose, which is a simple computational task. Since \\(\\mathbf{R}\\) is upper triangular, we can solve the remaining problem by back substitution, which is also fairly computationally easy. The R package pracma has a gramSchmidt function that will find the QR-factorization of a matrix. The columns of \\(\\mathbf{Q}\\) will have the Gram-Schmidt orthonormal basis for the column space of the original matrix. library(pracma) A &lt;- matrix(c(1,2,3,4,5,6), nrow = 3) A ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 gramSchmidt(A) ## $Q ## [,1] [,2] ## [1,] 0.2672612 0.8728716 ## [2,] 0.5345225 0.2182179 ## [3,] 0.8017837 -0.4364358 ## ## $R ## [,1] [,2] ## [1,] 3.741657 8.552360 ## [2,] 0.000000 1.963961 If the columns of the matrix are dependent, we will get an error. B &lt;- matrix(c(1,2,3,2,4,6), nrow = 3) B ## [,1] [,2] ## [1,] 1 2 ## [2,] 2 4 ## [3,] 3 6 gramSchmidt(B) ## Error in gramSchmidt(B): Matrix &#39;A&#39; does not have full rank. 4.6 Exercises Let \\(\\mathbf{v}=\\begin{bmatrix}1\\\\5\\\\-2\\end{bmatrix}.\\) Find a unit vector in the direction of \\(\\mathbf{v}\\). Find two independent vectors that are orthogonal to \\(\\mathbf{v}.\\) Let \\(\\mathbf{v}=\\begin{bmatrix}1\\\\-4\\\\6\\end{bmatrix}.\\) Find a unit vector in the direction of \\(\\mathbf{v}\\). Find two independent vectors that are orthogonal to \\(\\mathbf{v}.\\) Let \\(\\mathbf{a}_1=\\begin{bmatrix}1\\\\3\\\\4\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix} 2\\\\2\\\\-2\\end{bmatrix}.\\) Show that \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) are orthogonal. Find a third nonzero vector \\(\\mathbf{a}_3\\) that is orthogonal to both \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2.\\) Use the following method: let \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) be the rows of a matrix. A vector orthogonal to the rows would be in what space? Let \\(\\mathbf{a}_1=\\begin{bmatrix}1\\\\2\\\\6\\end{bmatrix}\\) and \\(\\mathbf{a}_2=\\begin{bmatrix} 2\\\\2\\\\-1\\end{bmatrix}.\\) Show that \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\) are orthogonal. Find a third nonzero vector \\(\\mathbf{a}_3\\) that is orthogonal to both \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2.\\) Let \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix},\\mathbf{v}_2=\\begin{bmatrix} 3\\\\1\\\\-4\\end{bmatrix},\\) and \\(\\mathbf{v}_3=\\begin{bmatrix}5\\\\-7\\\\2\\end{bmatrix}.\\) Show that \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{v}_3\\}\\) is an orthogonal set. Write \\(\\mathbf{y}=\\begin{bmatrix} 2\\\\3\\\\4\\end{bmatrix}\\) as a linear combination of \\(\\mathbf{v}_1,\\mathbf{v}_2,\\) and \\(\\mathbf{v}_3.\\) Let \\(\\mathbf{v}_1=\\begin{bmatrix}1\\\\2\\\\1\\end{bmatrix},\\mathbf{v}_2=\\begin{bmatrix} 2\\\\0\\\\-2\\end{bmatrix},\\) and \\(\\mathbf{v}_3=\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.\\) Show that \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{v}_3\\}\\) is an orthogonal set. Write \\(\\mathbf{y}=\\begin{bmatrix} 4\\\\3\\\\2\\end{bmatrix}\\) as a linear combination of \\(\\mathbf{v}_1,\\mathbf{v}_2,\\) and \\(\\mathbf{v}_3.\\) Let \\(\\mathbf{u}=\\begin{bmatrix}1\\\\2\\\\-3\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}.\\) Find \\(\\hat{\\mathbf{y}}=\\text{proj}_\\mathbf{u}\\mathbf{y}.\\) Show that \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\) is orthogonal to \\(\\mathbf{u}.\\) Let \\(\\mathbf{u}=\\begin{bmatrix}6\\\\0\\\\8\\end{bmatrix}\\) and \\(\\mathbf{y}=\\begin{bmatrix}2\\\\1\\\\0\\end{bmatrix}.\\) Find \\(\\hat{\\mathbf{y}}=\\text{proj}_\\mathbf{u}\\mathbf{y}.\\) Show that \\(\\mathbf{z}=\\mathbf{y}-\\hat{\\mathbf{y}}\\) is orthogonal to \\(\\mathbf{u}.\\) Turn \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{v}_3\\}\\) from Problem 5 into an orthonormal set. Turn \\(\\{\\mathbf{v}_1,\\mathbf{v}_2,\\mathbf{v}_3\\}\\) from Problem 6 into an orthonormal set. Let \\(W\\) be the subspace of \\(\\mathbb{R}^3\\) spanned by the orthonormal vectors \\[\\{\\mathbf{u}_1,\\mathbf{u}_2\\}=\\left\\{\\begin{bmatrix}3/7\\\\6/7\\\\-2/7\\end{bmatrix},\\begin{bmatrix}-2/7\\\\3/7\\\\6/7\\end{bmatrix}\\right\\}.\\] Let \\(\\mathbf{U}=\\begin{bmatrix} \\mathbf{u}_1 &amp; \\mathbf{u}_2\\end{bmatrix}.\\) Find the projection matrix \\(\\mathbf{U}\\mathbf{U}^T.\\) Project \\(\\mathbf{y}=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\) onto the subspace spanned by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2.\\) Let \\(W\\) be the subspace of \\(\\mathbb{R}^3\\) spanned by the orthonormal vectors \\[\\{\\mathbf{u}_1,\\mathbf{u}_2\\}=\\left\\{\\begin{bmatrix}3/13\\\\12/13\\\\-4/13\\end{bmatrix},\\begin{bmatrix}4/13\\\\3/13\\\\12/13\\end{bmatrix}\\right\\}.\\] Let \\(\\mathbf{U}=\\begin{bmatrix} \\mathbf{u}_1 &amp; \\mathbf{u}_2\\end{bmatrix}.\\) Find the projection matrix \\(\\mathbf{U}\\mathbf{U}^T.\\) Project \\(\\mathbf{y}=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\) onto the subspace spanned by \\(\\mathbf{u}_1\\) and \\(\\mathbf{u}_2.\\) Let \\[\\mathbf{A}=\\begin{bmatrix}1 &amp; 1 &amp; 0\\\\1 &amp; 1 &amp; 1\\\\0 &amp; 1 &amp; 1\\end{bmatrix}.\\] Perform the Gram-Schmidt orthonormalization procedure on the columns of \\(\\mathbf{A}\\) by hand. Find the QR-factorization of \\(\\mathbf{A}\\) Let \\[\\mathbf{A}=\\begin{bmatrix}0 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 0\\end{bmatrix}.\\] Perform the Gram-Schmidt orthonormalization procedure on the columns of \\(\\mathbf{A}\\) by hand. Find the QR-factorization of \\(\\mathbf{A}\\) Use the Gram-Schmidt procedure to find a third vector that is orthogonal to the two vectors given in Problem 3. Hint: \\(\\mathbf{e}_1,\\) the first standard basis vector is not in the span of \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\). Use the Gram-Schmidt procedure to find a third vector that is orthogonal to the two vectors given in Problem 4. Hint: \\(\\mathbf{e}_1,\\) the first standard basis vector is not in the span of \\(\\mathbf{a}_1\\) and \\(\\mathbf{a}_2\\). "],["regression.html", "Chapter 5 Regression 5.1 Least Squares Problems 5.2 Application: The Massey Method 5.3 Least Squares Regression 5.4 Correlation 5.5 Formulas for Least Squares Regression 5.6 Uncertainty in Least Squares 5.7 Multilinear Regression 5.8 Model Selection 5.9 Logistic Regression 5.10 Gradient Descent 5.11 Exercises", " Chapter 5 Regression The focus in this chapter is regression: fitting linear and similar models to data. There is a beautiful connection between this problem and the matrix algebra material that we have studied so far. 5.1 Least Squares Problems Some linear equations have no solution. For instance, if we try to solve \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), where \\[\\mathbf{A}=\\begin{bmatrix}4 &amp; 0\\\\0 &amp; 2\\\\ 1&amp; 1\\end{bmatrix}, \\text{ and } \\mathbf{b}=\\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix},\\] we run into a problem when we put the augmented matrix in reduced row echelon form: \\[\\left[\\begin{array}{cc|c}4 &amp; 0 &amp; 2\\\\0 &amp; 2 &amp; 0\\\\1 &amp; 1 &amp; 11\\end{array}\\right]\\leadsto \\left[\\begin{array}{cc|c} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{array}\\right].\\] Remember that the last row says \\(0=1\\), so there is no solution. In terms of linear algebra, what is going on here? The matrix product \\[\\mathbf{A}\\mathbf{x}=\\begin{bmatrix}4 &amp; 0\\\\0 &amp; 2\\\\ 1&amp; 1\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}=\\begin{bmatrix}4x_1+0x_2\\\\0x_1+2x_2\\\\1x_1+1x_2\\end{bmatrix}=x_1\\begin{bmatrix}4\\\\0\\\\1\\end{bmatrix}+x_2\\begin{bmatrix}0\\\\2\\\\1\\end{bmatrix}\\] is really a linear combination of the columns of \\(\\mathbf{A}.\\) There is no solution because \\[\\mathbf{b}=\\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix}\\] cannot be written as a linear combination of the columns of \\(\\mathbf{A}.\\) That is, \\(\\mathbf{b}\\) is not in the column space of \\(\\mathbf{A}.\\) In a least squares problem, we project \\(\\mathbf{b}\\) into the column space of \\(\\mathbf{A}\\) (see Fig. 5.1) \\[\\hat{\\mathbf{b}}=\\text{proj}_{\\text{Col}(\\mathbf{A})}\\mathbf{b},\\] and solve \\(\\mathbf{A}\\mathbf{x}=\\hat{\\mathbf{b}}\\), which we know will have a solution. Figure 5.1: A projection onto the column space Why do we call it least squares? Remember from Theorem 4.12 that the projection \\(\\hat{\\mathbf{b}}\\) satisfies \\[|\\mathbf{b}-\\hat{\\mathbf{b}}|&lt;|\\mathbf{b}-\\mathbf{v}|\\] for any vector \\(\\mathbf{v}\\) in \\(\\text{Col}(\\mathbf{A})\\) different from \\(\\hat{\\mathbf{b}}\\). If \\(\\mathbf{b}=(b_1,b_2,b_3)\\) and \\(\\mathbf{v}=(v_1,v_2,v_3),\\) then \\[|\\mathbf{b}-\\mathbf{v}|^2=(b_1-v_1)^2+(b_2-v_2)^2+(b_3-v_3)^2.\\] The projection \\(\\hat{\\mathbf{b}}\\) minimizes this sum of squares over all possible vectors in \\(\\text{Col}(\\mathbf{A})\\). How do we find the least squares solution? We want to find a vector \\(\\hat{\\mathbf{x}}\\) that satisfies \\[\\mathbf{A}\\hat{\\mathbf{x}}=\\hat{\\mathbf{b}}.\\] The key is that \\(\\mathbf{b}-\\hat{\\mathbf{b}}\\) is orthogonal to the column space of \\(\\mathbf{A}.\\) If \\(\\mathbf{a}_j\\) is any column of \\(\\mathbf{A},\\) then \\(\\mathbf{a}_j\\cdot(\\mathbf{b}-\\hat{\\mathbf{b}})=0\\), or \\[\\mathbf{a}_j^T (\\mathbf{b}-\\hat{\\mathbf{b}})=\\mathbf{a}_j^T (\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}})=0.\\] Since the \\(\\mathbf{a}_j^T\\) vectors form the rows of \\(\\mathbf{A}^T\\) we actually have \\[\\mathbf{A}^T(\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}})=\\mathbf{0}.\\] When we apply the distributive law to \\[\\mathbf{A}^T(\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}})=\\mathbf{0},\\] we get \\[\\mathbf{A}^T\\mathbf{b}-\\mathbf{A}^T\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{0},\\] or \\[\\mathbf{A}^T\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{A}^T\\mathbf{b}.\\] This leads us to what are called the normal equations. Theorem 5.1 The set of least-squares solutions of \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) coincides with the nonempty set of solutions of the normal equations \\[\\mathbf{A}^T\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{A}^T\\mathbf{b}.\\] By the nature of the problem, there will always be a least-squares solution for the equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\). Under additional conditions, the solution will be unique. Well see those conditions in a bit. Example 5.1 Lets return to our example \\[\\begin{bmatrix}4 &amp; 0\\\\0 &amp; 2\\\\ 1&amp; 1\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix}\\] and find a least squares solution. We calculate \\[\\mathbf{A}^T\\mathbf{A}=\\begin{bmatrix}4 &amp; 0 &amp; 1\\\\0 &amp; 2 &amp; 1\\end{bmatrix}\\begin{bmatrix}4 &amp; 0\\\\0 &amp; 2\\\\ 1&amp; 1\\end{bmatrix}=\\begin{bmatrix}17 &amp; 1\\\\1 &amp; 5\\end{bmatrix}\\] and \\[\\mathbf{A}^T\\mathbf{b}=\\begin{bmatrix}4 &amp; 0 &amp; 1\\\\0 &amp; 2 &amp; 1\\end{bmatrix}\\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix}=\\begin{bmatrix}19\\\\11\\end{bmatrix}.\\] To find our least squares solution, we solve \\(\\mathbf{A}^T\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{A}^T\\mathbf{b}\\): \\[\\begin{bmatrix}17 &amp; 1\\\\1 &amp; 5\\end{bmatrix}\\hat{\\mathbf{x}}=\\begin{bmatrix}19\\\\11\\end{bmatrix}.\\] Using the inverse of \\(\\mathbf{A}^T\\mathbf{A}\\), we get our solution: \\[\\hat{\\mathbf{x}}=\\frac{1}{17\\cdot 5-1\\cdot 1}\\begin{bmatrix}5 &amp; -1\\\\-1 &amp; 17\\end{bmatrix}\\begin{bmatrix}19\\\\11\\end{bmatrix}=\\frac{1}{84}\\begin{bmatrix}84\\\\168\\end{bmatrix}=\\begin{bmatrix}1\\\\2\\end{bmatrix}.\\] Example 5.2 Find the least squares solution to \\[\\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix}.\\] Note that the first column is the sum of the other three, so the columns are linearly dependent. If we try to find a regular (non-least squares) solution to the equation via row reduction, we get \\[\\left[\\begin{array}{cccc|r} 1 &amp; 1 &amp; 0 &amp; 0 &amp; -3\\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; -1\\\\1 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 1 &amp; 0 &amp; 2\\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 5\\\\1 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{array}\\right]\\leadsto \\left[\\begin{array}{cccr|r} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right],\\] so there is no solution. For the normal equations, we find \\[\\mathbf{A}^T\\mathbf{A}=\\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}=\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2\\\\2 &amp; 2 &amp; 0 &amp; 0\\\\2 &amp; 0 &amp; 2 &amp; 0\\\\2 &amp; 0 &amp; 0 &amp; 2\\end{bmatrix},\\] and \\[\\mathbf{A}^T\\mathbf{b}=\\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{bmatrix}\\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix}=\\begin{bmatrix}4\\\\-4\\\\2\\\\6\\end{bmatrix}.\\] It turns out that \\(\\mathbf{A}^T\\mathbf{A}\\) is not invertible, so we use row reduction to solve: \\[\\left[\\begin{array}{cccc|r} 6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\\\2 &amp; 2 &amp; 0 &amp; 0&amp; -4\\\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\\\2 &amp; 0 &amp; 0 &amp; 2&amp; 6\\end{array}\\right] \\leadsto \\left[\\begin{array}{cccr|r}1 &amp; 0 &amp; 0 &amp; 1 &amp; 3\\\\0 &amp; 1 &amp; 0 &amp; -1 &amp; -5\\\\0 &amp; 0 &amp; 1 &amp; -1 &amp; -2\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right].\\] With \\(x_4=t\\) as a free variable, we have \\(x_1=3-t,x_2=-5+t\\), and \\(x_3=-2+t\\). We can write our least squares solution as \\[\\hat{\\mathbf{x}}=\\begin{bmatrix}3\\\\-5\\\\-2\\\\0\\end{bmatrix}+t\\begin{bmatrix}-1\\\\1\\\\1\\\\1\\end{bmatrix}.\\] Note that the vector multiplied by \\(t\\) is in the null space of \\(\\mathbf{A}^T\\mathbf{A}\\) and the null space of \\(\\mathbf{A},\\) too. They actually share the same null space. Theorem 5.2 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. Then \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\mathbf{A}\\) have the same nullspace. Proof. Let \\(\\mathbf{x}\\in \\text{Null}(\\mathbf{A})\\). Then \\[ (\\mathbf{A}^T\\mathbf{A})\\mathbf{x}=\\mathbf{A}^T(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^T\\mathbf{0}=\\mathbf{0}.\\] Any vector in the nullspace of \\(\\mathbf{A}\\) is also in the nullspace of \\(\\mathbf{A}^T\\mathbf{A}\\). Suppose \\(\\mathbf{x}\\in\\text{Null}(\\mathbf{A}^T\\mathbf{A}).\\) Then \\[\\mathbf{A}^T(\\mathbf{A}\\mathbf{x})=(\\mathbf{A}^T\\mathbf{A})\\mathbf{x}=\\mathbf{0}.\\] That means that \\(\\mathbf{x}\\in \\text{Null}(\\mathbf{A}^T).\\) From Theorem 4.6 (see, were using it), we know that \\(\\text{Null}(\\mathbf{A}^T)=\\text{Col}(\\mathbf{A})^{\\perp}.\\) The vector \\(\\mathbf{A}\\mathbf{x}\\) is simultaneously in \\(\\text{Col}(\\mathbf{A})^{\\perp}\\) and \\(\\text{Col}(\\mathbf{A})\\). The only vector common to a subspace and its orthogonal complement is the zero vector, so \\(\\mathbf{A}\\mathbf{x}=\\mathbf{0}\\) and \\(\\mathbf{x}\\in\\text{Null}(\\mathbf{A}).\\) Any vector in \\(\\text{Null}(\\mathbf{A}^T\\mathbf{A})\\) is also in \\(\\text{Null}(\\mathbf{A}).\\) They are the same subspace. Why did Example 5.2 have infinitely many solutions? Since the columns of \\(\\mathbf{A}\\) were not linearly independent, there were actually infinitely many ways to write the projection \\(\\hat{\\mathbf{b}}\\) in terms of them. This does not happen when the columns are linearly independent. Theorem 5.3 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. The following are equivalent: The equation \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) has a unique least squares solution for each \\(\\mathbf{b}\\) in \\(\\mathbb{R}^m\\). The columns of \\(\\mathbf{A}\\) are linearly independent. The matrix \\(\\mathbf{A}^T\\mathbf{A}\\) is invertible. When these statements are true, the least squares solution is given by \\[\\hat{\\mathbf{x}}=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}.\\] The last formula \\[\\hat{\\mathbf{x}}=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}\\] looks awkward, and we might be tempted to use the formula for the inverse of a product \\[(\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1},\\] but there are two issues: \\(\\mathbf{A}\\) and \\(\\mathbf{A}^T\\) are not necessarily invertible and, They dont even need to be square. In fact, if \\(\\mathbf{A}\\) were invertible, then we wouldnt need a least squares solution, since \\(\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\\) is a regular solution. Least squares error How close does the least squares solution come to being an actual solution? The distance between \\(\\mathbf{b}\\) and \\(\\hat{\\mathbf{b}}\\), which is \\(|\\mathbf{b}-\\hat{\\mathbf{b}}|=|\\mathbf{b}-\\mathbf{A}\\hat{\\mathbf{x}}|\\), tells us this. In Example 5.1, \\[\\mathbf{b}=\\begin{bmatrix}2\\\\0\\\\11\\end{bmatrix},\\] while \\[\\hat{\\mathbf{b}}=\\mathbf{A}\\hat{\\mathbf{x}}=\\begin{bmatrix}4&amp;0\\\\0 &amp; 2\\\\1 &amp; 1\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix}=\\begin{bmatrix}4\\\\4\\\\3\\end{bmatrix}.\\] The least squares error is \\[\\sqrt{(2-4)^2+(0-4)^2+(11-3)^2}=\\sqrt{84}\\approx 9.2.\\] Least squares and the QR-factorization If the columns of \\(\\mathbf{A}\\) are linearly independent, then we can write \\(\\mathbf{A}=\\mathbf{Q}\\mathbf{R}\\), where \\(\\mathbf{Q}\\) is orthogonal and \\(\\mathbf{R}\\) is invertible. In this case, the least squares solution is given by \\[\\hat{\\mathbf{x}}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{b}.\\] Why does this work? If \\(\\hat{\\mathbf{x}}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{b}\\), then \\[\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{Q}\\mathbf{R}\\hat{\\mathbf{x}}=\\mathbf{Q}\\mathbf{R}\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{b}=\\mathbf{Q}\\mathbf{Q}^T\\mathbf{b},\\] which we learned in Theorem 4.13 is the orthogonal projection of \\(\\mathbf{b}\\) onto the column space of \\(\\mathbf{Q}\\), which is the same as the column space of \\(\\mathbf{A}\\). This projection is \\(\\hat{\\mathbf{b}}\\). 5.2 Application: The Massey Method So far, all of our ranking methods have ignored the points scored by teams or players. The Massey Method is a least squares method that is actually designed to predict the point spread in a competition. Ken Massey developed his method as part of an honors project in college [15]. His site is also an excellent source of sports schedule data. The development and notation in this section come from Amy Langville and Carl Meyer [11]. The idea behind the ratings is that if teams \\(i\\) and \\(j\\) play in game number \\(k\\), the point spread \\(y_k\\) will just be the difference of the two teams ratings: \\[r_i-r_j=y_k.\\] Note that this method can easily handle ties, since we can just set the margin of victory to be 0. If there are \\(n\\) teams in the league, and a total of \\(m\\) games, we will get an \\(m\\times n\\) system of equations. Since the number of games will quickly exceed the number of teams, the system of equations will be overdetermined, but we can still find a least squares solution. Recall our toy example. Aardvarks Beagles Crocs Donkeys Egrets Aardvarks x 17-31 6-42 x 17-7 Beagles x 3-59 10-35 x Crocs x 21-20 49-0 Donkeys x 28-21 Egrets x The first few lines of the system of equations will look like this \\[ \\left\\{\\begin{array}{rrr} r_1-r_2&amp;=&amp;-14\\\\ r_1-r_3&amp;=&amp;-36\\\\ r_1-r_5&amp;=&amp;10 \\end{array} \\right. . \\] In matrix form, the full system looks like this. \\[\\left[\\begin{array}{rrrrr}1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; -1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 0 &amp; 0 &amp; -1\\\\0 &amp; 1 &amp; -1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; -1 &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; -1\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -1\\end{array}\\right]\\begin{bmatrix}r_1\\\\r_2\\\\r_3\\\\r_4\\\\r_5\\end{bmatrix}=\\left[\\begin{array}{r}-14 \\\\-36\\\\10\\\\-56\\\\-25\\\\1\\\\49\\\\7\\end{array}\\right]\\] or \\[\\mathbf{X}\\mathbf{r}=\\mathbf{y}.\\] This system is overdetermined, so we instead solve the normal equations \\[\\mathbf{X}^T\\mathbf{X}\\mathbf{r}=\\mathbf{X}^T\\mathbf{y}.\\] That system is \\[\\left[\\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\\\-1 &amp; 0 &amp; -1 &amp; -1 &amp; 3\\end{array}\\right]\\begin{bmatrix}r_1\\\\r_2\\\\r_3\\\\r_4\\\\r_5\\end{bmatrix}=\\left[\\begin{array}{r}-40\\\\-67\\\\142\\\\31\\\\-66\\end{array}\\right].\\] First note that \\(\\mathbf{M}=\\mathbf{X}^T\\mathbf{X}\\) looks very similar to the Colley matrix. The only difference is that the diagonal entries are the number of games that each team has played. The \\(ij\\)-entry is still the number of times teams \\(i\\) and \\(j\\) have played, multiplied by \\(-1\\). The \\(\\mathbf{p}=\\mathbf{X}^T\\mathbf{y}\\) vector represents the scoring difference for each team. For instance, Team 1 (The Aardvarks) scored 40 fewer points than their opponents, while the Crocs (Team 3) outscored their opponents by a whopping 142 points, while the Aardvarks were outscored by 40 points. There is one problem with this system. The columns (and rows) all add up to 0, so they are dependent and there will not be a unique solution. Recall that if the columns of the original matrix were dependent, then there is not a unique solution to the least squares problem. In our original matrix, the rows all added up to 0, because each 1 was paired with a -1. Masseys workaround is to replace one of the rows (we will chose the last one) of \\(\\mathbf{M}\\) with a row of ones to create \\(\\overline{\\mathbf{M}}\\). He also replaces the last entry in \\(\\mathbf{p}\\) with a 0 to create \\(\\bar{\\mathbf{p}}\\). This essentially adds a new constraint on the system: all of the ratings will add up to 0. Here is the revised system: \\[\\left[\\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\end{array}\\right]\\begin{bmatrix}r_1\\\\r_2\\\\r_3\\\\r_4\\\\r_5\\end{bmatrix}=\\left[\\begin{array}{r}-40\\\\-67\\\\142\\\\31\\\\0\\end{array}\\right].\\] or \\[\\overline{\\mathbf{M}}\\mathbf{r}=\\bar{\\mathbf{p}}.\\] When we solve the revised system, we get our ratings vector: \\[\\hat{\\mathbf{r}}=\\begin{bmatrix}-12.73\\\\-13.47\\\\28.4\\\\10.93\\\\-13.13\\end{bmatrix}.\\] Our standings in order of Massey ratings is as follows. Team W-L PCT Rating PF PA PD Crocs 4-0 1.000 28.40 171 29 142 Donkeys 2-1 0.667 10.93 83 52 31 Aardvarks 1-2 0.333 -12.73 40 80 -40 Egrets 0-3 0.000 -13.13 28 94 -66 Beagles 1-2 0.333 -13.47 44 111 -67 Note that the Beagles drop to the bottom of the ratings, mostly due to their blowout losses to the Crocs and Donkeys. Again, the difference in ratings can be used to predict point spreads. If the Aardvarks play the Donkeys, the predicted point spread is \\((-12.73)-10.93=-23.66\\). The Donkeys are 23.66-point favorites. The Massey Method Create the matrix \\(\\mathbf{M}\\), whose diagonal entries \\(\\mathbf{M}_{ii}\\) are the number of games played by Team \\(i\\), and whose off-diagonal entries \\(\\mathbf{M}_{ij}\\) are the number of games between Teams \\(i\\) and \\(j\\) multiplied by \\(-1\\). Create the column vector \\(\\mathbf{p}\\), whose \\(i\\)th entry is the point difference for Team \\(i\\). Create the matrix \\(\\overline{\\mathbf{M}}\\) by replacing the last row of \\(\\mathbf{M}\\) with a row of ones. Create the vector \\(\\bar{\\mathbf{p}}\\) by replacing the last entry of \\(\\mathbf{p}\\) with a 0. Solve \\(\\overline{\\mathbf{M}}\\mathbf{r}=\\bar{\\mathbf{p}}\\) for the ratings vector \\(\\mathbf{r}=\\hat{\\mathbf{r}}\\). Adjustments What if you dont want to take points into consideration? You can change the margin of victory to 1 in every game. In this case, the new \\(\\mathbf{X}^T\\mathbf{y}\\) vector will just be the number of wins minus the number of losses for each team. For our running example, the system becomes \\[\\left[\\begin{array}{rrrrr}3 &amp; -1 &amp; -1 &amp; 0 &amp; -1\\\\-1 &amp; 3 &amp; -1 &amp; -1 &amp; 0\\\\-1 &amp; -1 &amp; 4 &amp; -1 &amp; -1\\\\0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\end{array}\\right]\\begin{bmatrix}r_1\\\\r_2\\\\r_3\\\\r_4\\\\r_5\\end{bmatrix}=\\left[\\begin{array}{r}-1\\\\-1\\\\4\\\\1\\\\0\\end{array}\\right].\\] The new ratings vector is \\((-0.333,-0.067,0.8,0.333,-0.733)\\). The Beagles arent penalized by their blowout losses. (We could also impose a cap on the margin of victory.) Offensive and defensive ratings Masseys method also allows you to split a teams rating into offensive and defensive ratings. Let \\(\\mathbf{f}\\) and \\(\\mathbf{a}\\) be the vectors of points for and against, respectively, for each team. Then the point difference vector \\(\\mathbf{p}=\\mathbf{f}-\\mathbf{a}\\). Write \\(\\mathbf{M}=\\mathbf{T}-\\mathbf{P},\\) where \\(\\mathbf{T}\\) is the diagonal part of \\(\\mathbf{M}\\), representing the number of games each team has played, and \\(\\mathbf{P}\\) shows the number of matchups between each pair of teams. We now introduce the offensive \\(\\mathbf{o}\\) and defensive \\(\\mathbf{d}\\) vectors, with \\(\\mathbf{r}=\\mathbf{o}+\\mathbf{d}\\) as follows. \\[\\begin{align*} \\mathbf{M}\\mathbf{r}&amp;=\\mathbf{p}\\\\ (\\mathbf{T}-\\mathbf{P})\\mathbf{r}&amp;=\\mathbf{p}\\\\ (\\mathbf{T}-\\mathbf{P})(\\mathbf{o}+\\mathbf{d})&amp;=\\mathbf{p}\\\\ \\mathbf{T}\\mathbf{o}-\\mathbf{P}\\mathbf{o}+\\mathbf{T}\\mathbf{d}-\\mathbf{P}\\mathbf{d}&amp;=\\mathbf{p}\\\\ \\mathbf{T}\\mathbf{o}-\\mathbf{P}\\mathbf{o}+\\mathbf{T}\\mathbf{d}-\\mathbf{P}\\mathbf{d}&amp;=\\mathbf{f}-\\mathbf{a}. \\end{align*}\\] We split this last equation into two separate ones: \\[\\mathbf{T}\\mathbf{o}-\\mathbf{P}\\mathbf{d}=\\mathbf{f}\\] and \\[\\mathbf{T}\\mathbf{d}-\\mathbf{P}\\mathbf{o}=-\\mathbf{a}.\\] The first equation says that the number of points a team scores depends on their offensive rating and their opponents defensive ratings. We can interpret the second in a similar manner. Having already solved for \\(\\mathbf{r},\\) we can use the first equation to solve for \\(\\mathbf{d},\\) using \\(\\mathbf{o}=\\mathbf{r}-\\mathbf{d}\\): \\[\\begin{align*} \\mathbf{T}\\mathbf{o}-\\mathbf{P}\\mathbf{d}&amp;=\\mathbf{f}\\\\ \\mathbf{T}(\\mathbf{r}-\\mathbf{d})-\\mathbf{P}\\mathbf{d}&amp;=\\mathbf{f}\\\\ (\\mathbf{T}+\\mathbf{P})\\mathbf{d}&amp;=\\mathbf{T}\\mathbf{r}-\\mathbf{f}. \\end{align*}\\] We can solve this for \\(\\mathbf{d},\\) and once we know \\(\\mathbf{d},\\) we can find \\(\\textbf{o}.\\) Here are the results. Team Rating Offense Defense Rank OffRank DefRank Crocs 28.40 28.82 -0.43 1 1 1 Donkeys 10.93 14.34 -3.41 2 2 2 Aardvarks -12.73 0.01 -12.74 3 5 3 Egrets -13.13 3.81 -16.94 4 4 4 Beagles -13.47 9.14 -22.61 5 3 5 The Massey method has some issues. First, if the underlying graph is disconnected, then the Massey matrix \\(\\bar{\\textbf{M}}\\) will still be singular. For example, in the following matrix, the first three teams have no connections with the last three. After adjustment, the Massey matrix is still singular. (Check to see that \\(\\det \\bar{\\mathbf{M}}=0\\).) \\[\\bar{\\mathbf{M}}=\\left[\\begin{array}{rrrrrr}2 &amp; -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\\\-1 &amp; 2 &amp; -1 &amp; 0 &amp; 0 &amp; 0\\\\-1 &amp; -1 &amp; 2 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 2 &amp; -1 &amp; -1\\\\0 &amp; 0 &amp; 0 &amp; -1 &amp; 2 &amp; -1\\\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\end{array}\\right] \\] It is also possible that the matrix \\(\\textbf{T}+\\textbf{P}\\) is singular, leading to non-unique offensive and defensive ratings. It will take some further manipulation to come up with reasonable adjustments in both of these cases. 5.3 Least Squares Regression You may have seen least squares regression in statistics or some other class. The idea is that, given a collection of pairs of points \\((x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n),\\) wed like to find a function of the form \\(y=b_0+b_1x\\) that fits the data the best. Figure 5.2: A scatterplot The problem is, when you have three or more points, they probably wont lie on the same line, as in the scatterplot in Figure 5.2. If they were on a straight line, then we could get \\(b_0\\) and \\(b_1\\) by solving the system of equations \\[\\begin{align*} b_0+b_1x_1&amp;=y_1\\\\ b_0+b_1x_2&amp;=y_2\\\\ \\vdots &amp;\\\\ b_0+b_1x_n&amp;=y_n, \\end{align*}\\] which we can write in matrix form as \\[\\begin{bmatrix}1 &amp; x_1\\\\1 &amp; x_2\\\\\\vdots &amp; \\vdots\\\\1 &amp; x_n\\end{bmatrix}\\begin{bmatrix}b_0\\\\b_1\\end{bmatrix}=\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix}\\] or \\(\\mathbf{X}\\mathbf{b}=\\mathbf{y}\\). The matrix \\(\\mathbf{X}\\) is called the design matrix. If you do have more than two pairs of points, the system is overdetermined, and there will only be a solution if the points all lie on a straight line. If not, we can, find a least squares approximation. That least squares solution will be \\(\\hat{\\mathbf{b}}\\), the solution to the normal equations \\[\\mathbf{X}^T\\mathbf{X}\\mathbf{b}=\\mathbf{X}^T\\mathbf{y}.\\] It will be unique if the columns of \\(X\\) are linearly independent. Since there are only two columns in this case, and the first column is all ones, this boils down to having at least two different \\(x\\) values in the data. The points on the scatterplot in Figure (5.2) are \\((4,1),(8,6),(10,5),\\) and \\((7,7)\\). That makes our system \\[\\begin{bmatrix}1 &amp; 4\\\\1 &amp; 8\\\\1 &amp; 10\\\\1 &amp; 7\\end{bmatrix}\\mathbf{b}=\\begin{bmatrix}1\\\\6\\\\5\\\\7\\end{bmatrix}.\\] We have \\[\\mathbf{X}^T\\mathbf{X}=\\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1\\\\4 &amp; 8 &amp; 10 &amp; 7\\end{bmatrix}\\begin{bmatrix}1 &amp; 4\\\\1 &amp; 8\\\\1 &amp; 10\\\\1 &amp; 7\\end{bmatrix}=\\begin{bmatrix}4 &amp; 29\\\\29 &amp; 229\\end{bmatrix}.\\] Also, \\[\\mathbf{X}^T\\mathbf{y}=\\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1\\\\4 &amp; 8 &amp; 10 &amp; 7\\end{bmatrix}\\begin{bmatrix}1\\\\6\\\\5\\\\7\\end{bmatrix}=\\begin{bmatrix}19\\\\151\\end{bmatrix},\\] so our least squares solution is \\[\\hat{\\mathbf{b}}=\\frac{1}{4\\cdot229-29\\cdot29}\\begin{bmatrix}229 &amp; -29\\\\-29 &amp; 4\\end{bmatrix}\\begin{bmatrix}19\\\\151\\end{bmatrix}=\\frac{1}{75}\\begin{bmatrix}-28\\\\53\\end{bmatrix}.\\] Our least squares coefficients are \\(\\hat{b}_0=-\\frac{28}{75}\\approx -0.373\\) and \\(\\hat{b}_1=\\frac{53}{75}\\approx0.707\\). When we put the line \\(y=-\\frac{28}{75}+\\frac{53}{75}x\\) on the scatterplot, we see the least squares regression line (Fig. 5.3). Its not a great fit, mostly because of the point \\((4,1).\\) Figure 5.3: A fitted least squares regression line If youve seen regression before, you might wonder how this method compares to that method. Note that \\[\\mathbf{X}^T\\mathbf{X}=\\begin{bmatrix}1 &amp; 1 &amp; \\cdots &amp; 1\\\\x_1 &amp; x_2 &amp; \\cdots &amp; x_n\\end{bmatrix}\\begin{bmatrix}1 &amp; x_1\\\\1 &amp; x_2\\\\\\vdots &amp; \\vdots\\\\1 &amp; x_n\\end{bmatrix}=\\begin{bmatrix} n &amp; \\sum x_i\\\\ \\sum x_i &amp; \\sum x_i^2\\end{bmatrix}\\] and \\[\\mathbf{X}^T\\mathbf{y}=\\begin{bmatrix}1 &amp; 1 &amp; \\cdots &amp; 1\\\\x_1 &amp; x_2 &amp; \\cdots &amp; x_n\\end{bmatrix}\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots \\\\y_n\\end{bmatrix}=\\begin{bmatrix} \\sum y_i\\\\ \\sum x_iy_i\\end{bmatrix}.\\] All of these terms show up in the formulas for the coefficients that you may have seen. This method produces the same results. Multilinear regression We can even add independent variables to our least squares linear regression models, without changing the complexity of the mathematical theory. Example 5.3 Consider the following data in \\((x,y,z)\\)-form. \\[(1,4,5),(2,3,1),(6,5,2),(2,1,1),(4,3,3)\\] Suppose we want to fit a linear model of the form \\(z=b_0+b_1x+b_2y\\) to the data. The graph of this function is a plane, and if all of our sample points lie on that plane, then there will be a solution to the following system of equations. \\[\\begin{equation*} \\left\\{ \\begin{array}{rcl} b_0+1b_1+4b_2&amp;=5\\\\ b_0+2b_1+3b_2&amp;=1\\\\ b_0+6b_1+5b_2&amp;=2\\\\ b_0+2b_1+1b_2&amp;=1\\\\ b_0+4b_1+3b_2&amp;=3\\\\ \\end{array} \\right. \\end{equation*}\\] We can write this in matrix form as \\[\\begin{bmatrix} 1 &amp; 1 &amp; 4\\\\1 &amp; 2 &amp; 3\\\\1 &amp; 6 &amp; 5\\\\1 &amp; 2 &amp; 1\\\\1 &amp; 4 &amp; 3\\end{bmatrix}\\begin{bmatrix} b_0\\\\b_1\\\\b_2\\end{bmatrix}=\\begin{bmatrix}5\\\\1\\\\2\\\\1\\\\3\\end{bmatrix}\\] or \\(\\mathbf{X}\\mathbf{b}=\\mathbf{z}.\\) Our matrix \\(\\mathbf{X}\\) has a first column of ones. The second column consists of the \\(x\\) values, and the third consists of the \\(y\\) values. The \\(\\mathbf{z}\\) vector contains the \\(z\\) values. If the points dont lie on a plane we can still find a least squares solution using the normal equations. The least squares solution will be unique if the columns of \\(\\mathbf{X}\\) are linearly independent. Dependence would only occur if \\(y\\) were a linear function of \\(x\\) or if \\(x\\) were a linear function of \\(y\\). We get \\[\\mathbf{X}^T\\mathbf{X}=\\begin{bmatrix}5 &amp; 15 &amp; 16\\\\15 &amp; 61 &amp; 54\\\\16 &amp; 54 &amp; 60\\end{bmatrix}\\] and \\[\\mathbf{X}^T\\mathbf{z}=\\begin{bmatrix}12\\\\33\\\\43\\end{bmatrix}.\\] Our least squares solution is \\[\\hat{\\mathbf{b}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{z}=\\begin{bmatrix}301/262\\\\-135/262\\\\229/262\\end{bmatrix}\\approx\\begin{bmatrix}1.149\\\\-0.515\\\\0.874\\end{bmatrix}.\\] The equation is \\[z=1.149-0.515x+0.874y.\\] 5.4 Correlation It would be nice to have a number to describe how linear the relationship between two variables is. We have a few of those. The first we will investigate is Pearsons correlation coefficient. Here is some motivation for the topic. In Figure 5.4, we have data where \\(y\\) is a linear function of \\(x\\) with positive slope. On the right, we have centered the data by subtracting the \\(x\\) mean from the \\(x\\) values and the \\(y\\) mean from the \\(y\\) values. Lets call the centered values \\(\\hat{x}\\) and \\(\\hat{y}\\). When we do this, we get points that are on a line with the same slope, and the intercept is 0. Figure 5.4: Centering data If we put the centered values in vectors, \\(\\hat{\\mathbf{x}}\\) and \\(\\hat{\\mathbf{y}}\\), we have \\(\\hat{\\mathbf{y}}=m\\hat{\\mathbf{x}},\\) where \\(m\\) is the positive slope. We cant see the \\(\\hat{\\mathbf{x}}\\) and \\(\\hat{\\mathbf{y}}\\) vectors because they live in \\(\\mathbb{R}^5\\), but the vectors are pointing in the same direction, so the angle between them, \\(\\theta,\\) is \\(0^{\\circ}.\\) Figure 5.5: Negative slope In Figure 5.5, the points lie on a straight line with negative slope. When we center the values, the angle between the centered vectors is \\(180^{\\circ}.\\) Recall that the angle between two vectors satisfies Equation (4.1) \\[\\cos\\theta=\\frac{\\mathbf{u}\\cdot \\mathbf{v}}{|\\mathbf{u}||\\mathbf{v}|}.\\] For the points on the line with positive slope, we have \\(\\cos\\theta=1,\\) and for the points on the line with negative slope, we have \\(\\cos\\theta=-1.\\) If points are not on a straight line, the angle between the centered vector will be between \\(0^{\\circ}\\) and \\(180^{\\circ},\\) and the cosine of that angle will be between \\(-1\\) and \\(1.\\) Were going to call this number \\(r.\\) Some notation and a formula If our points are \\((x_1,y_1),\\dots,(x_n,y_n),\\) then the dot product of the centered vectors is \\[\\hat{\\mathbf{x}}\\cdot\\hat{\\mathbf{y}}=(x_1-\\bar{x})(y_1-\\bar{y})+\\cdots+(x_n-\\bar{x})(y_n-\\bar{y})=\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}).\\] The sigma notation means we are adding all terms of the form \\((x_i-\\bar{x})(y_i-\\bar{y})\\) with subscripts \\(i\\) running from 1 to \\(n.\\) The magnitudes of \\(\\hat{\\mathbf{x}}\\) and \\(\\hat{\\mathbf{y}}\\) can be written as \\[\\sqrt{(x_1-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2}=\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\] and \\[\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}.\\] This gives our formula for Pearsons correlation coefficient. \\[\\begin{equation} r=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}} \\tag{5.1} \\end{equation}\\] We can get these terms using matrices as follows. Let \\[ \\mathbf{B}=\\begin{bmatrix}\\hat{\\mathbf{x}} &amp; \\hat{\\mathbf{y}}\\end{bmatrix}=\\begin{bmatrix} x_1-\\bar{x} &amp; y_1-\\bar{y}\\\\ \\vdots &amp; \\vdots \\\\ x_n-\\bar{x} &amp; y_n-\\bar{y}\\end{bmatrix}.\\] Then \\[\\begin{align*} \\mathbf{B}^T \\mathbf{B}&amp;=\\begin{bmatrix} x_1-\\bar{x} &amp; \\cdots &amp; x_n-\\bar{x}\\\\ x_1-\\bar{y} &amp; \\cdots &amp; y_n-\\bar{y}\\end{bmatrix} \\begin{bmatrix} x_1-\\bar{x} &amp; y_1-\\bar{y}\\\\ \\vdots &amp; \\vdots \\\\ x_n-\\bar{x} &amp; y_n-\\bar{y}\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} \\sum_{i=1}^n(x_i-\\bar{x})^2 &amp; \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})\\\\ \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) &amp; \\sum_{i=1}^n(y_i-\\bar{y})^2\\end{bmatrix}. \\end{align*}\\] Note: the matrix \\(\\mathbf{S}=\\frac{1}{n-1}\\mathbf{B}^T \\mathbf{B}\\) is called the covariance matrix (or sample covariance matrix). The diagonal entries of \\(\\mathbf{S}\\) give the sample variance for the corresponding values, while the off-diagonal entries give the sample covariance, which is like an unscaled version of the correlation coefficient. We will see this matrix again when we study principal component analysis. Before we do some calculations, here are some properties of \\(r.\\) Properties of Pearsons Correlation Coefficient \\(r\\) \\(-1\\leq r\\leq 1\\) If \\(r=1,\\) there is perfect positive correlation: all of the \\((x,y)\\) pairs lie on a straight line with positive slope. If \\(r=-1,\\) there is perfect negative correlation. The closer \\(r\\) is to \\(1\\) or \\(-1,\\) the stronger the correlation. The closer \\(r\\) is to \\(0,\\) the weaker the linear correlation. There may be some other type of correlation. It is always a good idea to look at a scatterplot to see if there is some sort of non-linear trend in the data. Correlation computations are almost always performed using calculators or software packages, but sometimes it is illustrative to do calculations by hand. In that case, there are some formulas that make it a little easier to do hand calculations. Here they are \\[\\begin{equation} S_{xx}=\\sum_{i=1}^n(x_i-\\bar{x})^2=\\left(\\sum_{i=1}^n x_i^2\\right)-n\\bar{x}^2 \\tag{5.2} \\end{equation}\\] \\[\\begin{equation} S_{yy}=\\sum_{i=1}^n(y_i-\\bar{y})^2=\\left(\\sum_{i=1}^n y_i^2\\right)-n\\bar{y}^2 \\tag{5.3} \\end{equation}\\] \\[\\begin{equation} S_{xy}=\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})=\\left(\\sum_{i=1}^n x_iy_i\\right)-n\\bar{x}\\bar{y} \\tag{5.4} \\end{equation}\\] These allow us to write the correlation coefficient as \\[\\begin{equation} r=\\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}. \\tag{5.5} \\end{equation}\\] The terms \\(\\sum_{i=1}^nx_i^2\\) and \\(\\sum_{i=1}^ny_i^2\\) mean we square the \\(x\\) and \\(y\\) values and add them up. The \\(\\sum_{i=1}^n x_iy_i\\) means we multiply each \\((x_i,y_i)\\) pair and add them up. These terms show up frequently in statistical settings. Example 5.4 Here is a simple example with five nice pairs of points. The sums appear in bold at the bottom. \\(x_i\\) \\(y_i\\) \\(x_i^2\\) \\(y_i^2\\) \\(x_i y_i\\) 4 2 16 4 8 -1 3 1 9 -3 5 2 25 4 10 2 -3 4 9 -6 0 1 0 1 0 10 5 46 27 9 We have \\(\\bar{x}=10/5=2,\\bar{y}=5/5=1,S_{xx}=46-5*2^2=26, S_{yy}=27-5*1^2=22,\\) and \\(S_{xy}=9-5*2*1=-1,\\) so \\(r=\\dfrac{-1}{\\sqrt{26*22}}=-0.042.\\) This is very weak negative linear correlation, as you can see from the scatterplot (Fig. 5.6). Figure 5.6: Weak correlation Example 5.5 \\(x_i\\) \\(y_i\\) \\(x_i^2\\) \\(y_i^2\\) \\(x_i y_i\\) 1 9 1 81 9 4 3 16 9 12 2 7 4 49 14 5 1 25 1 5 12 20 46 140 40 With \\(n=4\\) this time, \\(\\bar{x}=12/4=3\\) and \\(\\bar{y}=20/4=5.\\) That gives us \\(S_{xx}=46-4\\cdot3^2=10,S_{yy}=140-4\\cdot 5^2=40,\\) and \\(S_{xy}=40-4\\cdot 3\\cdot 5=-20.\\) So, \\[r=\\frac{-20}{\\sqrt{10\\cdot40}}=-1.\\] Our points lie on a straight line with negative slope (Fig. 5.7). Figure 5.7: Perfect negative correlation Example 5.6 Here is one last simple example. \\(x_i\\) \\(y_i\\) \\(x_i^2\\) \\(y_i^2\\) \\(x_i y_i\\) -2 4 4 16 -8 -1 1 1 1 -1 0 0 0 0 0 1 1 1 1 1 2 4 4 16 8 0 10 10 34 0 We have \\(\\bar{x}=0,\\bar{y}=10/5=2,\\) and \\(S_{xy}=0-5\\cdot0\\cdot2\\), so \\(r=0.\\) There is no linear correlation between the \\(x\\) and \\(y\\) values. We should always look at the scatterplot, though. Figure 5.8 tells us what we should have noticed from the table of values. The points all lie on the parabola \\(y=x^2.\\) Figure 5.8: Always look at the scatterplot Correlation in R The cor function in R will calculate Pearsons and other correlation coefficients. Pearsons is the default. cor(mtcars$wt,mtcars$mpg) ## [1] -0.8676594 The with function makes it easier to work with data frames. with(mtcars, cor(wt,mpg)) ## [1] -0.8676594 Figure 5.9 shows the scatterplot. Figure 5.9: Miles per gallon and weight have a strong negative correlation If you use the cor function on a numerical data frame, you get a table of pairwise correlations. Note that the correlation of a variable with itself is always 1. round(cor(mtcars), 2) ## mpg cyl disp hp drat wt qsec vs am gear carb ## mpg 1.00 -0.85 -0.85 -0.78 0.68 -0.87 0.42 0.66 0.60 0.48 -0.55 ## cyl -0.85 1.00 0.90 0.83 -0.70 0.78 -0.59 -0.81 -0.52 -0.49 0.53 ## disp -0.85 0.90 1.00 0.79 -0.71 0.89 -0.43 -0.71 -0.59 -0.56 0.39 ## hp -0.78 0.83 0.79 1.00 -0.45 0.66 -0.71 -0.72 -0.24 -0.13 0.75 ## drat 0.68 -0.70 -0.71 -0.45 1.00 -0.71 0.09 0.44 0.71 0.70 -0.09 ## wt -0.87 0.78 0.89 0.66 -0.71 1.00 -0.17 -0.55 -0.69 -0.58 0.43 ## qsec 0.42 -0.59 -0.43 -0.71 0.09 -0.17 1.00 0.74 -0.23 -0.21 -0.66 ## vs 0.66 -0.81 -0.71 -0.72 0.44 -0.55 0.74 1.00 0.17 0.21 -0.57 ## am 0.60 -0.52 -0.59 -0.24 0.71 -0.69 -0.23 0.17 1.00 0.79 0.06 ## gear 0.48 -0.49 -0.56 -0.13 0.70 -0.58 -0.21 0.21 0.79 1.00 0.27 ## carb -0.55 0.53 0.39 0.75 -0.09 0.43 -0.66 -0.57 0.06 0.27 1.00 To isolate a single variables correlations, you can use code like the following. cor(mtcars$mpg,mtcars) 5.5 Formulas for Least Squares Regression In this section, we are going to approach least squares regression from a slightly different perspective from that in Section 5.3. If we determine that two variables are linearly correlated, we would like to find an equation \\(\\hat{y}=\\hat{b}_0+\\hat{b}_1x\\) that best fits the data. (The value \\(\\hat{y}\\) is the predicted or average value of \\(y\\) for a given \\(x\\) value.) Figure 5.10: Minimizing the residuals The goal is to somehow minimize the residuals \\(e_i=y_i-\\hat{y}_i.\\) (See Fig. 5.10.) In any best fit, some points will be above the line, and some will be below the line. Because of this, some residuals will be positive, and some will be negative. The least-squares solution minimizes the sum of the squares of the residuals so theres no cancellation. The Least Squares Problem Find coefficients \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) that minimize \\[S=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2,\\] where \\(\\hat{y}_i=\\hat{b}_0+\\hat{b}_1 x_i.\\) Note: this is a multivariable calculus problem. For a short introduction to, or review of, the material, consult Appendix A. The sum we are trying to minimize \\[S=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2=\\sum_{i=1}^n(y_i-\\hat{b}_0-\\hat{b}_1x_i)^2\\] is a function of two variables, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). We can minimize it by finding the critical point, i.e., the solution to \\[\\frac{\\partial S}{\\partial \\hat{b}_0}=0\\text{ and }\\frac{\\partial S}{\\partial \\hat{b}_1}=0.\\] Remember that the derivative of a sum is the sum of the derivatives, and that holds for partial differentiation, too. The value \\(S\\) is just a sum of a bunch of squared terms. With \\[S=\\sum_{i=1}^n(y_i-\\hat{b}_0-\\hat{b}_1x_i)^2,\\] \\[\\begin{equation} \\frac{\\partial S}{\\partial \\hat{b}_0}=\\sum_{i=1}^n 2(y_i-\\hat{b}_0-\\hat{b}_1x_i)(-1)=0, \\tag{5.6} \\end{equation}\\] and \\[\\begin{equation} \\frac{\\partial S}{\\partial \\hat{b}_1}=\\sum_{i=1}^n 2(y_i-\\hat{b}_0-\\hat{b}_1x_i)(-x_i)=0. \\tag{5.7} \\end{equation}\\] We can divide by \\(-2\\) and rearrange things. From (5.6), We have \\[\\sum_{i=1}^n (y_i-\\hat{b}_0-\\hat{b}_1x_i)=0\\Rightarrow \\sum_{i=1}^ny_i=\\sum_{i=1}^n \\hat{b}_0+\\sum_{i=1}^n\\hat{b}_1x_i\\] We can rewrite this as \\[n\\bar{y}=n\\hat{b}_0+\\hat{b}_1n\\bar{x}\\] or \\[\\begin{equation} \\boxed{\\hat{b}_0=\\bar{y}-\\hat{b}_1\\bar{x}.} \\tag{5.8} \\end{equation}\\] Now we need to find \\(\\hat{b}_1\\). Setting \\(\\partial S/\\partial \\hat{b}_1\\) (5.7) equal to zero and dividing by \\(-2\\) gives us \\[\\sum_{i=1}^n (y_i-\\hat{b}_0-\\hat{b}_1x_i)(x_i)=0\\Rightarrow \\sum_{i=1}^n x_iy_i=\\sum_{i=1}^n \\hat{b}_0x_i+\\sum_{i=1}^n\\hat{b}_1x_i^2,\\] or \\[\\hat{b}_1\\sum_{i=1}^n x_i^2=\\left(\\sum_{i=1}^nx_iy_i\\right)-\\hat{b}_0 n\\bar{x}=\\left(\\sum_{i=1}^nx_iy_i\\right)-(\\bar{y}-\\hat{b}_1\\bar{x})n\\bar{x}.\\] Collecting the \\(\\hat{b}_1\\) terms gives us \\[\\hat{b}_1\\left(\\left(\\sum_{i=1}^nx_i^2\\right)-n\\bar{x}^2\\right)=\\left(\\sum_{i=1}^nx_iy_i\\right)-n\\bar{x}\\bar{y}\\] or \\[\\begin{equation} \\boxed{\\hat{b}_1=\\frac{\\left(\\sum_{i=1}^nx_iy_i\\right)-n\\bar{x}\\bar{y}}{\\left(\\sum_{i=1}^nx_i^2\\right)-n\\bar{x}^2}=\\frac{S_{xy}}{S_{xx}},} \\tag{5.9} \\end{equation}\\] where \\(S_{xy}\\) and \\(S_{xx}\\) are defined in Equations (5.4) and (5.2), respectively. Lets summarize our results. The Least Squares Equation The linear least-squares equation is \\(\\hat{y}=\\hat{b}_0+\\hat{b}_1 x,\\) where \\(\\hat{b}_1=\\dfrac{S_{xy}}{S_{xx}},\\) and \\(\\hat{b}_0=\\bar{y}-\\hat{b}_1 \\bar{x}.\\) Example 5.7 First, lets check to make sure that our formula works for truly linear data. Here is the data from Example 5.5. \\(x_i\\) \\(y_i\\) \\(x_i^2\\) \\(y_i^2\\) \\(x_i y_i\\) 1 9 1 81 9 4 3 16 9 12 2 7 4 49 14 5 1 25 1 5 12 20 46 140 40 Again, \\(\\bar{x}=3, \\bar{y}=5, S_{xy}=-20,\\) and \\(S_{xx}=10.\\) That gives us \\(\\hat{b}_1=-20/10=-2\\) and \\(\\hat{b}_0=5-(-2)(3)=11.\\) Our equation is \\(\\hat{y}=11-2x.\\) A quick check reveals this to be the correct line. Example 5.8 The gravity points (GP)[16] and alcohol content by volume (ABV) for five different one-gallon batches of homebrewed India Pale Ales (IPA) appear below. With GP as the independent variable, find the least-squares regression equation. GP (\\(x_i\\)) ABV (\\(y_i\\)) \\(x_i^2\\) \\(x_i y_i\\) 68 7.42 4624 504.56 64 6.03 4096 385.92 57 5.58 3249 318.06 72 7.22 5184 519.84 54 5.34 2916 288.36 315 31.59 20069 2016.74 We dont need the \\(y_i^2\\) column for the regression coefficients, so its not in the table. We get \\(\\bar{x}=63,\\bar{y}=6.318,S_{xy}=2016.74-5*63*6.318=26.57,\\) and \\(S_{xx}=20069-5*63^2=224.\\) Thus, \\(\\hat{b}_1=26.57/224=0.119,\\) and \\(\\hat{b}_0=6.318-(26.57/224)*63=-1.155.\\) Our equation is \\[\\hat{y}=-1.155+0.119x\\] or \\[\\text{ABV}=-1.155+0.119\\text{GP}.\\] We can interpret both the slope and the intercept. The slope tells us the predicted change in the \\(y\\) value for each unit change in \\(x.\\) Here, the slope (\\(\\hat{b}_1\\)) is 0.119, so for each additional gravity point, the ABV is predicted to increase by \\(0.119\\) percentage points. The \\(y\\)-intercept (\\(\\hat{b}_0\\)) tells us the value of \\(y\\) when \\(x=0.\\) In this context, the negative \\(y\\)-intercept does not make sense. This formula will not work for low GP values. Generally speaking, these regression equations will not work well for observations outside the scope of the original data. (Here from 54 to 72 gravity points.) We can use our regression equation to make predictions. The equation predicts a beer with 68 gravity points will have an ABV2 of \\(\\hat{y}=-1.155+0.119*68=6.94\\%.\\) The actual ABV of our 68-GP beer was 7.42%. The residual for this observation is \\(e=y-\\hat{y}=7.42-6.94=0.48.\\) The model underestimated the actual ABV. Here is how we can find the coefficients in R. We use the lm function. The dependent variable (ABV) comes first. The summary function applied to the named regression output gives us a lot of information. GP &lt;- c(68,64,57,72,54) ABV&lt;-c(7.42,6.03,5.58,7.22,5.34) abvfit &lt;- lm(ABV~GP) summary(abvfit) ## ## Call: ## lm(formula = ABV ~ GP) ## ## Residuals: ## 1 2 3 4 5 ## 0.50892 -0.40662 -0.02630 -0.16554 0.08954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.15481 1.65838 -0.696 0.5363 ## GP 0.11862 0.02618 4.531 0.0201 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3918 on 3 degrees of freedom ## Multiple R-squared: 0.8725, Adjusted R-squared: 0.83 ## F-statistic: 20.53 on 1 and 3 DF, p-value: 0.02011 The coefficients appear in the Estimates column from the Coefficients table. The (Intercept) value is, unsurprisingly, \\(\\hat{b}_0\\), while the slope, \\(\\hat{b}_1\\), appears next to GP. When we have additional independent variables, they will each get their own coefficient. Note that the output also calculates the individual residuals. We will explain most of the additional information later. It is easy to add a regression line to a scatterplot in base R if weve named the lm output. The lwd = 2 argument results in a slightly thicker line (Fig. 5.11). plot(ABV~GP, pch = 19, col = &quot;red&quot;) abline(abvfit, col = &quot;blue&quot;, lwd = 2) Figure 5.11: Percent ABV as a function of Gravity Points Here we apply the lm function on the mtcars data frame, with weight as the independent variable and fuel economy as the dependent variable. mtfit &lt;- lm(mpg~wt, data = mtcars) summary(mtfit) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 The formula is \\(\\text{mpg}=37.2851-5.3445\\text{wt}.\\) The weight is given in 1000 pounds, so the formula says that for each additional 1000 pounds in weight, the predicted mpg decreases by 5.3445. We can also get the scatterplot (Fig. 5.12). plot(mpg~wt, data = mtcars, pch = 19, col = &quot;red&quot;) abline(mtfit, col = &quot;blue&quot;, lwd = 2) Figure 5.12: Miles per gallon as a function of weight in 1000 pounds The coefficient of determination The deviation of the \\(y\\) values from the mean can be broken down into two pieces: the part explained by the model (\\(\\hat{y}_i-\\bar{y}\\)), and the part that is not explained (\\(y_i-\\hat{y}_i=e_i\\)) (Fig. 5.13). Figure 5.13: The parts of the variation in \\(y\\) For each observation \\((x_i,y_i),\\) \\[y_i-\\bar{y}=(y_i-\\hat{y}_i)+(\\hat{y}_i-\\bar{y}).\\] Whats not obvious, but is true, is that if we square each term and add them up, we still have equality. \\[\\begin{equation} \\sum_{i=1}^n (y_i-\\bar{y})^2=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2+\\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2. \\tag{5.10} \\end{equation}\\] This can be written as \\(SST=SSE+SSR,\\) where \\(SST\\) is the total sum of squares, \\(SSE\\) is the sum of squares of the error (the residuals), and \\(SSR\\) is the sum of squares from the regression. Rearranging the equation gives us \\[SSR=SST-SSE,\\] and dividing both sides by \\(SST\\) gives us \\[R^2\\overset{\\mathrm{def}}{=}\\frac{SSR}{SST}=1-\\frac{SSE}{SST}.\\] The coefficient of determination, \\(R^2,\\) is the fraction of the total squared variation that comes from the regression. (I.e. the fraction that is explained.) We dont need to mess around with \\(SST\\) and the others because we have the following formula for \\(R^2.\\) \\[R^2=r^2\\] In simple linear regression , the coefficient of determination is simply Pearsons correlation coefficient squared. Example 5.9 We saw in Section 5.4 that the correlation between mpg and wt from the mtcars data frame is -0.8676594. That means that \\(r^2=0.753\\) or around 75% of the variance in mpg can be explained by the regression equation involving weight. The coefficient of determination shows up in the R summary as Multiple R-Squared. (We will see Adjusted R-Squared in the future.) ## ## Call: ## lm(formula = ABV ~ GP) ## ## Residuals: ## 1 2 3 4 5 ## 0.50892 -0.40662 -0.02630 -0.16554 0.08954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.15481 1.65838 -0.696 0.5363 ## GP 0.11862 0.02618 4.531 0.0201 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3918 on 3 degrees of freedom ## Multiple R-squared: 0.8725, Adjusted R-squared: 0.83 ## F-statistic: 20.53 on 1 and 3 DF, p-value: 0.02011 In the GP/ABV example, the coefficient of determination is 0.8725, so around 87.25% of the variance in ABV is explained by the equation. 5.6 Uncertainty in Least Squares In both the matrix approach and the calculus approach to least squares, we are minimizing the same thing: the sum of squares of the residuals. \\[\\begin{equation*} \\sum_{i=1}^n(y_i-\\hat{y}_i)^2=\\sum_{i=1}^n(y_i-(b_0+b_1x_i))^2 \\end{equation*}\\] A more statistical/probabilistic approach involves what is called a linear model. In this linear model, the underlying assumption is that the relationship between the \\(x\\) and \\(y\\) values is indeed linear, and any departure from linearity is due to randomness. The model is, for \\(i=1\\dots n\\), \\[y_i=b_0+b_1x_i+\\varepsilon_i,\\] where \\((x_i,y_i)\\) is the \\(i\\)th pair, and \\(\\varepsilon_i\\) is the error term. In order to estimate \\(b_0\\) and \\(b_1\\) using the observed \\((x_i,y_i)\\) pairs, we make some assumptions on the error terms [1]. Assumptions on the error terms In the simplest situation, the following assumptions are satisfied. The errors \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are random and independent. In particular, the magnitude of any error \\(\\varepsilon_i\\) does not influence the value of the next error. The errors \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) all have mean 0. The errors \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) all have the same variance, which we denote by \\(\\sigma^2\\). The errors \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) are normally distributed. With the assumptions in place, we can find what are called maximum likelihood estimators, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), of the true intercept and slope, \\(b_0\\) and \\(b_1\\), respectively, based on the observed data. It turns out that the normality assumption on the errors leads to the same estimates \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) as the linear algebra and calculus approaches! One benefit of the linear model approach is that it allows us to estimate the uncertainty in the coefficients and in predictions based on the estimates. Lets revisit the output from our attempt to write mpg as a function of wt using the mtcars data frame. fit &lt;- lm(mpg~wt, data = mtcars) summary(fit)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.285126 1.877627 19.857575 8.241799e-19 ## wt -5.344472 0.559101 -9.559044 1.293959e-10 The standard error (Std. Error) of the coefficients is related to how much uncertainty there is in them. Consult a statistics textbook [1,9] for a detailed explanation of how to calculate them. When you divide the coefficient estimates by their standard errors, the resulting value follows what is known as Students t distribution with \\(n-2\\) degrees of freedom, where \\(n\\) is the number of pairs in the data. In the mtcars data frame, there are 32 cars, so there are 30 degrees of freedom. The value in the t value column is this \\(t\\)-distribution number. One question to ask is, Are the slope and intercept significantly different from 0? The values in the Pr(&gt;|t|) column tell us the probability of seeing values at least as extreme as they are if they were truly zero. These probabilities are called P-values. In both cases, it is extremely unlikely that they are zero. If the slope were not significantly different from 0, then the independent variable wouldnt have any significant effect on the dependent variable. We commonly consider \\(P\\)-values below 0.05 to be significant. You can also use the standard error values to find confidence interval estimates for the true values of \\(b_0\\) and \\(b_1\\). The R function confint applied to the lm output will produce these intervals. confint(fit, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 33.450500 41.119753 ## wt -6.486308 -4.202635 The numbers in the 2.5 % column are the lower bounds for 95% confidence intervals for the intercept and slope (wt). The numbers in the 97.5 % column are the upper bounds. Note that 95% (level = 0.95) is the default setting and doesnt need to be there if you are happy with that level. Confidence and prediction intervals for responses With the linear model approach, the value \\[\\hat{y}=\\hat{b}_0+\\hat{b}_1x\\] can be thought of as the estimated mean output for a given input \\(x\\). We can construct a confidence interval for the true mean response. We can also produce what is called a prediction interval: given an input \\(x\\), what is the likely range of \\(y\\) values? The predict function applied to the output of the lm function can calculate both of these intervals. To use it, we need to create a data frame with new values. (The interval setting will accept abbreviations of confidence and prediction.) predict(fit, newdata = data.frame(wt = 2), interval = &quot;conf&quot;) ## fit lwr upr ## 1 26.59618 24.82389 28.36848 predict(fit, newdata = data.frame(wt = 2), interval = &quot;predict&quot;) ## fit lwr upr ## 1 26.59618 20.12811 33.06425 These are 95% confidence and prediction intervals, respectively, for a 2000-lb vehicle (wt = 2). The prediction interval is much wider, since its giving a range of all likely values and not just the mean. The value under fit is what you get when you plug wt = 2 into the regression formula. It is fairly common to see 95% confidence intervals for the mean response included in regression plots. Here is how to get it using the ggplot2 package (5.14). library(ggplot2) mtplot &lt;- ggplot(mtcars, aes(wt,mpg)) + geom_point() + geom_smooth(method = &#39;lm&#39;) mtplot Figure 5.14: A regression plot with confidence interval It takes a little more work to include a prediction interval. Figure 5.15 shows the 95% confidence interval (gray shaded region) and 95% prediction interval (dashed) from the GP/ABV data in Example 5.8. Figure 5.15: Confidence and prediction intervals Checking assumptions A residual plot is a plot of residuals in a linear fit versus either the \\(x\\) values or the predicted \\(y\\) values. It can be used to see if the equal variance assumption is met. It can also display any nonlinear trend in the data. In Figure 5.16, the linear fit on the left looks pretty good, but the residual plot on the right reveals a distinct nonlinear trend. Figure 5.16: A nonlinear trend In Figure 5.17, the residual plot on the right shows that the residuals tend to spread out as \\(y\\) increases. This indicates that the equal variance assumption is not true. Figure 5.17: Increasing residuals In Figure 5.18, there is no obvious nonlinear trend and they seem to be fairly uniform across the board. Figure 5.18: A good looking residual plot If you apply the plot function to the output from lm, you get a sequence of diagnostic plots. The first is the residual plot. par(mfrow = c(2,2)) plot(fit) Figure 5.19: Diagnostic plots See www.statology.org [17] for a detailed discussion of all four plots. 5.7 Multilinear Regression In the real world, many quantities depend on more than one independent variable. For instance, the fuel efficiency of a car might depend on its weight and on its horsepower. If the relationship is linear, we can use multilinear regression to investigate it. We saw a short example in Section 5.3. Now we will develop the idea in a little more depth. Suppose that a quantity \\(z\\) is a linear function of \\(x\\) and \\(y\\), i.e. \\[z=b_0+b_1x+b_2y.\\] Further suppose that we have a bunch of \\((x,y,z)\\) data: \\[(x_1,y_1,z_1),(x_2,y_2,z_2),\\dots,(x_n,y_n,z_n).\\] Wed like to use this data to find the coefficients that best fit the data. We already have the tools to do this in the least squares sense. Our data gives us \\(n\\) equations \\[\\begin{align*} b_0+b_1x_1+b_2y_1&amp;=z_1\\\\ b_0+b_1x_2+b_2y_2&amp;=z_2\\\\ &amp;\\vdots\\\\ b_0+b_1x_n+b_2y_n&amp;=z_n. \\end{align*}\\] We can write this system in matrix form \\[\\begin{bmatrix}1 &amp; x_1 &amp; y_1\\\\1 &amp; x_2 &amp; y_2\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; y_n\\end{bmatrix}\\begin{bmatrix}b_0\\\\b_1\\\\b_2\\end{bmatrix}=\\begin{bmatrix}z_1\\\\z_2\\\\ \\vdots \\\\ z_n\\end{bmatrix}\\] or \\[\\mathbf{X}\\mathbf{b}=\\mathbf{z}.\\] Usually the system wont have a solution, but we can find a least squares approximation using the normal equations: \\[\\mathbf{X}^T\\mathbf{X}\\mathbf{b}=\\mathbf{X}^T\\mathbf{z},\\] which will have a solution \\(\\hat{\\mathbf{b}}\\). It will be unique if the columns of \\(\\mathbf{X}\\) are independent. Example 5.10 Lets find the least squares equation \\(\\hat{z}=\\hat{b}_0+\\hat{b}_1x+\\hat{b}_2 y\\) that best fits the points in \\((x,y,z)\\) form: \\((1,1,1),(2,0,2),(-1,-1,1),\\) and \\((1,1,3).\\) The matrix equation is \\[\\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\mathbf{b}= \\begin{bmatrix} 1\\\\2\\\\1\\\\3 \\end{bmatrix}.\\] The first column of \\(\\mathbf{X}\\) is all 1s, the second column is the \\(x\\) values, and the third is the \\(y\\) values. The normal equation(s) are \\[\\begin{bmatrix} 4 &amp; 3 &amp; 1\\\\3 &amp; 7 &amp; 3\\\\1 &amp; 3 &amp; 3 \\end{bmatrix}\\mathbf{b}=\\begin{bmatrix} 7\\\\7\\\\3 \\end{bmatrix} .\\] The least squares solution is \\(\\hat{\\mathbf{b}}=(\\mathbf{X}^T\\mathbf{X})^{-1}(\\mathbf{X}^T\\mathbf{z})=\\begin{bmatrix} 3/2\\\\1/4\\\\1/4 \\end{bmatrix}\\), so the equation is \\[\\hat{z}=\\frac{3}{2}+\\frac{1}{4}x+\\frac{1}{4}y.\\] The beauty of the linear algebra approach is that we can use as many independent variables as we want and the theory stays the same. The matrices involved get bigger, and its harder to do by hand, but thats what computers are for. R and multilinear regression Example 5.11 Lets use R to find a model for mpg as a linear function of wt and hp from the mtcars data frame. We again use the lm function, with a plus sign separating the independent variables. fit2 &lt;- lm(mpg~wt+hp, data = mtcars) summary(fit2) ## ## Call: ## lm(formula = mpg ~ wt + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.941 -1.600 -0.182 1.050 5.854 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.22727 1.59879 23.285 &lt; 2e-16 *** ## wt -3.87783 0.63273 -6.129 1.12e-06 *** ## hp -0.03177 0.00903 -3.519 0.00145 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.593 on 29 degrees of freedom ## Multiple R-squared: 0.8268, Adjusted R-squared: 0.8148 ## F-statistic: 69.21 on 2 and 29 DF, p-value: 9.109e-12 The coefficients appear in the Estimate column. Our model is \\[\\texttt{mpg}=37.227-3.878\\texttt{wt}-0.032\\texttt{hp}.\\] We can use the model to estimate the mpg for various wt-hp pairs. For instance, a 2500-lb car with a 175-hp engine would on average get \\[\\texttt{mpg}=37.227-3.878\\cdot 2.5-0.032\\cdot 175=21.932 \\text{ miles per gallon}.\\] We can also use the predict function. predict(fit2, newdata = data.frame(wt = 2.5, hp = 175)) ## 1 ## 21.97243 The discrepancies between the two answers are due to the rounding of the coefficients. What about the rest of the output? The intercept, the wt coefficient, and the hp coefficient are all significantly different from 0. Note that we lose an additional degree of freedom when we add a variable. The model from last section, that only had weight as a predictor, had a coefficient of determination of 0.7528. Now its 0.8268. It will always increase when additional variables are added. The adjusted \\(R^2\\) of 0.8148 is the appropriate value for comparing multiple regression models. (Well talk about Adjusted \\(R^2\\) in the next section.) In simple linear regression, the \\(P\\)-value associated with the \\(F\\) statistic is the same as the \\(P\\)-value for the slope. That changes in multilinear regression. Were not going to get into the details of the \\(F\\) statistic here, but what were looking for is a \\(P\\)-value of less than \\(0.05\\). Otherwise, theres basically no linear relationship. Example 5.12 We can add all of the other variables into the mix by using a period after the tilde. fitall &lt;- lm(mpg~., data = mtcars) summary(fitall) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs 0.31776 2.10451 0.151 0.8814 ## am 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## carb -0.19942 0.82875 -0.241 0.8122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 But maybe we shouldnt. Only the wt coefficient is significantly different from 0. Well talk about model selection next section. Indicator variables We can include categorical (non-numeric) variables in linear regression through the use of indicator variables. An indicator variable takes on the value 1 if the categorical variable takes on a specific value and a 0 otherwise. For instance, in the mtcars data frame, the am (1 for manual, 0 for automatic transmission) and vs (0 for V-shaped engine, 1 for straight) variables are actually indicator variables. Example 5.13 If we find a linear model for fuel efficiency as a function of horsepower and transmission type, we get the following. fitind &lt;- lm(mpg~hp+am, data = mtcars) summary(fitind) ## ## Call: ## lm(formula = mpg ~ hp + am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3843 -2.2642 0.1366 1.6968 5.8657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.584914 1.425094 18.655 &lt; 2e-16 *** ## hp -0.058888 0.007857 -7.495 2.92e-08 *** ## am 5.277085 1.079541 4.888 3.46e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.909 on 29 degrees of freedom ## Multiple R-squared: 0.782, Adjusted R-squared: 0.767 ## F-statistic: 52.02 on 2 and 29 DF, p-value: 2.55e-10 The model is \\[\\texttt{mpg}=26.585-0.059\\texttt{hp}+5.277\\texttt{am}.\\] Basically, the presence of a manual transmission adds 5.277 miles per gallon to the estimated fuel efficiency. We get two regression lines: one for automatics and one for manuals (Fig. 5.20). Figure 5.20: A model with an indicator term Many categorical variables take on more than two different values or levels. In that case, a different indicator variable is included for each level, except one. For instance, if the season is an important variable, there could be one indicator variable for Spring (1 if Spring, 0 otherwise), one for Summer, and one for Autumn. The final season, Winter doesnt get one, because that would make the variables dependent. Winter would be the baseline value, to which all others are compared. Example 5.14 Here is a fit of the population of a state as a function of the states area and its region. Region is a categorical variable. The Northeast region is treated as the baseline. The other regions get their own indicators. state &lt;- data.frame(state.x77) state$Region &lt;- state.region fitstate &lt;- lm(Population~Area+Region, data = state) summary(fitstate) ## ## Call: ## lm(formula = Population ~ Area + Region, data = state) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5952.2 -2226.6 -858.2 713.2 18110.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.352e+03 1.514e+03 3.534 0.000959 *** ## Area 7.875e-03 8.631e-03 0.912 0.366383 ## RegionSouth -1.574e+03 1.909e+03 -0.825 0.413931 ## RegionNorth Central -1.043e+03 2.029e+03 -0.514 0.609901 ## RegionWest -3.496e+03 2.202e+03 -1.588 0.119327 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4519 on 45 degrees of freedom ## Multiple R-squared: 0.05919, Adjusted R-squared: -0.02444 ## F-statistic: 0.7078 on 4 and 45 DF, p-value: 0.5908 Note how R pastes the level of the variable onto the variable name in the output. Also note the terrible coefficient of determination. This is not a good model. Fitting polynomials We can actually use linear regression to fit polynomial functions to data. For instance, what if we want to find the best fitting quadratic function \\[y=b_0+b_1x+b_2x^2\\] for a set of data? You may recall that three points (with different \\(x\\) values) will determine a quadratic. What if we have more than 3 points? Example 5.15 Lets find the least squares best fitting quadratic that goes through the points \\((1,3),(2,5),(3,6),\\) and \\((4,7)\\). The linear system of equations for \\(y=b_0+b_1x+b_2x^2\\) with this data would be \\[\\begin{align*} b_0+1b_1+1^2b_2&amp;=3\\\\ b_0+2b_1+2^2b_2&amp;=5\\\\ b_0+3b_1+3^2b_2&amp;=6\\\\ b_0+4b_1+4^2b_2&amp;=7 \\end{align*}\\] or in matrix form \\[\\begin{bmatrix}1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; 4\\\\1 &amp; 3 &amp; 9\\\\1 &amp; 4 &amp; 16\\end{bmatrix}\\begin{bmatrix}b_0\\\\b_1\\\\b_2\\end{bmatrix}=\\begin{bmatrix}3\\\\5\\\\6\\\\7\\end{bmatrix}.\\] Solving the normal equations gives us \\[\\hat{y}=\\frac{3}{4}+\\frac{51}{20}x-\\frac{1}{4}x^2.\\] To do this in R, we need to be a little careful. x &lt;- c(1,2,3,4) y &lt;- c(3,5,6,7) fitq &lt;- lm(y~x+I(x^2)) summary(fitq) ## ## Call: ## lm(formula = y ~ x + I(x^2)) ## ## Residuals: ## 1 2 3 4 ## -0.05 0.15 -0.15 0.05 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7500 0.6225 1.205 0.441 ## x 2.5500 0.5679 4.490 0.140 ## I(x^2) -0.2500 0.1118 -2.236 0.268 ## ## Residual standard error: 0.2236 on 1 degrees of freedom ## Multiple R-squared: 0.9943, Adjusted R-squared: 0.9829 ## F-statistic: 87 on 2 and 1 DF, p-value: 0.07559 To use powers of a variable in a model in R, we need to wrap them in the I function. See the fit in Figure 5.21. Figure 5.21: A quadratic fit using linear regression In the last example, our design matrix looked like \\[\\mathbf{X}=\\begin{bmatrix} 1&amp; x_1 &amp; x_1^2 \\\\1 &amp; x_2 &amp; x_2^2\\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2\\end{bmatrix}.\\] We actually had functions of \\(x\\) in the columns. We can do this for any function. For instance, if we wanted a fit of the form \\[y=b_0 +b_1\\sin x+b_2\\cos x,\\] the design matrix would look like \\[\\mathbf{X}=\\begin{bmatrix}1 &amp; \\sin x_1 &amp; \\cos x_1\\\\1 &amp; \\sin x_2 &amp; \\cos x_2 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; \\sin x_n &amp; \\cos x_n\\end{bmatrix}.\\] Interactions Lets say you want to model an individuals response to a medication. The response is going to depend on a number of things, including the dosage and the persons weight. In our previous models, the response to a change in one variable wasnt dependent on the value of the other. Its not hard to imagine, however, that the response to a change in dosage will depend on the persons weight. There is an interaction between these two variables. The simplest way to include an interaction between variables \\(x_1\\) and \\(x_2\\) is to include the product \\(x_1 x_2\\) as a separate independent variable. We then have the model \\[y=b_0+b_1 x_1+b_2 x_2+b_3 x_1 x_2.\\] Adding this new term doesnt really complicate things at all. Example 5.16 The price of a diamond in U.S. Dollars is a function of many things, including its weight in carats and its depth percentage. The ggplot2 package contains a data set with this information. library(ggplot2) diamondfit &lt;- lm(price~carat*depth, data = diamonds) summary(diamondfit) ## ## Call: ## lm(formula = price ~ carat * depth, data = diamonds) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15039.2 -799.3 -18.1 539.6 12666.5 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7823.738 592.049 -13.215 &lt;2e-16 *** ## carat 20742.600 567.672 36.540 &lt;2e-16 *** ## depth 90.043 9.588 9.391 &lt;2e-16 *** ## carat:depth -210.075 9.187 -22.868 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1534 on 53936 degrees of freedom ## Multiple R-squared: 0.8521, Adjusted R-squared: 0.8521 ## F-statistic: 1.036e+05 on 3 and 53936 DF, p-value: &lt; 2.2e-16 The interaction term is carat:depth. (If you only want the interaction term and not the individual variables, use the colon instead of the asterisk.) Our model with interaction term is \\[\\texttt{price}=-7824+20743\\texttt{carat}+90\\texttt{depth}-210(\\texttt{carat})(\\texttt{depth}).\\] A diamond with a depth percentage of 60 would have a price function of \\[\\begin{align*} \\texttt{price}&amp;=-7824+20743\\texttt{carat}+(90)(60)-(210)(60)\\texttt{carat}\\\\ &amp;=(-7824+5400)+(20743-12600)\\texttt{carat}\\\\ &amp;=-2424+8143\\texttt{carat}. \\end{align*}\\] Example 5.17 When we included the am variable in our earlier mpg versus hp model, this had the effect of creating two models with the same slope and different intercepts. When we include an interaction term, we get something different. For this example, we will be using wt and am as the interacting terms. fitam &lt;- lm(mpg~wt*am, data = mtcars) summary(fitam) ## ## Call: ## lm(formula = mpg ~ wt * am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6004 -1.5446 -0.5325 0.9012 6.0909 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.4161 3.0201 10.402 4.00e-11 *** ## wt -3.7859 0.7856 -4.819 4.55e-05 *** ## am 14.8784 4.2640 3.489 0.00162 ** ## wt:am -5.2984 1.4447 -3.667 0.00102 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.591 on 28 degrees of freedom ## Multiple R-squared: 0.833, Adjusted R-squared: 0.8151 ## F-statistic: 46.57 on 3 and 28 DF, p-value: 5.209e-11 The overall model is \\[\\texttt{mpg}=31.416-3.786\\texttt{wt}+14.878\\texttt{am}-5.298\\texttt{wt}\\cdot \\texttt{am}.\\] We basically have two different models. For automatics, we set \\(\\texttt{am}=0\\) and get \\[\\texttt{mpg}=31.416-3.786\\texttt{wt}.\\] Changing am to 1 will change the intercept and the slope. \\[\\begin{align*} \\texttt{mpg}&amp;=31.416-3.786\\texttt{wt}+14.878-5.298\\texttt{wt} \\\\ &amp;=46.294-9.084\\texttt{wt}. \\end{align*}\\] When we use the indicator variable (coerced into a factor variable) as the color in ggplot, we get the two separate lines (Fig. 5.22). # Change the am variable to a factor variable for ggplot2 mt2&lt;-mtcars mt2$am &lt;- as.factor(mtcars$am) ginteract &lt;- ggplot(mt2, aes(mpg, wt, color = am))+ geom_point()+ geom_smooth(method= &quot;lm&quot;) ginteract Figure 5.22: Model with a categorical variable in an interaction 5.8 Model Selection Given a data set with many independent variables, it is tempting to choose the model that uses all of them. This might not be wise, however. Some of the variables might have nothing to do with the response. Some variables might be so closely related to others that they add nothing to the model. Some statisticians believe in the principle of parsimony [1]. Principle of Parsimony A model should contain the smallest number of variables necessary to fit the data. We have already observed that the value of \\(R^2\\) increases as the number of independent variables increases. Using \\(R^2\\) as a criterion for choosing the best model would therefore be in conflict with the principle of parsimony. For this reason, among others, we consider the adjusted \\(R^2\\). Definition 5.1 \\[\\text{Adjusted } R^2=R^2-\\left(\\frac{k}{n-k-1}\\right)(1-R^2), \\] where \\(n\\) is the number of observations and \\(k\\) is the number of independent variables in the model. Since were always subtracting a positive amount from \\(R^2\\), the adjusted \\(R^2\\) value is always less than \\(R^2\\), though if \\(n\\) is large, the difference can be negligible. In earlier sections, we developed models using the mtcars data frame for the fuel efficiency (mpg) of a car as a function of several variables. With only the variable wt, we had \\(R^2=0.7528\\). With \\(n=32\\) and \\(k=1\\), \\[\\text{Adjusted } R^2=0.7528-\\frac{1}{30}(1-0.7528)=0.74456.\\] When the variables were horsepower and weight, we had \\(R^2=0.8268\\). With \\(k=2\\), we have \\[\\text{Adjusted }R^2=0.8268-\\frac{2}{29}(1-0.8268)=0.8149,\\] an improvement over the single variable model. Adding displacement to the model barely changed \\(R^2\\), as its still 0.8268 to 4 decimal places, but with \\(k=3\\), we have \\[ \\text{Adjusted } R^2=0.8268-\\frac{3}{28}(1-0.8268)=0.8082,\\] a decrease from our two-variable model. There are other ways of comparing the qualities of multilinear models, including AIC (Akaike information criterion) [18], and BIC (Bayesian information criterion) [19]. Having chosen a criterion, there are different ways of identifying the best model. Best subsets regression The best subsets regression procedure simply compares all possible models with a given number of independent variables and selects the best one. Depending on the total number of independent variables available, this can be an inefficient procedure. The regsubsets function from the R package leaps handles best subsets regression. Example 5.18 In this example, we use best subsets on the swiss data frame to find the best model to predict Fertility. library(leaps) fitbest &lt;- regsubsets(Fertility~., data = swiss) summary(fitbest)$outmat ## Agriculture Examination Education Catholic Infant.Mortality ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; The best one-variable model uses Education as the independent variable. The best two-variable model uses Education and Catholic. To find the best overall model, you can apply the plot function to the regsubsets output (Fig. 5.23). The best model appears at the top. In this example, the model that includes every variable except Examination is the best. The default selection statistic is BIC. For BIC, the lower the better. plot(fitbest) Figure 5.23: A best subsets plot Stepwise regression A somewhat more efficient method for finding the best model is stepwise regression. In stepwise regression, you either start with the empty model or full model. If you start with the empty model, here is a summary of the forwards regression process. Find the single variable that produces the best model. Find the variable that, when added to the first, improves the model the most. Repeat, until no additions will improve the situation. Note: it is possible that after the addition of one variable, the removal of one of the already included variables will improve the model. You can include this if desired. Backwards regression starts with the full model and eliminates variables one-by-one. It is a little easier to perform in R. The step function is the function to use. Example 5.19 For our stepwise regression example, we again use the swiss data frame. fitall &lt;- lm(Fertility~., data = swiss) fitstep &lt;- step(fitall) ## Start: AIC=190.69 ## Fertility ~ Agriculture + Examination + Education + Catholic + ## Infant.Mortality ## ## Df Sum of Sq RSS AIC ## - Examination 1 53.03 2158.1 189.86 ## &lt;none&gt; 2105.0 190.69 ## - Agriculture 1 307.72 2412.8 195.10 ## - Infant.Mortality 1 408.75 2513.8 197.03 ## - Catholic 1 447.71 2552.8 197.75 ## - Education 1 1162.56 3267.6 209.36 ## ## Step: AIC=189.86 ## Fertility ~ Agriculture + Education + Catholic + Infant.Mortality ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2158.1 189.86 ## - Agriculture 1 264.18 2422.2 193.29 ## - Infant.Mortality 1 409.81 2567.9 196.03 ## - Catholic 1 956.57 3114.6 205.10 ## - Education 1 2249.97 4408.0 221.43 The output shows all steps in the process. The tables show the effect of removing each variable. Removing any variable above the &lt;none&gt; line improves the model (based on AIC). The variable at the top (if it is above &lt;none&gt;) goes. In this example, Examination gets dropped first. After that, the process stops, because the removal of any of the other variables will make the model worse. The step output includes the final model, which is handy. summary(fitstep) ## ## Call: ## lm(formula = Fertility ~ Agriculture + Education + Catholic + ## Infant.Mortality, data = swiss) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.6765 -6.0522 0.7514 3.1664 16.1422 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.10131 9.60489 6.466 8.49e-08 *** ## Agriculture -0.15462 0.06819 -2.267 0.02857 * ## Education -0.98026 0.14814 -6.617 5.14e-08 *** ## Catholic 0.12467 0.02889 4.315 9.50e-05 *** ## Infant.Mortality 1.07844 0.38187 2.824 0.00722 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.168 on 42 degrees of freedom ## Multiple R-squared: 0.6993, Adjusted R-squared: 0.6707 ## F-statistic: 24.42 on 4 and 42 DF, p-value: 1.717e-10 Here is the process for forwards regression. In addition to the full model, we need to create an intercept-only model. fitio &lt;- lm(Fertility~1, data = swiss) fitallf &lt;- formula(fitall) fitstepf &lt;- step(fitio, direction = &quot;forward&quot;, scope=fitallf) ## Start: AIC=238.35 ## Fertility ~ 1 ## ## Df Sum of Sq RSS AIC ## + Education 1 3162.7 4015.2 213.04 ## + Examination 1 2994.4 4183.6 214.97 ## + Catholic 1 1543.3 5634.7 228.97 ## + Infant.Mortality 1 1245.5 5932.4 231.39 ## + Agriculture 1 894.8 6283.1 234.09 ## &lt;none&gt; 7178.0 238.34 ## ## Step: AIC=213.04 ## Fertility ~ Education ## ## Df Sum of Sq RSS AIC ## + Catholic 1 961.07 3054.2 202.18 ## + Infant.Mortality 1 891.25 3124.0 203.25 ## + Examination 1 465.63 3549.6 209.25 ## &lt;none&gt; 4015.2 213.04 ## + Agriculture 1 61.97 3953.3 214.31 ## ## Step: AIC=202.18 ## Fertility ~ Education + Catholic ## ## Df Sum of Sq RSS AIC ## + Infant.Mortality 1 631.92 2422.2 193.29 ## + Agriculture 1 486.28 2567.9 196.03 ## &lt;none&gt; 3054.2 202.18 ## + Examination 1 2.46 3051.7 204.15 ## ## Step: AIC=193.29 ## Fertility ~ Education + Catholic + Infant.Mortality ## ## Df Sum of Sq RSS AIC ## + Agriculture 1 264.176 2158.1 189.86 ## &lt;none&gt; 2422.2 193.29 ## + Examination 1 9.486 2412.8 195.10 ## ## Step: AIC=189.86 ## Fertility ~ Education + Catholic + Infant.Mortality + Agriculture ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2158.1 189.86 ## + Examination 1 53.027 2105.0 190.69 The output is similar. Notice the plus sign in front of the variables instead of minus signs. In this process, the variable at the top is added. Replacing forward with both allows for potential additions and subtractions. Some warnings Before we undertake any model selection, we should consider what we are trying to accomplish. Are we trying to gain some understanding of the underlying relationships between variables? Note that adding and removing variables will change the coefficients in the model, unless those variables are completely uncorrelated with the other variables. Are we trying to make predictions with our models? Oftentimes, using too many variables can lead to overfitting, which can lead to bad predictions. 5.9 Logistic Regression Logistic regression is frequently used to make predictions about an objects classification based upon numerical measures. Here are some examples: Can you use the number of typos in an email to tell whether or not it is spam? Can you use a baseball players college batting average to determine whether or not they will make it to the Major Leagues? Even if we cant offer a guaranteed yes or no to these questions, we can at least try to give a probability that it is one thing or another. Lets start with a motivating example. The Pima.tr data frame in R contains diabetes-related data obtained from a sample of Akimel Oodham, or Pima, women. If we create a boxplot of blood glucose levels grouped by diabetes status (Fig. 5.24), we see that those with diabetes tend to have a higher glucose level. library(ggplot2) Pima &lt;- MASS::Pima.tr pimabox &lt;- ggplot(Pima, aes(glu, fill = type))+geom_boxplot() pimabox Figure 5.24: Blood glucose levels grouped by diabetes status Can we use the glucose level to estimate the probability that an individual has diabetes? That is what logistic regression tries to do. In Figure 5.25, the types No and Yes are recoded as 0 and 1. If we try to fit a straight line to the data, some bad things happen. First, its not a very good fit, and second, if we try to use the model to estimate probabilities, we will get some negative probabilities and some that are greater than one. Figure 5.25: A straight line fit to 0-1 data If we instead try to fit a s-shaped logistic curve to the data, we get the graph in Figure 5.26. The function only takes on values between 0 and 1, so we can at least attempt to use it to estimate probabilities. Figure 5.26: A logistic fit So, how do we find the logistic curve? To get to the logistic model, we start with the odds. If \\(p\\) is the probability of an event, we define the odds to be \\[\\text{odds}=\\frac{p}{1-p}.\\] Note that these are the odds in favor of the event. In gambling, people usually give the odds against the event, which is just the reciprocal of these odds. Probabilities are between 0 and 1, whereas odds can be any non-negative number, unless \\(p=1.\\) In that case, the odds are undefined. The last step before regression involves taking the (natural) log of the odds. This gives us the logit function. \\[\\text{logit}(p)=\\ln\\left(\\frac{p}{1-p}\\right).\\] The logit function has a range of \\((-\\infty,\\infty).\\) We want to fit a model of the form \\[\\text{logit}(p)=b_0+b_1 x,\\] but there is a little problem. In our data, the values of \\(p\\) we have are 0 (type = No) and 1 (type = Yes). The logit function is undefined for both of those values. Long story short, we cant use our linear regression formulas to find \\(b_0\\) and \\(b_1.\\) Software packages use algorithmic methods, such as gradient descent, to find maximum likelihood estimates for the coefficients. Lets say we have the coefficients, \\(b_0\\) and \\(b_1.\\) If we solve \\[ \\text{logit}(p)=\\ln\\frac{p}{1-p}=b_0+b_1x\\] for the odds (\\(p/(1-p)\\)), we get \\[\\text{odds}=e^{b_0+b_1 x}=e^{b_0}({e^{b_1}})^{ x}.\\] The value \\(e^{b_1}\\) is the odds ratio. If \\(x\\) increases by one unit, the odds will be multiplied by \\(e^{b_1}\\). If we solve the odds equation for \\(p\\), we get \\[\\begin{equation} p=\\frac{e^{b_0+b_1x}}{1+e^{b_0+b_1x}}=\\frac{1}{e^{-(b_0+b_1x)}+1},\\tag{5.11} \\end{equation}\\] which is our logistic function. Lets use R to find our coefficients. We use the glm function with family=\"binomial\". (When a function of our response variable is a linear function of independent variables, we are in the category of generalized linear models.) We dont have to recode the Yes and No values. logfit &lt;- glm(type~glu, data = Pima, family = &quot;binomial&quot;) summary(logfit) ## ## Call: ## glm(formula = type ~ glu, family = &quot;binomial&quot;, data = Pima) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.503636 0.836077 -6.583 4.62e-11 *** ## glu 0.037784 0.006278 6.019 1.76e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 256.41 on 199 degrees of freedom ## Residual deviance: 207.37 on 198 degrees of freedom ## AIC: 211.37 ## ## Number of Fisher Scoring iterations: 4 If we exponentiate the coefficients, we get the following. exp(logfit$coefficients) ## (Intercept) glu ## 0.00407194 1.03850660 Our odds ratio is around 1.0385, so each additional unit increase in the glucose value leads to an estimated 3.85% increase in the odds of having diabetes. The effect on the probability is not constant. The logit function (see Figure 5.26 for the graph) is \\[p=\\frac{e^{-5.5036+0.0378\\text{glu}}}{1+e^{-5.5036+0.0378\\text{glu}}}.\\] The predict function works with glu output. To get probabilities, we need to set type = \"response\". predict(logfit, newdata = data.frame(glu = c(100,200)), type = &quot;response&quot;) ## 1 2 ## 0.1511944 0.8862613 The predicted probability that a woman with a glucose value of 100 has diabetes is 0.151, while a woman with a value of 200 has a probability of 0.886. What about the other output? The standard error, \\(z\\)-value, and \\(p\\)-value have similar interpretations as in the linear regression case. Both coefficients are significantly different from zero. We will talk about the null and residual deviances a little later. Logistic regression with multiple independent variables As is the case with linear regression, we can add independent variables and interactions. Lets add age as a second independent variable in our diabetes model. logfit2 &lt;- glm(type~glu+age, data = Pima, family = &quot;binomial&quot;) summary(logfit2) ## ## Call: ## glm(formula = type ~ glu + age, family = &quot;binomial&quot;, data = Pima) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.590597 0.950274 -6.935 4.05e-12 *** ## glu 0.032860 0.006375 5.155 2.54e-07 *** ## age 0.052295 0.016796 3.114 0.00185 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 256.41 on 199 degrees of freedom ## Residual deviance: 197.11 on 197 degrees of freedom ## AIC: 203.11 ## ## Number of Fisher Scoring iterations: 4 All coefficients are significantly different from zero. When we exponentiate those coefficients, we get the odds ratios for both glucose and age. exp(logfit2$coefficients) ## (Intercept) glu age ## 0.00137322 1.03340625 1.05368617 Again, we can make predictions. pdata &lt;- data.frame(glu = c(100,200), age = c(20,30)) predict(logfit2, newdata = pdata, type = &quot;response&quot;) ## 1 2 ## 0.09460463 0.82495991 Likelihood and deviance The diagnostic output includes null and residual deviances. Lets talk about those. First, we need to talk about the likelihood. Suppose our \\(i\\)th observation is given by a pair \\((x_i,y_i),\\) where \\(y_i\\) is either 0 or 1. When we create our logistic model, we get an estimated probability \\(\\hat{p}_i\\) that \\(y=1\\) given that \\(x=x_i\\). The likelihood of this particular observation is then \\(\\hat{p}_i\\) if \\(y_i=1\\) or \\(1-\\hat{p}_i\\) if \\(y_i=0.\\) We can combine this likelihood into a single product: \\[\\hat{p}_i^{y_i}(1-\\hat{p}_i)^{1-y_i}.\\] If \\(y_i=1,\\) then the \\((1-\\hat{p}_i)\\) exponent is 0, and similarly, when \\(y_i=0,\\) the \\(\\hat{p}_i\\) exponent is 0. We assume that individual observations are independent of each other, which allows us to get a likelihood function for the whole data set by multiplying the individual likelihoods. (Assume we have \\(n\\) observations.) This is a function of the coefficients \\(b_0\\) and \\(b_1\\) \\[L(b_0,b_1)=\\prod_{i=1}^n \\hat{p}_i^{y_i}(1-\\hat{p}_i)^{1-y_i},\\] where \\[p_i=\\frac{e^{b_0+b_1x_i}}{1+e^{b_0+b_1x_i}}.\\] The \\(\\Pi\\) represents a product of multiple terms much like a \\(\\Sigma\\) represnts a sum of multiple terms. The maximum likelihood estimates are the values of \\(b_0\\) and \\(b_1\\) that maximize this function. It is often easier in practice to maximize the natural logarithm of the likelihood function. Remember that the log of a product is the sum of the logs, so we have \\[\\ell=\\ln L=\\sum_{i=1}^n\\left( y_i\\ln(\\hat{p}_i)+(1-y_i)\\ln(1-\\hat{p}_i)\\right).\\] For observation \\(i\\), the regular residual is \\(e_i=y_i-\\hat{p}_i.\\) We then define the deviance residuals to be \\[d_i=\\text{sign}(e_i)\\left[-2(y_i\\ln(\\hat{p}_i)+(1-y_i)\\ln(1-\\hat{p}_i))\\right]^{1/2},\\] where \\(\\text{sign}(e_i)\\) is -1 if \\(e_i&lt;0\\) and 1 if \\(e_i&gt;0.\\) These deviance residuals are stored in the logistic regression output. head(residuals(logfit),4) ## 1 2 3 4 ## -0.4467738 0.5368785 -0.3795802 -1.4991921 The residual deviance is then the sum of the squares of the deviance residuals. sum((residuals(logfit))^2) ## [1] 207.3727 deviance(logfit) ## [1] 207.3727 We define the null deviance similarly, but we set \\(\\hat{p}_i=\\hat{p},\\) where \\(\\hat{p}\\) is the proportion of ones (or Yesses) in the data. In our Pima data set, \\(\\hat{p}=68/200=0.34.\\) The null model is basically an intercept-only model. If a logistic model is any good, its residual deviance should be significantly smaller than the null deviance. logfit$null.deviance ## [1] 256.4142 One very simple \\(R^2\\) analog for logistic regression is \\[\\begin{equation}R^2=1-\\frac{\\text{Residual Deviance}}{\\text{Null Deviance}}. \\tag{5.12} \\end{equation}\\] In our one-variable example, we have \\[R^2=1-\\frac{207.37}{256.41}=0.1913.\\] When we add age as the second independent variable, we get \\[R^2=1-\\frac{197.11}{256.41}=0.2313.\\] 5.10 Gradient Descent In Section 5.9, we couldnt use a formula to find the coefficient estimates. There are various approximation techniques we can use to estimate parameters. One of them is the method of gradient descent, which can find minimum and maximum values of functions. This discussion does assume a working knowledge of partial differentiation, which is covered in Appendix A. We will start with the one-variable case. Suppose we want to find the minimum value of the function \\[f(x)=x^4-3x^2+5x+10.\\] Its derivative is the cubic function \\[f&#39;(x)=4x^3-6x+5.\\] Finding the critical number involves solving a cubic equation, which is a pain in the neck. Instead, we will be taking advantage of the geometry of the graph to estimate the critical number. From the graph (Fig. 5.27), we see that the minimum value occurs between \\(-2\\) and \\(-1.\\) Figure 5.27: Finding a local minimum Note that to the left of the minimum, \\(f&#39;&lt;0\\), while to the right, \\(f&#39;&gt;0\\). Also, because of the continuity of \\(f&#39;\\), the closer \\(x\\) gets to the minimum, the closer \\(f&#39;(x)\\) gets to 0. Our algorithm will start with a guess \\(x_0\\) that we hope is close to the critical number. The sign of \\(f&#39;(x_0)\\) will tell us whether we need to move to the left or right: to the right if \\(f&#39;(x_0)&lt;0\\) and to the left if \\(f&#39;(x_0)&gt;0\\) (Fig. 5.28). Figure 5.28: Left or right The magnitude (absolute value) of \\(f&#39;(x_0)\\) tells us approximately how far we need to move. Remember that \\(f&#39;(x)\\) approaches 0 as \\(x\\) approaches the critical number. How far we move in either direction can be adjusted by what is called the learning rate, which we will denote by \\(\\lambda\\), where \\(\\lambda&gt;0\\). Once we settle on a learning rate, we have our iteration formula. \\[\\begin{align*} x_1&amp;=x_0-\\lambda f&#39;(x_0)\\\\ x_2&amp;=x_1-\\lambda f&#39;(x_1)\\\\ &amp;\\vdots \\\\ x_{n+1}&amp;=x_n-\\lambda f&#39;(x_n). \\end{align*}\\] If \\(f&#39;(x_n)&lt;0\\), then, since we are subtracting \\(\\lambda f&#39;(x_n)\\), we will move to the right. If \\(f&#39;(x_n)&gt;0\\), we will move to the left. The larger the value of \\(|f&#39;(x_n)|\\), the farther we move. We need to choose \\(\\lambda\\) so that we dont move too far. This can involve trial and error. It also can be adjusted using a learning rate schedule as we get closer to our critical number. Suppose we start with an initial guess of \\(x_0=-2.0\\) and a learning rate of \\(\\lambda=0.001\\) Then \\[x_1=-2.0-0.001\\cdot f&#39;(-2.0)=-2.0-0.001\\cdot(-15)=-1.985.\\] Our first 5 steps look like this. \\(n\\) \\(x_n\\) \\(0\\) \\(-2.0\\) \\(1\\) \\(-1.985\\) \\(2\\) \\(-1.9706\\) \\(3\\) \\(-1.9568\\) \\(4\\) \\(-1.9436\\) \\(5\\) \\(-1.9309\\) If we look at the \\((x_n,f(x_n))\\)-pairs on the graph (Fig. 5.29), we see that we are making slow, but steady progress. Figure 5.29: Gradient descent Our gradient descent algorithm needs a mechanism for deciding when were close enough. We can choose to terminate the process when consecutive \\(x\\) values agree within a certain amount, or perhaps when the absolute value of \\(f&#39;(x_n)\\) gets below a certain amount. We should also set a maximum number of iterations that we want to go through, just in case there is a problem. Here is our algorithm. # Define f and f&#39; f &lt;- function(x) x^4-3*x^2+5*x+10 df &lt;- function(x) 4*x^3-6*x+5 # initialization x0 &lt;- -2 #initial guess lambda &lt;- 0.001 #learning rate tol &lt;- 0.0001 #tolerance (When do we stop?) maxsteps &lt;- 5000 steps &lt;- 0 # The iteration while(abs(df(x0)) &gt; tol &amp; steps &lt; maxsteps){ x0&lt;-x0-lambda*df(x=x0) steps&lt;-steps+1} c1 &lt;- paste(&quot;x = &quot;,round(x0,3)) c2 &lt;- paste(&quot;minimum = &quot;,round(f(x0),3)) c3 &lt;- paste(&quot;steps = &quot;, steps) cat(c1,c2,c3,sep=&quot;\\n&quot;) ## x = -1.523 ## minimum = 0.807 ## steps = 507 Weve found the location of the minimum in a measly 507 steps. If that seems too slow, we can increase the learning rate. When the learning rate is adjusted to \\(\\lambda=0.01\\), we get convergence in 46 steps. With \\(\\lambda=0.05\\), it converges in six (Fig. 5.30)! Figure 5.30: Learning rates \\(\\lambda=0.01\\) (left),\\(\\lambda=0.05\\) (right) If we pick a value of \\(\\lambda\\) that is too high, for instance, \\(\\lambda=0.1\\), something bad happens. It just bounces around between two values (Fig. 5.31). Figure 5.31: A learning rate that is too high This function in Figure 5.32 has two local minima. If we start near the one on the right, and use a small enough learning rate, we converge to it. If our learning rate is too large, we end up heading to the other minimum. Figure 5.32: Two local minima Functions of several variables Now lets turn to functions of several variables. As an example, lets try to find the minimum value of \\[f(x,y)=x^4+y^4-x+2y.\\] The 3D and contour plots for \\(f\\) appear in Figure 5.33. Figure 5.33: A function of two variables. Recall that at every point \\((x,y)\\), the gradient of \\(f\\), \\(\\nabla f(x,y)\\), points in the direction of greatest increase (Fig. 5.34). If we want to find the minimum of \\(f\\), we need to go in the opposite direction. Figure 5.34: The gradient points uphill If our initial guess in vector form is \\(\\begin{bmatrix}x_0\\\\y_0\\end{bmatrix}\\), then our iteration formula is \\[\\begin{bmatrix}x_{n+1}\\\\y_{n+1}\\end{bmatrix}=\\begin{bmatrix}x_{n}\\\\y_{n}\\end{bmatrix}-\\lambda \\nabla f(x_n,y_n).\\] The process can either terminate when successive approximations get to be within a certain amount of each other, or when the norm of \\(\\nabla f(x_n,y_n)\\) gets below a certain value. It can also terminate if it takes too long to converge. With our example, \\(f(x,y)=x^4+y^4-x+2y\\), we have \\[\\nabla f(x,y)=\\begin{bmatrix}4x^3-1\\\\4y^3+2\\end{bmatrix}.\\] Choosing an initial guess of \\((x_0,y_0)=(0,0)\\), and \\(\\lambda=0.01\\), our next step is \\[\\begin{bmatrix} x_1\\\\y_1\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}-0.01\\left[\\begin{array}{r}-1\\\\2\\end{array}\\right]=\\left[\\begin{array}{r}0.01\\\\-0.02\\end{array}\\right].\\] Here is the algorithm in R. # Define the functions and partial derivatives f&lt;-function(x,y) x^4+y^4-x+2*y fx&lt;-function(x,y) 4*x^3-1 fy&lt;-function(x,y) 4*y^3+2 # Initialize with (x0,y0)=(0,0) x0 &lt;- 0 y0 &lt;- 0 lambda &lt;- 0.01 maxsteps &lt;- 5000 steps &lt;- 0 tol &lt;- 0.0001 # Find the norm of the gradient at the initial point gnorm &lt;- sqrt(fx(x0,y0)^2+fy(x0,y0)^2) # The iteration while(gnorm&gt;tol &amp; steps&lt;maxsteps){ x1 &lt;- x0-lambda*fx(x0,y0) y1 &lt;- y0-lambda*fy(x0,y0) x0 &lt;- x1 y0 &lt;- y1 steps&lt;-steps+1 gnorm&lt;-sqrt(fx(x0,y0)^2+fy(x0,y0)^2) } c1 &lt;- paste(&quot;(x,y) = (&quot;,round(x0,3),&quot;,&quot;,round(y0,3),&quot;)&quot;) c2 &lt;- paste(&quot;minimum = &quot;,round(f(x0,y0),3)) c3 &lt;- paste(&quot;steps = &quot;, steps) cat(c1,c2,c3,sep=&quot;\\n&quot;) ## (x,y) = ( 0.63 , -0.794 ) ## minimum = -1.663 ## steps = 243 Figure 5.35 shows the path to the minimum. Of course, we can improve covergence by adjusting the learning rate. Figure 5.35: Gradient descent to a minimum for a function of two variables Optimizing parameters An important application of gradient descent algorithms is in the optimization of parameters. As a simple example, we will use it to approximate the least squares regression coefficients. The gradient descent algorithm needs a function to minimize. In least squares regression, we have a natural candidate: the sum of squares of the residuals, \\[S=\\sum_{i=1}^n(y_i-(\\hat{b}_0+\\hat{b}_1x_i))^2,\\] which is a function of \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), the parameters that we are trying to estimate. Were going to adjust this a little bit. The generic term for the function that we try to minimize in gradient descent is the cost function. The traditional cost function for regression is \\[f(\\hat{b}_0,\\hat{b}_1)=\\frac{1}{2n}\\sum_{i=1}^n(y_i-(\\hat{b}_0+\\hat{b}_1x_i))^2\\] (or sometimes just \\(1/n\\) in front). The scaling means that the term wont grow as the sample size grows. The 2 goes away when we take the partial derivatives. Gradient descent requires the gradient, and the partial derivatives are \\[\\frac{\\partial f}{\\partial \\hat{b}_0}=-\\frac{1}{n}\\sum_{i=1}^n(y_i-(\\hat{b}_0+\\hat{b}_1x_i))\\] and \\[\\frac{\\partial f}{\\partial \\hat{b}_1}=-\\frac{1}{n}\\sum_{i=1}^n(y_i-(\\hat{b}_0+\\hat{b}_1x_i))x_i.\\] We saw the unscaled version of these when we derived the formulas for \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). Lets try to use gradient descent to find \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) for the following data. \\(x_i\\) \\(y_i\\) 1 3 2 4 3 2 4 5 Well start with initial guesses of \\(\\hat{b}_0=2\\) and \\(\\hat{b}_1=1\\), and use a learning rate of \\(\\lambda=0.01\\). Were going to calculate each pairs contribution to the gradient in this table. \\(x_i\\) \\(y_i\\) \\(-\\frac{1}{4}(y_i-(\\hat{b}_0+\\hat{b}_1x_i))\\) \\(-\\frac{1}{4}(y_i-(\\hat{b}_0+\\hat{b}_1x_i))x_i\\) 1 3 \\(-\\frac{1}{4}(3-(2+1\\cdot 1))=0\\) \\(-\\frac{1}{4}(3-(2+1\\cdot 1))\\cdot 1=0\\) 2 4 \\(-\\frac{1}{4}(4-(2+1\\cdot 2))=0\\) \\(-\\frac{1}{4}(4-(2+1\\cdot 2))\\cdot 2=0\\) 3 2 \\(-\\frac{1}{4}(2-(2+1\\cdot 3))=0.75\\) \\(-\\frac{1}{4}(2-(2+1\\cdot 3)\\cdot 3=2.25\\) 4 5 \\(-\\frac{1}{4}(5-(2+1\\cdot 4))=0.25\\) \\(-\\frac{1}{4}(5-(2+1\\cdot 4))\\cdot 4=1\\) \\(\\sum\\) \\(1\\) \\(3.25\\) Our next guess for \\(\\hat{b}_0\\) will be \\(2-0.01\\cdot 1=1.99\\) and our next guess for \\(\\hat{b}_1\\) will be \\(1-0.01\\cdot 3.25=0.9675\\). Well continue with R. # Cost function and its partials cost &lt;- function(x,y,b0,b1){ sum((y-(b0+b1*x))^2)/(2*length(x))} dcostb0 &lt;- function(x,y,b0,b1){ -sum(y-(b0+b1*x))/length(x)} dcostb1 &lt;- function(x,y,b0,b1){ -sum(y*x-b0*x-b1*x^2)/length(x)} # Data x &lt;- c(1,2,3,4) y &lt;- c(3,4,2,5) # Initialization b0&lt;-2 b1&lt;-1 lambda&lt;-0.1 tol&lt;-0.00001 maxsteps&lt;-10000 gnorm&lt;-sqrt(dcostb0(x,y,b0,b1)^2+dcostb1(x,y,b0,b1)^2) steps&lt;-0 #Iteration while(gnorm&gt;tol &amp; steps &lt;maxsteps){ b0a&lt;-b0-lambda*dcostb0(x,y,b0,b1) b1a&lt;-b1-lambda*dcostb1(x,y,b0,b1) b0&lt;-b0a b1&lt;-b1a gnorm&lt;-sqrt(dcostb0(x,y,b0,b1)^2+dcostb1(x,y,b0,b1)^2) steps&lt;-steps+1 } c1 &lt;- paste(&quot;b0 = &quot;,round(b0,5)) c2 &lt;- paste(&quot;b1 = &quot;, round(b1,5)) c3 &lt;- paste(&quot;steps = &quot;,steps) cat(c1,c2,c3,sep=&quot;\\n&quot;) ## b0 = 2.49994 ## b1 = 0.40002 ## steps = 611 The actual intercept and slope are \\(\\hat{b}_0=2.5\\) and \\(\\hat{b}_1=0.4\\), respectively. A larger learning rate will improve things, but we dont really need to use gradient descent for linear regression on a small data set. Using traditional gradient descent on large data sets can be computationally expensive and slow. Stochastic gradient descent intends to improve the speed by using a single randomly selected data points contribution to the gradient to update the parameters at each step. While it may take more steps for convergence, each step involves significantly fewer calculations. 5.11 Exercises Find the least squares solution to \\[\\begin{bmatrix} 4 &amp; 3\\\\2 &amp; 5\\\\2 &amp; 0\\\\4 &amp; 1\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}2\\\\3\\\\1\\\\2\\end{bmatrix}.\\] Find the least squares solution to \\[\\begin{bmatrix} 2 &amp; 1\\\\1 &amp; 3\\\\4 &amp; 2\\\\1 &amp; 5\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}2\\\\0\\\\1\\\\5\\end{bmatrix}.\\] Find the least squares solution to the following matrix equation. Note: the matrix on the left has dependent columns. You will probably want to use technology. \\[\\begin{bmatrix} 1 &amp; 1 &amp; 2\\\\1 &amp; 2 &amp; 3\\\\1 &amp; 3 &amp; 4\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}3\\\\4\\\\6\\end{bmatrix}\\] Find the least squares solution to the following matrix equation. Note: the matrix on the left has dependent columns. You will probably want to use technology. \\[\\begin{bmatrix} 1 &amp; 3 &amp; 2\\\\1 &amp; 2 &amp; 3\\\\1 &amp; 1 &amp; 4\\end{bmatrix}\\mathbf{x}=\\begin{bmatrix}4\\\\3\\\\3\\end{bmatrix}\\] Find the least squares error in the solution to Problem 1. Find the least squares error in the solution to Problem 2. Find the Massey ratings for the results we saw in Problem 39 from Chapter 1 and Problems 29 and 31 from Chapter 3. Winner Score Loser Score Argos 20 Brahmas 13 Cobbers 31 Brahmas 10 Argos 24 Cobbers 21 Dukes 10 Brahmas 7 Brahmas 35 Dukes 3 Find the Massey ratings for the results we saw in Problem 40 from Chapter 1 and Problems 30 and 32 from Chapter 3. Winner Score Loser Score Argos 28 Cobbers 14 Brahmas 21 Argos 13 Brahmas 17 Dukes 10 Dukes 31 Brahmas 28 Find the offensive and defensive team ratings for the results in Problem 7. Find the offensive and defensive team ratings for the results in Problem 8. Suppose we have three \\((x,y)\\)-pairs: \\((2,5),(1,7),\\) and \\((4,2)\\). Find the least squares regression line \\(\\hat{y}=\\hat{b}_0+\\hat{b}_1x\\) using the normal equations (Theorem 5.1). Suppose we have three \\((x,y)\\)-pairs: \\((4,1),(2,6),\\) and \\((6,3)\\). Find the least squares regression line \\(\\hat{y}=\\hat{b}_0+\\hat{b}_1x\\) using the normal equations. Suppose we have four \\((x,y,z)\\)-triples: \\((1,2,3),(2,4,1),(5,0,2),\\) and \\((3,3,1).\\). Find the least squares regression equation \\(\\hat{z}=\\hat{b}_0+\\hat{b}_1x+\\hat{b}_2y\\) using the normal equations. Suppose we have four \\((x,y,z)\\)-triples: \\((2,1,4),(3,2,1),(5,1,1),\\) and \\((0,2,3).\\). Find the least squares regression equation \\(\\hat{z}=\\hat{b}_0+\\hat{b}_1x+\\hat{b}_2y\\) using the normal equations. Find Pearsons correlation coefficient for the following data, using only basic calculator functions. x y 1 2 3 5 4 3 6 6 2 3 Find Pearsons correlation coefficient for the following data, using only basic calculator functions. x y 2 1 3 1 5 2 3 3 5 4 Spearmans rank correlation ranks the \\(x\\) and \\(y\\) values and then finds the correlation of the ranks. For instance, for the data in Problem 15, when we rank the values, we get the table below. (Ties get averaged.) Find Spearmans rank correlation by finding Pearsons correlation of the ranks. (Note: you might find some special formulas for Spearmans rank correlation, but those often only work when there are no ties in the data.) x y x.rank y.rank 1 2 1 1.0 3 5 3 4.0 4 3 4 2.5 6 6 5 5.0 2 3 2 2.5 Find Spearmans correlation for the data in Problem 16. Use Rs cor function to find the variable in the swiss data frame that is most strongly correlated with Education other than Education itself. Note: the strongest correlation might be negative. Use Rs cor function to find the variable in the swiss data frame that is most strongly correlated with Agriculture other than Agriculture itself. Note: the strongest correlation might be negative. Use Formulas (5.9) and (5.8) to find the least squares regression equation for the data in Problem 15. Find the residual for each observation. Use Formulas (5.9) and (5.8) to find the least squares regression equation for the data in Problem 16. Find the residual for each observation. Use R to do the following. Find the least squares regression equation for Education as a function of Examination from the swiss data frame. State the effect of a one-unit increase in the Examination value on the mean Education value. State the coefficient of determination in the context of the data. Determine which coefficents are significantly different from zero. (Which \\(P\\)-values are below 0.05?) Find 95% confidence intervals for the coefficients. Find a 95% confidence interval for the mean Education value for an Examination value of 16. Find a 95% prediction interval for the Education value for an Examination value of 16. Use R to do the following. Find the least squares regression equation for Agriculture as a function of Examination from the swiss data frame. State the effect of a one-unit increase in the Examination value on the mean Agriculture value. State the coefficient of determination in the context of the data. Determine which coefficents are significantly different from zero. (Which \\(P\\)-values are below 0.05?) Find 95% confidence intervals for the coefficients. Find a 95% confidence interval for the mean Agriculture value for an Examination value of 16. Find a 95% prediction interval for the Agriculture value for an Examination value of 16. Use Rs lm function to find a linear equation for \\(z\\) as a function of \\(x\\) and \\(y\\) for the data in Problem 13. Write out the equation. Find the coefficient of determination. Identify which coefficients are significantly different from zero. (Which \\(P\\)-values are below 0.05?) Use Rs lm function to find a linear equation for \\(z\\) as a function of \\(x\\) and \\(y\\) for the data in Problem 14. Write out the equation. Find the coefficient of determination. Identify which coefficients are significantly different from zero. (Which \\(P\\)-values are below 0.05?) Use R to do the following. Write Education as a linear function of Examination and Agriculture. Find the coefficient of determination. Identify which coefficients are significantly different from zero. Find a 95% prediction interval for the Education value if Examination = 16 and Agriculture = 50. Use R to do the following. Write Agriculture as a linear function of Examination and Education. Find the coefficient of determination. Identify which coefficients are significantly different from zero. Find a 95% prediction interval for the Agriculture value if Examination = 16 and Education = 10. Use R to do the following. Write mpg as a linear function of hp and vs from the mtcars data frame. The vs variable is an indicator variable. What effect does changing from a v-shaped engine (vs = 0) to a straight engine (vs = 1) have on mpg? Use R to do the following. Write wt as a linear function of hp and vs from the mtcars data frame. The vs variable is an indicator variable. What effect does changing from a v-shaped engine (vs = 0) to a straight engine (vs = 1) have on wt? Create a linear model with the same variables as in Problem 29, but include an interaction term between hp and vs. How does going from vs = 0 to vs = 1 affect the intercept? How does going from vs = 0 to vs = 1 affect the slope? Create a linear model with the same variables as in Problem 30, but include an interaction term between hp and vs. How does going from vs = 0 to vs = 1 affect the intercept? How does going from vs = 0 to vs = 1 affect the slope? The cars data frame has data on stopping distances in feet (dist) for cars based on their speed in miles per hour (speed). Use R to find a model for stopping distance as a quadratic function of speed. Is the quadratic fit better than a linear fit? Explain. (See Problem 33.) Use R to find a model for stopping distance as a cubic function (third degree polynomial) of speed. Include first and second degree terms. Is the cubic fit better than a linear or quadratic fit? Explain. Suppose a model with four independent variables based on 50 observations has a regular \\(R^2\\) value of 0.85. Find the adjusted \\(R^2\\) value. Suppose a model with five independent variables based on 40 observations has a regular \\(R^2\\) value of 0.92. Find the adjusted \\(R^2\\) value. Use the regsubsets function from the leaps package on the mtcars data frame with qsec as the dependent variable, and all the other variables as independent variables. Which variables are involved in the best 2-variable model? Which variables are involved in the best 3-variable model? According to the BIC plot, which variables are involved in the best overall model? Use the regsubsets function from the leaps package on the mtcars data frame with wt as the dependent variable, and all the other variables as independent variables. Which variables are involved in the best 2-variable model? Which variables are involved in the best 3-variable model? According to the BIC plot, which variables are involved in the best overall model? Use the step function to perform backwards stepwise regression on the mtcars data frame with qsec as the dependent variable and all of the other variables as independent variables. Which is the first variable removed? What variables are involved in the final model? Does forward stepwise regression produce a different final model? Use the step function to perform backwards stepwise regression on the mtcars data frame with wt as the dependent variable and all of the other variables as independent variables. Which is the first variable removed? What variables are involved in the final model? Does forward stepwise regression produce a different final model? Remove the setosa species from the iris data frame, creating a new data frame called iris2 by executing the command iris2 &lt;- droplevels(subset(iris, Species != \"setosa\")). Now create a logistic model with Species as a function of Petal.Width (units are centimeters). The model will have versicolor as species 0, and virginica as species 1. Write the logistic model that gives the probability a flower is virginica as a function of its petal width. How does an increase of 1 centimeter in petal width change the odds that a flower is a virginica? Estimate the probabilities a flower is a virginica for petal widths of 1 and 2 centimeters. Find the logistic \\(R^2\\) value (Equation (5.12)). Working with the same data as in Problem 41, create a logistic model with Species as a function of Sepal.Length Write the logistic model that gives the probability a flower is virginica as a function of its sepal length. How does an increase of 1 centimeter in sepal length change the odds that a flower is a virginica? Estimate the probabilities a flower is a virginica for sepal lengths of 6 and 7 centimeters. Find the logistic \\(R^2\\) value. Working with the same data as in Problem 41, create a logistic model with Species as a function of Petal.Width and Petal.Length. Estimate the probability a flower is a virginica if it has a petal length of 5cm and a petal width of 1.5cm. Working with the same data as in Problem 41, create a logistic model with Species as a function of Sepal.Width and Sepal.Length. Estimate the probability a flower is a virginica if it has a sepal length of 6cm and a sepal width of 3cm. The function \\(f(x)=x^4-2x^3+4x^2-x\\) has a local minimum between \\(x=0\\) and \\(x=1.\\) Apply the gradient descent algorithm, starting with an initial guess of \\(x_0=1\\) and a learning rate of \\(\\lambda=0.1.\\) Calculate \\(x_1,x_2,\\) and \\(x_3\\) by hand. Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001. The function \\(f(x)=x^4+2x^3+3x^2-2x\\) has a local minimum between \\(x=0\\) and \\(x=1.\\) Apply the gradient descent algorithm, starting with an initial guess of \\(x_0=1\\) and a learning rate of \\(\\lambda=0.1.\\) Calculate \\(x_1,x_2,\\) and \\(x_3\\) by hand. Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001. The function \\(f(x,y)=x^4+y^2-x-y\\) has a global minimum in the square \\(0\\leq x \\leq 1, 0\\leq y\\leq 1.\\) Apply the gradient descent algorithm, starting with an initial guess of \\((x_0,y_0)=(1,1)\\) and a learning rate of \\(\\lambda=0.1.\\) Calculate \\((x_1,y_1)\\) and \\((x_2,y_2)\\) by hand. Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001. The function \\(f(x,y)=x^4+2y^2-2x-3y\\) has a global minimum in the square \\(0\\leq x \\leq 1, 0\\leq y\\leq 1.\\) Apply the gradient descent algorithm, starting with an initial guess of \\((x_0,y_0)=(1,1)\\) and a learning rate of \\(\\lambda=0.1.\\) Calculate \\((x_1,y_1)\\) and \\((x_2,y_2)\\) by hand. Implement the gradient descent algorithm using R or some other software package to locate the minimum within 0.0001. References 1. Navidi W (2015) Statistics for engineers and scientists, 4th ed., McGraw-Hill Education. 9. Olofsson P, Andersson M (2012) Probability, statistics, and stochastic processes, 2nd ed., John Wiley &amp; Sons, Inc. 11. Langville AN, Meyer CD (2012) Whos #1? The science of rating and ranking, Princeton University Press. 15. Massey K (1997) Statistical models applied to the rating of sports teams. 16. Smith B (2015) Calculating original gravity for beer recipe design. 17. Bobbitt Z (2021) How to interpret diagnostic plots in R. 18. Akaike H (1974) A new look at the statistical model identification. IEEE Trans Automatic Control AC-19: 716723. 19. Schwarz G (1978) Estimating the dimension of a model. Ann Statist 6: 461464. Or mean ABV. Well discuss this later. "],["svd-and-pca.html", "Chapter 6 SVD and PCA 6.1 Diagonalizable and Symmetric Matrices 6.2 Quadratic Forms and Constrained Optimization 6.3 The Singular Value Decomposition 6.4 Applications of the SVD 6.5 Principal Component Analysis 6.6 Exercises", " Chapter 6 SVD and PCA In this chapter, we will learn about two common techniques involving data matrices: the Singular Value Decomposition (SVD) and Principal Component Analysis (PCA). In order to discuss them, we need to lay a foundation. This development is based on that of David C. Lay, Steven R. Lay, and Judi J. McDonald [5]. 6.1 Diagonalizable and Symmetric Matrices Diagonal matrices are nice to work with. For instance \\[\\begin{bmatrix}\\lambda_1 &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3 &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n\\end{bmatrix}^2=\\begin{bmatrix}\\lambda_1^2 &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2^2 &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3^2 &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n^2\\end{bmatrix}\\] (the off-diagonal entries are all 0), and for positive integers \\(k\\), \\[\\begin{bmatrix}\\lambda_1 &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3 &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n\\end{bmatrix}^k=\\begin{bmatrix}\\lambda_1^k &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2^k &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3^k &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n^k\\end{bmatrix}\\] For example, \\[\\begin{bmatrix}2 &amp; 0 &amp; 0\\\\0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 3\\end{bmatrix}^3=\\begin{bmatrix}2^3 &amp; 0 &amp; 0\\\\0 &amp; (-1)^3 &amp; 0\\\\0 &amp; 0 &amp; 3^3\\end{bmatrix}=\\begin{bmatrix}8 &amp; 0 &amp; 0\\\\0 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 27\\end{bmatrix}.\\] Note what multiplication on the left by a diagonal matrix does. \\[\\mathbf{D}\\mathbf{A}=\\begin{bmatrix}\\lambda_1 &amp; 0 &amp; 0\\\\0 &amp; \\lambda_2 &amp; 0\\\\0 &amp; 0 &amp; \\lambda_3\\end{bmatrix}\\begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13}\\\\a_{21} &amp; a_{22} &amp; a_{23}\\\\a_{31} &amp; a_{32} &amp; a_{33}\\end{bmatrix}= \\begin{bmatrix}\\lambda_1a_{11}&amp; \\lambda_1a_{12} &amp; \\lambda_1a_{13}\\\\\\lambda_2a_{21} &amp; \\lambda_2a_{22}&amp; \\lambda_2a_{23}\\\\\\lambda_3a_{31} &amp; \\lambda_3a_{32} &amp; \\lambda_3a_{33}\\end{bmatrix}.\\] Row \\(i\\) of \\(\\mathbf{A}\\) is multiplied by \\(\\lambda_i\\). For example, \\[\\begin{bmatrix}2 &amp; 0\\\\0 &amp; -3\\end{bmatrix}\\begin{bmatrix}1 &amp; 2\\\\3 &amp; 4\\end{bmatrix}=\\begin{bmatrix}2 &amp; 4\\\\-9&amp;-12\\end{bmatrix}.\\] Multiplication on the right by a diagonal matrix does the analogous thing to the columns. \\[\\mathbf{A}\\mathbf{D}=\\begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13}\\\\a_{21} &amp; a_{22} &amp; a_{23}\\\\a_{31} &amp; a_{32} &amp; a_{33}\\end{bmatrix}\\begin{bmatrix}\\lambda_1 &amp; 0 &amp; 0\\\\0 &amp; \\lambda_2 &amp; 0\\\\0 &amp; 0 &amp; \\lambda_3\\end{bmatrix}=\\begin{bmatrix}\\lambda_1a_{11} &amp; \\lambda_2a_{12} &amp; \\lambda_3a_{13}\\\\\\lambda_1a_{21} &amp; \\lambda_2a_{22} &amp; \\lambda_3 a_{23}\\\\ \\lambda_1a_{31} &amp; \\lambda_2 a_{32} &amp; \\lambda_3 a_{33}\\end{bmatrix}.\\] If we write \\(\\mathbf{A}=\\begin{bmatrix}\\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n\\end{bmatrix}\\),then \\[\\begin{bmatrix}\\mathbf{a}_1 &amp; \\mathbf{a}_2 &amp; \\cdots &amp; \\mathbf{a}_n\\end{bmatrix}\\begin{bmatrix}\\lambda_1 &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3 &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n\\end{bmatrix}=\\begin{bmatrix}\\lambda_1\\mathbf{a}_1 &amp; \\lambda_2\\mathbf{a}_2 &amp; \\cdots &amp; \\lambda_n\\mathbf{a}_n\\end{bmatrix}.\\] Diagonalizable matrices Some matrices can be factored with one of the factors being a diagonal matrix. Heres a definition. Definition 6.1 An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is said to be diagonalizable if there exists an invertible \\(n\\times n\\) matrix \\(\\mathbf{P}\\) and an \\(n\\times n\\) diagonal matrix \\(\\mathbf{D}\\) such that \\[\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}.\\] The following theorem tells us exactly which matrices are diagonalizable and how to diagonalize them [5]. Theorem 6.1 An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if and only if \\(\\mathbf{A}\\) has \\(n\\) linearly independent eigenvectors. In particular, \\(\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\\) with \\(\\mathbf{D}\\) a diagonal matrix if and only if the columns of \\(\\mathbf{P}\\) are linearly independent eigenvectors of \\(\\mathbf{A}\\) and the diagonal entries of \\(\\mathbf{D}\\) are the corresponding eigenvalues, in order. Proof. (Idea) Let \\(\\mathbf{P}=\\begin{bmatrix}\\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{v}_n\\end{bmatrix}\\), with \\(\\mathbf{v}_i\\) being the \\(i\\)th column of \\(\\mathbf{P}.\\) Then \\[\\mathbf{A}\\mathbf{P}=\\mathbf{A}\\begin{bmatrix}\\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{v}_n\\end{bmatrix}=\\begin{bmatrix}\\mathbf{A}\\mathbf{v}_1 &amp; \\mathbf{A}\\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{A}\\mathbf{v}_n\\end{bmatrix}.\\] Also, \\[\\mathbf{P}\\mathbf{D}=\\mathbf{P}\\begin{bmatrix}\\lambda_1 &amp; &amp; &amp; &amp; \\\\ &amp; \\lambda_2 &amp;&amp;&amp;\\\\ &amp; &amp; \\lambda_3 &amp; &amp;\\\\ &amp;&amp;&amp;\\ddots &amp;\\\\&amp;&amp;&amp;&amp; \\lambda_n\\end{bmatrix}=\\begin{bmatrix}\\lambda_1\\mathbf{v}_1 &amp; \\lambda_2\\mathbf{v}_2 &amp; \\cdots &amp; \\lambda_n\\mathbf{v}_n \\end{bmatrix}.\\] If \\(\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\\), then \\(\\mathbf{A}\\mathbf{P}=\\mathbf{P}\\mathbf{D}\\), and \\(\\mathbf{A}\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i\\) for all \\(i\\). That is, the \\(i\\)th column of \\(\\mathbf{P}\\) is an eigenvector of \\(\\mathbf{A}\\) with corresponding eigenvalue \\(\\lambda_i\\). Example 6.1 Let \\(\\mathbf{A}=\\begin{bmatrix}1 &amp; 2\\\\2 &amp; 1\\end{bmatrix}\\). The eigenvalues of \\(\\mathbf{A}\\) are \\(\\lambda_1=3\\) and \\(\\lambda_2=-1\\), with corresponding eigenvectors \\(\\mathbf{v}_1=(1,1)\\) and \\(\\mathbf{v}_2=(-1,1)\\). The eigenvectors go in the columns of \\(\\mathbf{P}.\\) With \\(\\mathbf{P}=\\begin{bmatrix}1 &amp; -1\\\\1 &amp; 1\\end{bmatrix}\\), we have \\(\\mathbf{D}=\\begin{bmatrix}3 &amp; 0\\\\0 &amp; -1\\end{bmatrix}\\) and \\(\\mathbf{P}^{-1}=\\frac{1}{2}\\begin{bmatrix}1 &amp; 1\\\\-1 &amp; 1\\end{bmatrix}\\). That means that we can write \\[\\begin{bmatrix}1 &amp; 2\\\\2 &amp; 1\\end{bmatrix}=\\begin{bmatrix}1 &amp; -1\\\\1 &amp; 1\\end{bmatrix}\\begin{bmatrix}3 &amp; 0\\\\0 &amp; -1\\end{bmatrix}\\begin{bmatrix}\\frac{1}{2} &amp; \\frac{1}{2}\\\\-\\frac{1}{2} &amp; \\frac{1}{2}\\end{bmatrix}.\\] Note that if we enter the eigenvectors in a different order, we will get a different diagonalization. Diagonalization has many applications. For one thing, it makes diagonalizable matrices easy to exponentiate. Indeed, for a positive integer \\(k\\), \\[\\begin{align*} \\mathbf{A}^k=(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})^k&amp;=\\underbrace{\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\\cdots\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}}_{k \\text{ terms }}\\\\ &amp;=\\mathbf{P}\\mathbf{D}^k\\mathbf{P}^{-1} \\end{align*}\\] For the matrix from Example 6.1, \\[\\begin{align*} \\mathbf{A}^3&amp;=\\mathbf{P}\\mathbf{D}^3\\mathbf{P}^{-1}\\\\ \\begin{bmatrix}1 &amp; 2\\\\2 &amp; 1\\end{bmatrix}^3&amp;=\\begin{bmatrix}1 &amp; -1\\\\1 &amp; 1\\end{bmatrix}\\begin{bmatrix}3^3 &amp; 0\\\\0 &amp; (-1)^3\\end{bmatrix}\\begin{bmatrix}\\frac{1}{2} &amp; \\frac{1}{2}\\\\-\\frac{1}{2} &amp; \\frac{1}{2}\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}13 &amp; 14\\\\14 &amp; 13\\end{bmatrix}. \\end{align*}\\] Not all matrices are diagonalizable. For instance, the triangular matrix \\[\\mathbf{A}=\\begin{bmatrix}3 &amp; 2\\\\0 &amp; 3\\end{bmatrix}\\] has a repeated eigenvalue of \\(\\lambda=3\\). When we try to solve for the eigenvectors, we get \\[\\mathbf{A}-3\\mathbf{I}=\\begin{bmatrix}0 &amp; 2\\\\0 &amp; 0\\end{bmatrix}.\\] Solving \\((\\mathbf{A}-3\\mathbf{I})\\mathbf{v}=\\mathbf{0}\\) using an augmented matrix, we have \\[\\left[\\begin{array}{rr|r}0 &amp; 2 &amp; 0\\\\0 &amp; 0 &amp; 0\\end{array}\\right],\\] which gives us \\(v_2=0\\). Any eigenvector is of the form \\((v_1,0)\\). There is no second linearly independent eigenvector. Symmetric matrices Symmetric matrices are going to play an important role in future applications. Heres the definition. Definition 6.2 A matrix \\(\\mathbf{A}\\) is symmetric if \\[\\mathbf{A}^T=\\mathbf{A}.\\] Note that since the transpose of an \\(m\\times n\\) matrix is a \\(n\\times m\\) matrix, only square matrices can be symmetric. Here are some examples. The matrices \\[\\begin{bmatrix}1 &amp; 2\\\\2 &amp; 1\\end{bmatrix} \\text{ and } \\begin{bmatrix} 2 &amp; 4 &amp; -1\\\\4 &amp; 5 &amp; 2\\\\-1 &amp; 2 &amp; 8\\end{bmatrix}\\] are symmetric. The matrices \\[\\begin{bmatrix} 2 &amp; 1\\\\2 &amp; 3\\end{bmatrix} \\text{ and } \\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\2 &amp; 1 &amp; 5\\end{bmatrix}\\] are not. You might remember that when we were using normal equations to solve least squares problems, we encountered symmetric matrices. In fact, we have the following theorem. Theorem 6.2 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix. Then the \\(n\\times n\\) matrix \\(\\mathbf{M}=\\mathbf{A}^T\\mathbf{A}\\) is symmetric. Proof. The proof is simple: \\[\\mathbf{M}^T=(\\mathbf{A}^T\\mathbf{A})^T=\\mathbf{A}^T(\\mathbf{A}^T)^T=\\mathbf{A}^T\\mathbf{A}=\\mathbf{M}.\\] Note that by the same reasoning, \\(\\mathbf{A}\\mathbf{A}^T\\) is also symmetric. Eigenvalues and eigenvectors of symmetric matrices Eigenvectors of symmetric matrices have a special property. Lets start with an example. Let \\[\\mathbf{A}=\\left[\\begin{array}{rrr}6 &amp; -2 &amp; -1\\\\-2 &amp; 6 &amp; -1\\\\-1 &amp; -1 &amp; 5\\end{array}\\right].\\] With a little work, we find that the eigenvalues are \\(\\lambda_1=8,\\lambda_2=6,\\) and \\(\\lambda_3=3\\), with corresponding eigenvectors, \\[\\mathbf{v}_1=\\left[\\begin{array}{r}-1\\\\1\\\\0\\end{array}\\right],\\mathbf{v}_2=\\left[\\begin{array}{r}-1\\\\-1\\\\2\\end{array}\\right], \\text{ and } \\mathbf{v}_3=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}.\\] A quick check reveals that these eigenvectors are mutually orthogonal! In Example 6.1, we diagonalized the symmetric matrix \\(\\begin{bmatrix}1 &amp; 2\\\\2&amp;1\\end{bmatrix}.\\) Not mentioned at the time was the fact that the eigenvectors \\[\\mathbf{v}_1=\\begin{bmatrix}1\\\\1\\end{bmatrix} \\text{ and } \\mathbf{v}_2=\\begin{bmatrix}-1\\\\1\\end{bmatrix}\\] are also mutually orthogonal. We have a theorem. Theorem 6.3 If \\(\\mathbf{A}\\) is symmetric, then any two eigenvectors of \\(\\mathbf{A}\\) corresponding to different eigenvalues are orthogonal. Proof. The proof is simple enough. Let \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) be eigenvectors with corresponding eigenvalues \\(\\lambda_1\\) and \\(\\lambda_2\\), where \\(\\lambda_1\\neq \\lambda_2\\). We want to show that \\(\\mathbf{v}_1\\cdot \\mathbf{v}_2=0\\). We have \\[\\begin{align*} \\lambda_1\\mathbf{v}_1\\cdot \\mathbf{v}_2&amp;=(\\lambda_1\\mathbf{v}_1)^T\\mathbf{v}_2\\\\ &amp;=(\\mathbf{A}\\mathbf{v}_1)^T\\mathbf{v}_2\\\\ &amp;=\\mathbf{v}_1^T\\mathbf{A}^T\\mathbf{v}_2=\\mathbf{v}_1^T(\\mathbf{A}\\mathbf{v}_2) \\text{( because} \\mathbf{A}^T=\\mathbf{A})\\\\ &amp;=\\mathbf{v}_1^T(\\lambda_2\\mathbf{v}_2)\\\\ &amp;=\\lambda_2\\mathbf{v}_1^T\\mathbf{v}_2=\\lambda_2\\mathbf{v}_1\\cdot \\mathbf{v}_2. \\end{align*}\\] Since \\(\\lambda_1\\neq \\lambda_2\\), we can only have equality if \\(\\mathbf{v}_1\\cdot \\mathbf{v}_2=0\\). Orthogonal diagonalization Assuming we have a full set of linearly independent eigenvectors, when we diagonalize a symmetric matrix, we can use orthonormal eigenvectors in the matrix \\(\\mathbf{P},\\) making \\(\\mathbf{P}\\) an orthogonal matrix. (Remember them? They satisfy the property that \\(\\mathbf{P}^{-1}=\\mathbf{P}^T\\).) A matrix that can be diagonalized using an orthogonal matrix is called orthogonally diagonalizable. Suppose a matrix \\(\\mathbf{A}\\) is orthogonally diagonalizable. Then \\[\\begin{align*} \\mathbf{A}^T&amp;=(\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1})^T\\\\ &amp;=(\\mathbf{P}\\mathbf{D}\\mathbf{P}^T)^T\\\\ &amp;=(\\mathbf{P}^T)^T\\mathbf{D}^T\\mathbf{P}^T\\\\ &amp;=\\mathbf{P}\\mathbf{D}\\mathbf{P}^T=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}=\\mathbf{A} \\end{align*}\\] All orthogonally diagonalizable matrices are symmetric. It works both ways, though showing that all symmetric matrices are orthogonally diagonalizable is more complicated. Theorem 6.4 An \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is orthogonally diagonalizable if and only if \\(\\mathbf{A}\\) is a symmetric matrix. This theorem is key for our future applications. 6.2 Quadratic Forms and Constrained Optimization Many important properties of symmetric matrices appear in the context of quadratic forms. If \\(\\mathbf{A}\\) is a symmetric \\(n\\times n\\) matrix and \\(\\mathbf{x}\\) is a vector in \\(\\mathbb{R}^n\\), then the real-valued function \\[Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\] is called a quadratic form. Example 6.2 Let \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 3\\\\3 &amp; 1\\end{bmatrix}\\) and let \\(\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\). Then \\[\\begin{align*} Q(\\mathbf{x})&amp;=\\begin{bmatrix}x_1 &amp; x_2\\end{bmatrix}\\begin{bmatrix}2 &amp; 3\\\\3 &amp; 1\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix}x_1 &amp; x_2\\end{bmatrix}\\begin{bmatrix}2x_1+3x_2\\\\3x_1+x_2\\end{bmatrix}\\\\ &amp;=2x_1^2+3x_1x_2+3x_1x_2+x_2^2\\\\ &amp;=2x_1^2+6x_1x_2+x_2^2 \\end{align*}\\] Some notes: This is a quadratic function in \\(x_1\\) and \\(x_2\\). The \\(x_1^2\\) and \\(x_2^2\\) coefficients are the diagonal entries of \\(\\mathbf{A},\\) while the \\(x_1x_2\\) cross term is twice the off-diagonal entry. Example 6.3 Let \\(\\mathbf{A}=\\begin{bmatrix}4 &amp; -2\\\\-2 &amp; 5\\end{bmatrix}\\). Then \\[Q(\\mathbf{x})=4x_1^2-4x_1x_2+5x_2^2.\\] Example 6.4 Heres a \\(3\\times 3\\) example. Let \\[\\mathbf{A}=\\begin{bmatrix} 2 &amp; 5 &amp; 6\\\\5 &amp; 3 &amp; -3\\\\6 &amp; -3 &amp; -1\\end{bmatrix}\\] The \\(ij\\)-entry in \\(\\mathbf{A}\\) represents the \\(x_ix_j\\)-coefficient. Since \\(x_{ij}=x_{ji}\\) and \\(a_{ij}=a_{ji},\\) the off-diagonal terms double up when simplified. Thus, \\(Q(\\mathbf{x})=2x_1^2+3x_2^2-x_3^2+10x_1x_2+12x_1x_3-6x_2x_3.\\) The nicest quadratic forms come from diagonal matrices. For instance, if \\[\\mathbf{A}=\\begin{bmatrix}2 &amp; 0 &amp; 0\\\\0 &amp; 5 &amp; 0\\\\0 &amp; 0 &amp; -3\\end{bmatrix},\\] then \\(Q(\\mathbf{x})=2x_1^2+5x_2^2-3x_3^2\\), with no cross terms. It turns out that we can apply whats called a change of variables to turn any quadratic form into one without cross terms. Change of variables If \\(\\mathbf{x}\\) is a variable vector in \\(\\mathbb{R}^n\\) and \\(\\mathbf{P}\\) is an invertible \\(n\\times n\\) matrix, then a change of variables is an equation of the form \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y},\\) or equivalently, \\(\\mathbf{y}=\\mathbf{P}^{-1}\\mathbf{x}.\\) Basically, we are writing \\(\\mathbf{x}\\) in terms of a new basis for \\(\\mathbb{R}^n\\) and the coefficients in that new basis are the components of \\(\\mathbf{y}.\\) We can write the quadratic form \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) in terms of \\(\\mathbf{y}\\) as follows: \\[\\mathbf{x}^T\\mathbf{A}\\mathbf{x}=(\\mathbf{P}\\mathbf{y})^T\\mathbf{A}(\\mathbf{P}\\mathbf{y})=\\mathbf{y}^T\\mathbf{P}^T\\mathbf{A}\\mathbf{P}\\mathbf{y}=\\mathbf{y}^T(\\mathbf{P}^T\\mathbf{A}\\mathbf{P})\\mathbf{y}.\\] Since \\(\\mathbf{A}\\) is symmetric, \\[(\\mathbf{P}^T\\mathbf{A}\\mathbf{P})^T=\\mathbf{P}^T\\mathbf{A}^T(\\mathbf{P}^T)^T=\\mathbf{P}^T\\mathbf{A}\\mathbf{P},\\] so \\(\\mathbf{P}^T\\mathbf{A}\\mathbf{P}\\) is also symmetric and \\(R(\\mathbf{y})=\\mathbf{y}^T(\\mathbf{P}^T\\mathbf{A}\\mathbf{P})\\mathbf{y}\\) is a quadratic form in \\(\\mathbf{y}.\\) Why are we doing this? We learned in Section 6.1 that if \\(\\mathbf{A}\\) is symmetric, then it is orthogonally diagonalizable. That is, there is an orthogonal matrix \\(\\mathbf{P}\\) such that \\(\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^T\\), where \\(\\mathbf{D}\\) is diagonal. If we choose such a \\(\\mathbf{P}\\) for our change of variable, then the matrix \\(\\mathbf{P}^T\\mathbf{A}\\mathbf{P}=\\mathbf{D}\\) is diagonal, and the resulting quadratic form can be written in terms of the eigenvalues of \\(\\mathbf{A},\\) with no cross terms. In Section 6.1, we diagonalized the symmetric matrix \\(\\mathbf{A}=\\begin{bmatrix}1 &amp; 2\\\\2 &amp; 1\\end{bmatrix}\\) using the orthogonal matrix \\[\\mathbf{P}=\\begin{bmatrix} 1/\\sqrt{2} &amp; -1/\\sqrt{2}\\\\1/\\sqrt{2} &amp; 1/\\sqrt{2}\\end{bmatrix},\\] and \\(\\mathbf{D}=\\begin{bmatrix}3 &amp; 0\\\\0 &amp; -1\\end{bmatrix}\\). Under the change of variable \\(\\mathbf{x}=\\mathbf{P}\\mathbf{y}\\), the quadratic form \\(Q(\\mathbf{x})=x_1^2+4x_1x_2+x_2^2\\) becomes \\(R(\\mathbf{y})=3y_1^2-y_2^2\\). Classifying quadratic forms There are just a few types of quadratic forms. Figure 6.1 shows some representative graphs in \\(\\mathbb{R}^2\\). Figure 6.1: Some quadratic forms on \\(\\mathbb{R}^2\\) The upper left quadratic form \\(Q(\\mathbf{x})=3x_1^2+7x_2^2\\) is positive for all nonzero \\(\\mathbf{x}.\\) The last is negative for all nonzero \\(\\mathbf{x}.\\) Here are some definitions. Definition 6.3 A quadratic form \\(Q\\) is positive definite if \\(Q(\\mathbf{x})&gt;0\\) for all \\(\\mathbf{x}\\neq \\mathbf{0}\\), negative definite if \\(Q(\\mathbf{x})&lt;0\\) for all \\(\\mathbf{x}\\neq \\mathbf{0}\\), indefinite if \\(Q(\\mathbf{x})\\) assumes both positive and negative values. In Figure 6.1, the upper left form is positive definite. The bottom right is negative definite. The saddle-shaped one is indefinite. The one on the upper right is never negative, but is 0 at places other than the origin. If \\(Q(\\mathbf{x})\\geq 0\\) for \\(\\mathbf{x}\\neq \\mathbf{0}\\), \\(Q\\) is said to be positive semidefinite. Negative semidefinite has an analogous definition. The eigenvalues of a symmetric matrix determine whether the corresponding quadratic form is positive definite, negative definite, indefinite, and so on. Theorem 6.5 Let \\(\\mathbf{A}\\) be an \\(n\\times n\\) symmetric matrix. Then the quadratic form \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) is positive definite if and only if the eigenvalues of \\(\\mathbf{A}\\) are all positive, negative definite if and only if the eigenvalues of \\(\\mathbf{A}\\) are all negative, or indefinite if and only if some eigenvalues of \\(\\mathbf{A}\\) are positive and some are negative. The positive semidefinite example \\(Q(\\mathbf{x})=3x_1^2\\) had one positive eigenvalue and the other was 0. Remember the Second Derivative Test for functions of two variables (Theorem A.3 in Appendix A)? It boils down to determining whether the matrix of second order partial derivatives at a critical point is positive definite (local minimum), negative definite (local maximum), indefinite (saddle point), or something else. This idea can be extended to functions of any number of variables. Constrained optimization An important property of all matrices, and symmetric matrices in particular, is how they act on unit vectors. Recall that a unit vector is a vector that has length 1. This can be written as \\(|\\mathbf{x}|=1\\), or equivalently, \\(|\\mathbf{x}|^2=1\\), \\(\\mathbf{x}^T\\mathbf{x}=1\\), or \\(x_1^2+x_2^2+\\cdots+x_n^2=1\\). The form \\(\\mathbf{x}^T\\mathbf{x}=1\\) is especially useful when working with quadratic forms. A constrained optimization problem for a quadratic form would be like the following: find the maximum and minimum values of the quadratic form \\(Q(\\mathbf{x})=7x_1^2+3x_2^2+2x_3^2\\), subject to the constraint \\(\\mathbf{x}^T\\mathbf{x}=1\\). Note that, since \\(x_2^2\\geq 0\\) and \\(x_3^2\\geq 0\\), we can write \\[\\begin{align*} Q(\\mathbf{x})&amp;=7x_1^2+3x_2^2+2x_3^2\\\\ &amp;\\leq 7x_1^2+7x_2^2+7x_3^2\\\\ &amp;=7(x_1^2+x_2^2+x_3^2)\\\\ &amp;=7|\\mathbf{x}|^2=7. \\end{align*}\\] We have an upper bound of 7, which is attained, since \\(Q(1,0,0)=Q(-1,0,0)=7\\). Our maximum value is 7. Similarly, \\[Q(\\mathbf{x})\\geq 2x_1^2+2x_2^2+2x_3^2=2(x_1^2+x_2^2+x_3^2)=2,\\] and \\(Q(0,0,1)=2\\), so the minimum value is 2. Our example quadratic form has the associated diagonal matrix \\[\\mathbf{A}=\\begin{bmatrix}7 &amp; 0 &amp; 0\\\\0 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 2\\end{bmatrix},\\] whose eigenvalues are 7, 3, and 2. Our maximum for \\(Q\\) was 7, the largest eigenvalue, and the minimum was 2, the smallest. This is true for all quadratic forms, not just diagonal ones[5]. Theorem 6.6 Let \\(\\mathbf{A}\\) be a symmetric \\(n\\times n\\) matrix and define \\(m=\\min\\{\\mathbf{x}^T\\mathbf{A}\\mathbf{x},\\mathbf{x}^T\\mathbf{x}=1\\}\\) and \\(M=\\max\\{\\mathbf{x}^T\\mathbf{A}\\mathbf{x},\\mathbf{x}^T\\mathbf{x}=1\\}\\). Then \\(M\\) is the greatest eigenvalue \\(\\lambda_1\\) of \\(\\mathbf{A}\\) and \\(m\\) is the least eigenvalue of \\(\\mathbf{A}, \\lambda_n.\\) The value of \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) is \\(M\\) when \\(\\mathbf{x}\\) is a unit eigenvector \\(\\mathbf{u}_1\\) of \\(\\mathbf{A}\\) corresponding to \\(M\\) and is \\(m\\) when \\(\\mathbf{x}\\) is a unit eigenvector of \\(\\mathbf{A}\\) corresponding to \\(m\\). The proof relies on orthogonal diagonalization and the fact that when \\(\\mathbf{P}\\) is orthogonal, \\(|\\mathbf{x}|=|\\mathbf{P}\\mathbf{y}|=|\\mathbf{y}|\\). So, a quadratic form is maximized in the direction of the eigenvector corresponding to the largest eigenvalue. What can we say about the second largest eigenvalue and its corresponding eigenvector? Theorem 6.7 Let \\(\\mathbf{A}, \\lambda_1\\), and \\(\\mathbf{u}_1\\) be as in the previous theorem. Then the maximum value of \\(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) subject to the constraints \\[\\mathbf{x}^T\\mathbf{x}=1,\\mathbf{x}^T\\mathbf{u}_1=0\\] is the second greatest eigenvalue \\(\\lambda_2\\), and this maximum is attained when \\(\\mathbf{x}\\) is an eigenvector \\(\\mathbf{u}_2\\) corresponding to \\(\\lambda_2\\)[5]. The new constraint \\(\\mathbf{x}^T\\mathbf{u}_1=0\\) means that we are basically eliminating the \\(\\mathbf{u}_1\\) direction, and optimizing whats left. This theorem can be generalized down to the smallest eigenvalue. Example 6.5 Theorem 6.7 isnt very exciting for \\(2\\times 2\\) matrices, because there are only 2 directions. Heres a \\(3\\times 3\\) example. The matrix \\[\\mathbf{A}=\\begin{bmatrix}11 &amp; -1 &amp; 2\\\\-1 &amp; 14 &amp; -1\\\\2 &amp; -1 &amp; 11\\end{bmatrix}\\] has eigenvalues \\(\\lambda_1=15,\\lambda_2=12,\\) and \\(\\lambda_3=9\\). Corresponding unit eigenvectors are \\[\\mathbf{u}_1=\\begin{bmatrix}1/\\sqrt{6}\\\\-2/\\sqrt{6}\\\\1/\\sqrt{6}\\end{bmatrix},\\mathbf{u}_2=\\begin{bmatrix}1/\\sqrt{3}\\\\1/\\sqrt{3}\\\\1/\\sqrt{3}\\end{bmatrix}, \\mathbf{u}_3=\\begin{bmatrix}-1/\\sqrt{2}\\\\0\\\\1/\\sqrt{2}\\end{bmatrix}.\\] For unit vectors, the maximum value of \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\) is \\(Q(\\mathbf{u}_1)=15\\). For unit vectors orthogonal to \\(\\mathbf{u}_1\\), the maximum value is \\(Q(\\mathbf{u}_2)=12\\). These theorems are important in principal component analysis, which we will see later in this chapter. In PCA, we diagonalize a symmetric covariance matrix, creating new, uncorrelated variables. These new variables can be listed in order of decreasing variance. 6.3 The Singular Value Decomposition Recall that an \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is diagonalizable if it can be written as \\[\\mathbf{A}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1},\\] where \\(\\mathbf{P}\\) is an invertible matrix and \\(\\mathbf{D}\\) is a diagonal matrix. This factorization tells us a lot about how multiplication by \\(\\mathbf{A}\\) transforms a vector \\(\\mathbf{x}.\\) We can write \\[\\mathbf{A}\\mathbf{x}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}\\mathbf{x}.\\] The product \\(\\mathbf{P}^{-1}\\mathbf{x}\\) is a change of variables. Multiplication by \\(\\mathbf{D}\\) then scales the resulting vector by different amounts in different directions, according to the eigenvalues of \\(\\mathbf{A}\\), which are the entries in \\(\\mathbf{D}\\). The final multiplication by \\(\\mathbf{P}\\) returns the result to the original basis. It turns out, we can do a similar analysis for any matrixeven non-square onesusing the singular value decomposition. Heres a motivating example. Let \\[\\mathbf{A}=\\begin{bmatrix}3 &amp; 0\\\\8 &amp; 3\\end{bmatrix}.\\] Find the maximum value of \\(|\\mathbf{A}\\mathbf{x}|\\) for \\(|\\mathbf{x}|=1\\). We can maximize \\(|\\mathbf{A}\\mathbf{x}|\\) by maximizing \\[|\\mathbf{A}\\mathbf{x}|^2=(\\mathbf{A}\\mathbf{x})\\cdot(\\mathbf{A}\\mathbf{x})=(\\mathbf{A}\\mathbf{x})^T(\\mathbf{A}\\mathbf{x})=\\mathbf{x}^T(\\mathbf{A}^T\\mathbf{A})\\mathbf{x}.\\] Since \\(\\mathbf{A}^T\\mathbf{A}\\) is symmetric, this is a quadratic form, which we can maximize by finding the largest eigenvalue. We have \\[\\mathbf{A}^T\\mathbf{A}=\\begin{bmatrix}3 &amp; 8\\\\0 &amp; 3\\end{bmatrix}\\begin{bmatrix}3 &amp; 0 \\\\8 &amp; 3\\end{bmatrix}=\\begin{bmatrix}73 &amp; 24\\\\24 &amp; 9\\end{bmatrix}.\\] The eigenvalues are \\(\\lambda_1=81\\) and \\(\\lambda_2=1\\). The corresponding eigenvectors are \\(\\mathbf{x}_1=(3,1)\\) and \\(\\mathbf{x}_2=(-1,3)\\), which we can make unit length by dividing by \\(\\sqrt{10}\\): \\[\\mathbf{v}_1=\\begin{bmatrix}3/\\sqrt{10}\\\\1/\\sqrt{10}\\end{bmatrix},\\mathbf{v}_2=\\begin{bmatrix}-1/\\sqrt{10}\\\\ 3/\\sqrt{10}\\end{bmatrix}.\\] The maximum value of \\(|\\mathbf{A}\\mathbf{x}|^2\\) on the unit circle is \\(81\\), which happens at \\((3/\\sqrt{10},1/\\sqrt{10})\\), so the maximum value of \\(|\\mathbf{A}\\mathbf{x}|\\) is \\(\\sqrt{81}=9\\). Meanwhile the minimum value of \\(|\\mathbf{A}\\mathbf{x}|\\) is \\(\\sqrt{1}=1\\), at the point \\((-1/\\sqrt{10},3/\\sqrt{10})\\). Geometrically speaking, multiplication by \\(\\mathbf{A}\\) takes points on the unit circle onto an ellipse. The point \\(\\mathbf{v}_1=(3/\\sqrt{10},1/\\sqrt{10})\\) gets mapped to the point \\[\\mathbf{A}\\mathbf{v}_1=\\begin{bmatrix}3 &amp; 0\\\\8 &amp; 3\\end{bmatrix}\\begin{bmatrix}3/\\sqrt{10}\\\\1/\\sqrt{10}\\end{bmatrix}=\\begin{bmatrix}9/\\sqrt{10}\\\\27/\\sqrt{10}\\end{bmatrix},\\] which is 9 units away from the origin, while \\[\\mathbf{A}\\mathbf{v}_2=\\begin{bmatrix}3 &amp; 0\\\\8 &amp; 3\\end{bmatrix}\\begin{bmatrix}-1/\\sqrt{10}\\\\3/\\sqrt{10}\\end{bmatrix}=\\begin{bmatrix}-3/\\sqrt{10}\\\\1/\\sqrt{10}\\end{bmatrix},\\] which is 1 unit away. (Note that \\(\\mathbf{A}(-\\mathbf{v}_1)\\) and \\(\\mathbf{A}(-\\mathbf{v}_2)\\) are also 9 and 1 units away from the origin, respectively.) Figure 6.2 shows the geometry. Figure 6.2: Multiplication by \\(\\mathbf{A}\\) Singular values For any \\(m\\times n\\) matrix \\(\\mathbf{A},\\) we can orthogonally diagonalize \\(\\mathbf{A}^T\\mathbf{A}\\). Let \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) be an orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(\\mathbf{A}^T\\mathbf{A}\\) with corresponding eigenvalues \\(\\lambda_1,\\dots, \\lambda_n\\). Note that \\[|\\mathbf{A}\\mathbf{v}_i|^2=\\mathbf{v}_i^T\\mathbf{A}^T\\mathbf{A}\\mathbf{v}_i=\\mathbf{v}_i^T(\\lambda_i\\mathbf{v}_i)=\\lambda_i(\\mathbf{v}_i^T\\mathbf{v}_i)=\\lambda_i.\\] Since \\(|\\mathbf{A}\\mathbf{v}_i|^2\\geq 0\\), we have \\(\\lambda_i\\geq 0\\) for all \\(i\\). We can renumber them, if necessary, in decreasing order, so that \\[\\lambda_1\\geq \\lambda_2\\geq \\cdots \\geq \\lambda_n\\geq 0.\\] The singular values of \\(\\mathbf{A}\\) are the square roots of these numbers, \\(\\sigma_i=\\sqrt{\\lambda_i}\\) and satisfy \\[\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq \\sigma_n\\geq 0.\\] In our example, not only were \\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_2\\) orthogonal, so were \\(\\mathbf{A}\\mathbf{v}_1\\) and \\(\\mathbf{A}\\mathbf{v}_2\\). This is not a coincidence. Heres the key theorem that leads to the singular value decomposition of a matrix[5]. Theorem 6.8 Suppose \\(\\{\\mathbf{v}_1,\\dots,\\mathbf{v}_n\\}\\) is an orthonormal basis for \\(\\mathbb{R}^n\\) consisting of eigenvectors of \\(\\mathbf{A}^T\\mathbf{A}\\) whose eigenvalues satisfy \\(\\lambda_1\\geq \\cdots \\geq \\lambda_n\\), and suppose that \\(\\mathbf{A}\\) has \\(r\\) nonzero singular values. Then \\(\\{\\mathbf{A}\\mathbf{v}_1,\\dots,\\mathbf{A}\\mathbf{v}_r\\}\\) is an orthogonal basis for \\(\\text{Col }\\mathbf{A}\\) and \\(\\text{rank }\\mathbf{A}=r\\). There are three results in this theorem: (1) the nonzero \\(\\mathbf{A}\\mathbf{v}_i\\)s form an orthogonal set, (2) they form a basis for the column space of \\(\\mathbf{A},\\) and (3) the rank of \\(\\mathbf{A}\\) is equal to the number of nonzero singular values of \\(\\mathbf{A}.\\) The orthogonal part is simple. Since \\(\\mathbf{v}_i\\) and \\(\\lambda_j\\mathbf{v}_j\\) are orthogonal for \\(i\\neq j\\) and \\(\\mathbf{v}_j\\) is an eigenvector of \\(\\mathbf{A}^T\\mathbf{A}\\), \\[(\\mathbf{A}\\mathbf{v}_i)^T(\\mathbf{A}\\mathbf{v}_j)=\\mathbf{v}_i^T(\\mathbf{A}^T\\mathbf{A})\\mathbf{v}_j=\\mathbf{v}_i^T(\\lambda_j\\mathbf{v}_j)=0.\\] The singular value decomposition of an \\(m\\times n\\) matrix with rank \\(r\\) involves a diagonal-like \\(m\\times n\\) matrix \\[\\Sigma=\\begin{bmatrix}\\mathbf{D} &amp; \\mathbf{0}\\\\\\mathbf{0} &amp; \\mathbf{0}\\end{bmatrix}.\\] The submatrix \\(\\mathbf{D}\\) is an \\(r\\times r\\) diagonal matrix consisting of the nonzero singular values of \\(\\mathbf{A}.\\) The rest of the matrix is filled with zeros, if necessary, to create the \\(m \\times n\\) matrix. For example, if \\(\\mathbf{A}\\) is a \\(3\\times 4\\) matrix of rank 2, with nonzero singular values 5 and 2, then \\[\\Sigma=\\begin{bmatrix} 5 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 2 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}.\\] When actually using the SVD, only \\(\\mathbf{D}\\) is needed, as we will see. Here is the theorem that presents the singular value decomposition[5]. Theorem 6.9 Let \\(\\mathbf{A}\\) be an \\(m\\times n\\) matrix with rank \\(r\\). Then there exists an \\(m\\times n\\) matrix \\(\\Sigma\\) as before, for which the diagonal entries in \\(\\mathbf{D}\\) are the \\(r\\) nonzero singular values of \\(\\mathbf{A},\\sigma_1\\geq \\sigma_2\\geq \\cdots \\geq \\sigma_r&gt;0\\), and there exist an \\(m\\times m\\) orthogonal matrix \\(\\mathbf{U}\\) and an \\(n\\times n\\) orthogonal matrix \\(\\mathbf{V}\\) such that \\[\\mathbf{A}=\\mathbf{U}\\Sigma\\mathbf{V}^T.\\] (Remember that for an orthogonal matrix \\(\\mathbf{V}, \\mathbf{V}^{-1}=\\mathbf{V}^T\\).) The columns of \\(\\mathbf{U}\\) are the left singular vectors of \\(\\mathbf{A},\\) while the columns of \\(\\mathbf{V}\\) are the right singular vectors. Except for \\(\\Sigma\\), the decomposition isnt unique. Weve already discussed where \\(\\Sigma\\) comes from. For \\(\\mathbf{V},\\) we use \\[\\mathbf{V}=\\begin{bmatrix} \\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{v}_n\\end{bmatrix},\\] where the \\(\\mathbf{v}_i\\)s are as before. Since \\(\\{\\mathbf{A}\\mathbf{v}_1,\\dots,\\mathbf{A}\\mathbf{v}_r\\}\\) is already an orthogonal basis for the column space of \\(\\mathbf{A},\\) we set \\[\\mathbf{u}_i=\\frac{1}{|\\mathbf{A}\\mathbf{v}_i|}\\mathbf{A}\\mathbf{v}_i=\\frac{1}{\\sigma_i}\\mathbf{A}\\mathbf{v}_i, 1\\leq i\\leq r.\\] If \\(r&lt;m\\), we need to extend this to a full orthonormal basis for \\(\\mathbb{R}^m,\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_r,\\mathbf{u}_{r+1},\\dots \\mathbf{u}_m\\}\\). (In practice, this step is usually not necessary, as the extra vectors contribute nothing to the final product.)3 This gives us \\(\\mathbf{U}=\\begin{bmatrix}\\mathbf{u}_1 &amp; \\mathbf{u}_2 &amp; \\cdots &amp; \\mathbf{u}_m\\end{bmatrix}\\). How does this factorization work? Note that \\[\\mathbf{A}\\mathbf{V}=\\begin{bmatrix}\\mathbf{A}\\mathbf{v}_1 &amp; \\cdots &amp; \\mathbf{A}\\mathbf{v}_n\\end{bmatrix}=\\begin{bmatrix}\\sigma_1\\mathbf{u}_1 &amp; \\cdots &amp; \\sigma_r \\mathbf{u}_r &amp; \\mathbf{0} &amp; \\cdots &amp;\\mathbf{0}\\end{bmatrix}.\\] Meanwhile, \\[\\begin{align*} \\mathbf{U}\\Sigma&amp;=\\begin{bmatrix}\\mathbf{u}_1 \\cdots \\mathbf{u}_m\\end{bmatrix} \\left[\\begin{array}{cccc|c}\\sigma_1 &amp;&amp;&amp; &amp;\\\\&amp; \\sigma_2 &amp;&amp;&amp;\\mathbf{0}\\\\&amp;&amp;\\ddots &amp;&amp;\\\\ &amp; &amp;&amp; \\sigma_r &amp; \\\\ \\hline &amp;&amp;\\mathbf{0}&amp;&amp;\\mathbf{0}\\end{array}\\right]\\\\ &amp;=\\begin{bmatrix}\\sigma_1\\mathbf{u}_1 &amp; \\cdots &amp; \\sigma_r\\mathbf{u}_r &amp; \\mathbf{0} &amp; \\cdots &amp; \\mathbf{0}\\end{bmatrix}\\\\ &amp;=\\mathbf{A}\\mathbf{V} \\end{align*}\\] Since \\(\\mathbf{U}\\Sigma=\\mathbf{A}\\mathbf{V}\\), and \\(\\mathbf{V}\\) is orthogonal, \\(\\mathbf{A}=\\mathbf{U}\\Sigma \\mathbf{V}^{-1}=\\mathbf{U}\\Sigma \\mathbf{V}^T.\\) Example 6.6 Lets return to our example from the beginning, with \\(\\mathbf{A}=\\begin{bmatrix}3 &amp; 0\\\\8 &amp; 3\\end{bmatrix}\\). The eigenvalues of \\(\\mathbf{A}^T\\mathbf{A}\\) are \\(\\lambda_1=81\\) and \\(\\lambda_2=1\\), so \\(\\sigma_1=9\\) and \\(\\sigma_2=1.\\) ( So \\(\\mathbf{A}\\) has rank 2.) The orthonormal eigenvectors are \\(\\mathbf{v}_1=(3/\\sqrt{10},1/\\sqrt{10})\\) and \\(\\mathbf{v}_2=(-1/\\sqrt{10},3/\\sqrt{10})\\). We have \\[\\mathbf{u}_1=\\frac{1}{\\sigma_1}\\mathbf{A}\\mathbf{v}_1=\\frac{1}{9}(9/\\sqrt{10},27/\\sqrt{10})=(1/\\sqrt{10},3/\\sqrt{10})\\] and \\[\\mathbf{u}_2=\\frac{1}{\\sigma_2}\\mathbf{A}\\mathbf{v}_2=(-3/\\sqrt{10},1/\\sqrt{10}).\\] Our singular value decomposition is \\[\\begin{align*} \\mathbf{A}&amp;=\\mathbf{U}\\Sigma\\mathbf{V}^T\\\\ &amp;=\\begin{bmatrix} 1/\\sqrt{10} &amp; -3\\sqrt{10}\\\\ 3\\sqrt{10} &amp; 1\\sqrt{10}\\end{bmatrix}\\begin{bmatrix}9 &amp; 0\\\\0 &amp; 1\\end{bmatrix}\\begin{bmatrix}3/\\sqrt{10} &amp; 1/\\sqrt{10}\\\\-1/\\sqrt{10} &amp; 3/\\sqrt{10}\\end{bmatrix}. \\end{align*}\\] In the standard SVD formulation \\[\\underbrace{\\mathbf{A}}_{m\\times n}=\\underbrace{\\mathbf{U}}_{m\\times m}\\underbrace{\\Sigma}_{m\\times n}\\underbrace{\\mathbf{V}^T}_{n\\times n}.\\] In practice, when \\(r&lt;m\\) and/or \\(r&lt;n\\), there may be columns of \\(\\mathbf{U}\\) or rows of \\(\\mathbf{V}^T\\) that arent needed, because theyll get zeroed out when multiplying by \\(\\Sigma\\). As a result, we can write \\[\\underbrace{\\mathbf{A}}_{m\\times n}=\\underbrace{\\mathbf{U}&#39;}_{m\\times r}\\underbrace{\\mathbf{D}}_{r\\times r}\\underbrace{\\mathbf{V}&#39;^T}_{r\\times n}.\\] This is particularly useful, because we dont have to worry about extending our \\(\\mathbf{u}_i\\) vectors to a full basis of \\(\\mathbb{R}^m\\). We can also approximate \\(\\mathbf{A}\\) with a lower rank matrix by eliminating some of the smaller singular values from \\(\\mathbf{D}.\\) Example 6.7 Let \\[\\mathbf{A}=\\begin{bmatrix} 17 &amp; 6\\\\22 &amp; 21\\\\10 &amp; 30\\end{bmatrix}.\\] We have \\[\\mathbf{A}^T\\mathbf{A}=\\begin{bmatrix}873 &amp; 864 \\\\864 &amp; 1377\\end{bmatrix},\\] which has eigenvalues \\(\\lambda_1=2025\\) and \\(\\lambda_2=225\\), and corresponding eigenvectors \\(\\mathbf{x}_1=(3,4)\\) and \\(\\mathbf{x}_2=(-4,3)\\), respectively. These normalize to \\(\\mathbf{v}_1=(3/5,4/5)\\) and \\(\\mathbf{v}_2=(-4/5,3/5)\\), respectively. The singular values are \\(\\sigma_1=\\sqrt{2025}=45\\) and \\(\\sigma_2=\\sqrt{225}=15\\). Our first two left singular vectors are \\[\\mathbf{u}_1=\\frac{1}{45}\\begin{bmatrix}17 &amp; 6\\\\22 &amp; 21\\\\10 &amp; 30\\end{bmatrix}\\begin{bmatrix}3/5\\\\4/5\\end{bmatrix}=\\begin{bmatrix}1/3\\\\2/3\\\\2/3\\end{bmatrix},\\] and \\[\\mathbf{u}_2=\\frac{1}{15}\\begin{bmatrix}17 &amp; 6\\\\22 &amp; 21\\\\10 &amp; 30\\end{bmatrix}\\begin{bmatrix}-4/5\\\\3/5\\end{bmatrix}=\\begin{bmatrix}-2/3\\\\-1/3\\\\2/3\\end{bmatrix}.\\] We could find a third orthogonal vector, but we dont need it. The full SVD for our matrix \\(\\mathbf{A},\\) with asterisks representing the unknown vector, is this: \\[\\mathbf{A}=\\begin{bmatrix} 17 &amp; 6\\\\22 &amp; 21\\\\10 &amp; 30\\end{bmatrix}=\\begin{bmatrix} 1/3 &amp; -2/3 &amp; *\\\\2/3 &amp; -1/3 &amp; *\\\\2/3 &amp; 2/3 &amp; *\\end{bmatrix}\\begin{bmatrix}45 &amp; 0\\\\0 &amp; 15 \\\\0 &amp; 0\\end{bmatrix} \\begin{bmatrix}3/5 &amp; 4/5\\\\-4/5 &amp; 3/5\\end{bmatrix},\\] but we can get away with writing \\[\\mathbf{A}=\\begin{bmatrix} 17 &amp; 6\\\\22 &amp; 21\\\\10 &amp; 30\\end{bmatrix}=\\begin{bmatrix} 1/3 &amp; -2/3 \\\\2/3 &amp; -1/3 \\\\2/3 &amp; 2/3 \\end{bmatrix}\\begin{bmatrix}45 &amp; 0\\\\0 &amp; 15\\end{bmatrix} \\begin{bmatrix}3/5 &amp; 4/5\\\\-4/5 &amp; 3/5\\end{bmatrix}.\\] SVD with R The R function SVD calculates the singular value decomposition for a matrix. The example in this chunk uses the matrix \\(\\mathbf{A}\\) from Example 6.7. A &lt;- matrix(c(17,22,10,6,21,30), nrow = 3) svd(A) ## $d ## [1] 45 15 ## ## $u ## [,1] [,2] ## [1,] -0.3333333 0.6666667 ## [2,] -0.6666667 0.3333333 ## [3,] -0.6666667 -0.6666667 ## ## $v ## [,1] [,2] ## [1,] -0.6 0.8 ## [2,] -0.8 -0.6 Note that \\(\\mathbf{u}_1\\) and \\(\\mathbf{v}_1\\) have the opposite signs from what we used. By default, the number of columns in \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) for an \\(m\\times n\\) matrix is the minimum of \\(m\\) and \\(n.\\) This can be changed by changing the nu or nv settings, respectively. Here, we set nu = 3 to get a third left singular vector. svd(A, nu = 3)$u ## [,1] [,2] [,3] ## [1,] -0.3333333 0.6666667 0.6666667 ## [2,] -0.6666667 0.3333333 -0.6666667 ## [3,] -0.6666667 -0.6666667 0.3333333 6.4 Applications of the SVD Many real-world data matrices are essentially rank-deficient, but due to noise they are technically full rank. We can use the singular value decomposition to approximate a matrix with a lower-rank matrix. In fact, the Eckart-Young-Mirsky Theorem[21] says that, for a given matrix \\(\\mathbf{A},\\) with singular value decomposition \\(\\mathbf{A}=\\mathbf{U}\\Sigma\\mathbf{V}^T\\), the matrix \\[\\mathbf{U}_k\\mathbf{D}_k\\mathbf{V}_k^T,\\] where \\(\\mathbf{U}_k=\\begin{bmatrix}\\mathbf{u}_1 &amp; \\cdots &amp; \\mathbf{u}_k\\end{bmatrix}\\), \\(\\mathbf{D}_k\\) is the \\(k\\times k\\) diagonal matrix with entries \\(\\sigma_1,\\dots,\\sigma_k\\), and \\(\\mathbf{V}_k=\\begin{bmatrix}\\mathbf{v}_1 &amp; \\cdots &amp; \\mathbf{v}_k\\end{bmatrix}\\) is the best rank-\\(k\\) approximation of \\(\\mathbf{A}.\\) This best approximation idea can be used to compress the sizes of images. The images in Figure 6.3 are of Hahns Peak in Colorado. The image on the right has its grayscale values stored in a \\(2016\\times 1512\\) matrix. Figure 6.3: Color and grayscale images of Hahns Peak If we look at the first hundred singular values of the grayscale matrix, we see that the first one is huge, but they get smaller quickly (Fig. 6.4). Figure 6.4: The singular values of the grayscale image Figure 6.5 shows the rank 2, 50, 100, and 200 approximations of the grayscale matrix, moving from left to right. The full rank image is on the bottom right. Figure 6.5: Rank 2, 50, 100, and 200 images with full rank version for comparison The original \\(2016\\times 1512\\) matrix contains 3,048,192 grayscale values. For the rank-200 version, \\(\\mathbf{U}_{200}\\) is a \\(2016\\times 200\\) matrix, \\(\\mathbf{D}_{200}\\) contains just the 200 largest singular values, and \\(\\mathbf{V}_{200}\\) is a \\(1512\\times 200\\) matrix, for a total of \\[2016\\times200+200+1512\\times200=705{,}800\\] stored values. Movie reviews and latent factors Consider the following collection of movie ratings. Movie 1 Movie 2 Movie 3 Movie 4 Movie 5 User 1 5 1 4 4 3 User 2 1 4 4 3 4 User 3 4 2 4 4 5 User 4 5 3 3 4 4 We are going to put these ratings in a matrix \\(\\mathbf{R},\\) with \\(r_{ij}\\) being User \\(i\\)s rating for movie \\(j.\\) We will use the singular value decomposition of the matrix \\(\\mathbf{R}\\) to analyze the ratings. \\[\\mathbf{R}=\\begin{bmatrix}5&amp;1&amp;4&amp;4&amp;3\\\\1&amp;4&amp;4&amp;3&amp;4\\\\4&amp;2&amp;4&amp;4&amp;5\\\\5&amp;3&amp;3&amp;4&amp;4\\end{bmatrix}\\] We will use R to do the heavy lifting for us. R &lt;- matrix(c(5,1,4,5,1,4,2,3, 4,4,4,3,4,3,4,4,3,4,5,4),nrow=4) svd(R) ## $d ## [1] 16.114148 3.744072 1.497255 1.036521 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] 0.4904799 0.51873885 0.4468865 0.53910294 ## [2,] 0.4301572 -0.83671990 0.1141805 0.31910404 ## [3,] 0.5414149 0.02460074 0.3164221 -0.77855112 ## [4,] 0.5303435 0.17379475 -0.8289345 0.03740046 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] 0.4778368 0.72764379 -0.35422907 0.08433651 ## [2,] 0.3031478 -0.60296777 -0.63472869 0.35755979 ## [3,] 0.4616588 -0.17417852 0.68335245 0.41564503 ## [4,] 0.4678761 0.09571868 0.05345639 0.14386708 ## [5,] 0.4977313 -0.25973865 0.04258056 -0.81949974 Note that the fifth column of \\(\\mathbf{V},\\) which corresponds to the 5th right singular vector of \\(\\mathbf{R}\\) isnt displayed. It does not contribute to the overall product. We can use the SVD to decompose an individual rating. Consider User 3s rating of Movie 2: \\(r_{32}=2.\\) Working from right to left: The product \\(\\mathbf{D}\\mathbf{V}^T\\) scales the rows of \\(\\mathbf{V}^T\\) by the corresponding singular values. The entry \\(r_{32}\\) is then the dot product of row 3 of \\(\\mathbf{U}\\) and column 2 of \\(\\mathbf{D}\\mathbf{V}^T.\\) \\[\\begin{bmatrix} &amp;&amp;&amp;\\\\&amp;&amp;&amp;\\\\&amp;2&amp;&amp;\\\\&amp;&amp;&amp;\\end{bmatrix}=\\begin{bmatrix}&amp;&amp;&amp;\\\\&amp;&amp;&amp;\\\\0.54 &amp; 0.02 &amp; 0.32 &amp; -0.78\\\\&amp;&amp;&amp;\\end{bmatrix}\\begin{bmatrix}&amp;16.1\\cdot 0.30 &amp;&amp;&amp;\\\\&amp;3.7\\cdot (-0.60)&amp;&amp;&amp;\\\\&amp;1.5\\cdot (-0.63)&amp;&amp;&amp;\\\\&amp;1.0\\cdot 0.36&amp;&amp;&amp;\\end{bmatrix}\\] How User 3s rating of a movie differs from any other users rating is determined by the values in row 3, columns 1-4 of \\(\\mathbf{U}\\). We can call these columns user latent factors. Looking at column 1 (latent factor 1) of \\(\\mathbf{U},\\) we dont see much variation among the values. Latent factor 2 seems to separate the users quite a bit, with a big negative number for User 2. The third latent factor seems to separate User 4 from the rest, while the fourth separates User 3. We can do a similar analysis with the rows of \\(\\mathbf{V}^T,\\) which are the columns of \\(\\mathbf{V}.\\) We can call these the movie or item latent factors. As with the first user latent factor, the first movie latent factor values are fairly uniform. The second, third, and fourth latent factors do show some differences between the movies. Note that the user and item latent factors are matched up with a singular value. The first singular value is by far the largest, and that factor pair tends to contribute the most to the overall rating. The singular values basically tell us how important each user/item latent factor pair is. If we center the ratings by subtracting the movie mean from each column, we get the following SVD. scale(R, scale = FALSE) #centers the columns of R ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.25 -1.5 0.25 0.25 -1 ## [2,] -2.75 1.5 0.25 -0.75 0 ## [3,] 0.25 -0.5 0.25 0.25 1 ## [4,] 1.25 0.5 -0.75 0.25 0 ## attr(,&quot;scaled:center&quot;) ## [1] 3.75 2.50 3.75 3.75 4.00 svd(scale(R, scale = FALSE)) ## $d ## [1] 3.868878e+00 1.599410e+00 1.312886e+00 9.227372e-17 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] 0.4984524 -0.65140307 -0.2778834 -0.5 ## [2,] -0.8297467 -0.17701836 -0.1737382 -0.5 ## [3,] 0.1016640 0.09703764 0.8545456 -0.5 ## [4,] 0.2296302 0.73138379 -0.4029240 -0.5 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] 0.83159063 0.3820382 -0.1215589 0.38338893 ## [2,] -0.49841726 0.6432060 -0.3599069 0.34707557 ## [3,] -0.05935318 -0.4572834 0.3068994 0.71230782 ## [4,] 0.21446701 0.1106773 0.1323330 -0.47417231 ## [5,] -0.10255904 0.4679480 0.8625492 0.01815668 The first latent factors are now more interesting. Note that the fourth singular value is essentially zero. When we center the ratings matrix we potentially reduce the rank by one, because the columns now add to 0. 6.5 Principal Component Analysis Principal Component Analysis begins with the covariance matrix for a data set. We saw this matrix briefly in Section 5.4. Suppose we have the following height-weight data for 5 individuals. ht (cm) wt (kg) 150 50 154 56 165 64 183 82 178 98 The average height is 166 cm, while the average weight is 70 kg. When we subtract the column means and put them in a matrix, we get something called mean-deviation form. \\[\\mathbf{B}=\\begin{bmatrix}(150-166) &amp; (50-70)\\\\(154-166) &amp; (56-70)\\\\(165-166) &amp; (64-70)\\\\(183-166) &amp; (82-70)\\\\(178-166) &amp; (98-70)\\end{bmatrix}=\\left[\\begin{array}{rr} -16 &amp; -20\\\\-12 &amp; -14\\\\-1 &amp; -6\\\\ 17 &amp; 12\\\\12 &amp; 28\\end{array}\\right].\\] Note: many discussions of PCA have the transpose of this matrix. We are using this orientation to be consistent with Rs use of columns to represent variables. The sample covariance matrix will end up being the same. Once we have our data in the matrix \\(\\mathbf{B}\\) in mean-deviation form, we can find the sample covariance matrix \\(\\mathbf{S}:\\) \\[\\mathbf{S}=\\frac{1}{n-1}\\mathbf{B}^T\\mathbf{B},\\] where \\(n\\) is the sample size. In our example \\[\\mathbf{S}=\\frac{1}{5-1}\\begin{bmatrix} -16 &amp; -12 &amp; -1 &amp; 17 &amp; 12\\\\-20 &amp; -14 &amp; -6 &amp; 12 &amp; 28\\end{bmatrix}\\left[\\begin{array}{rr} -16 &amp; -20\\\\-12 &amp; -14\\\\-1 &amp; -6\\\\ 17 &amp; 12\\\\12 &amp; 28\\end{array}\\right]=\\begin{bmatrix}208.5 &amp; 258.5\\\\258.5 &amp; 390\\end{bmatrix}.\\] The numbers along the diagonal are the sample variances of the heights (208.5 cm2) and weights (390 kg2). The value \\(s_{12}=s_{21}=258.5 \\text{ cm}\\cdot\\text{kg}\\) is the sample covariance between height and weight. We get the correlation between these two by dividing by the product of their standard deviations (the square roots of the variances). \\[r=\\frac{258.5}{\\sqrt{208.5\\cdot 390}}\\approx 0.9065.\\] In general, when we have \\(n\\) observations of \\(p\\) variables, we put the data in an \\(n\\times p\\) matrix, subtract off the mean from each column to get it into mean-deviation form \\(\\mathbf{B},\\) and then find the \\(p\\times p\\) covariance matrix \\[\\mathbf{S}=\\frac{1}{n-1}\\mathbf{B}^T\\mathbf{B}.\\] The diagonal entries of \\(\\mathbf{S},\\) \\(s_{ii}\\), will be the individual variable variances. An off-diagonal entry \\(s_{ij}\\) will be the covariance between variable \\(i\\) and variable \\(j\\). The trace of \\(\\mathbf{S}\\) will be the sum of the variances, called the total variance of the data. Since the matrix \\(\\mathbf{S}=\\frac{1}{n-1}\\mathbf{B}^T\\mathbf{B}\\) is symmetric, it can be orthogonally diagonalized \\[\\mathbf{S}=\\mathbf{P}\\mathbf{D}\\mathbf{P}^T,\\] where \\[\\mathbf{D}=\\begin{bmatrix} \\lambda_1 &amp; &amp;&amp;\\\\ &amp; \\lambda_2 &amp;&amp;\\\\ &amp;&amp;\\ddots &amp;\\\\ &amp;&amp;&amp; \\lambda_p\\end{bmatrix},\\] with \\(\\lambda_1\\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_p\\geq 0\\). (This is because \\(\\mathbf{S}\\) is a positive multiple of \\(\\mathbf{B}^T\\mathbf{B}\\), and we saw with the singular value decomposition that matrices of this form had nonnegative eigenvalues.) The principal components of the original data matrix are the columns of \\(\\mathbf{P},\\) \\(\\{\\mathbf{u}_1,\\dots, \\mathbf{u}_p\\}\\). The \\(i\\)th principal component is the eigenvector corresponding to \\(\\lambda_i\\), the \\(i\\)th largest eigenvalue. We have taken our original set of \\(p\\) centered variables \\(\\{x_1,\\dots,x_p\\}\\) and created from them, via a change of variables, a new set of variables \\(\\{y_1,\\dots,y_p\\}\\), where \\[\\begin{bmatrix}x_1\\\\ \\vdots \\\\ x_p\\end{bmatrix}=\\mathbf{P}\\begin{bmatrix} y_1\\\\ \\vdots \\\\ y_p\\end{bmatrix}.\\] We can write \\[\\begin{bmatrix} y_1\\\\ \\vdots \\\\ y_p\\end{bmatrix}=\\mathbf{P}^T\\begin{bmatrix}x_1\\\\ \\vdots \\\\ x_p\\end{bmatrix}=\\begin{bmatrix} \\mathbf{u}_1^T\\\\ \\vdots \\\\ \\mathbf{u}_p^T\\end{bmatrix}\\begin{bmatrix}x_1\\\\ \\vdots \\\\ x_p\\end{bmatrix}.\\] If the components of \\(\\mathbf{u}_1\\) are \\(c_1,\\dots,c_p\\), then \\(y_1=c_1x_1+c_2x_2+\\cdots+c_px_p\\). That is, the components of \\(\\mathbf{u}_1\\) tell us how to write the new variable \\(y_1\\) as a linear combination of \\(x_1,\\dots,x_p\\). Similarly, the components of \\(\\mathbf{u}_i\\) tell us how to write \\(y_i\\) in terms of \\(x_1,\\dots,x_p\\). The most important part is that the diagonal matrix \\(\\mathbf{D}=\\mathbf{P}^T\\mathbf{S}\\mathbf{P}\\) is the covariance matrix for our new variables, which means that our new variables are uncorrelated! Two more things: The eigenvalue \\(\\lambda_i\\) is the variance of \\(y_i\\), so of all the new variables, \\(y_1\\) has the greatest variance, \\(y_2\\) has the second greatest, and so on. Recall that the trace of \\(\\mathbf{S}\\) was the total variance, and it is also the sum of the eigenvalues. The matrix \\(\\mathbf{D}\\) has the same total variance. Our height-weight example had the covariance matrix \\[\\mathbf{S}=\\begin{bmatrix} 208.5 &amp; 258.5\\\\258.5 &amp; 390\\end{bmatrix}.\\] Its eigenvalues are (rounded) \\(\\lambda_1=573.2\\) and \\(\\lambda_2=25.3\\), and corresponding unit eigenvectors are \\[\\mathbf{u}_1=\\begin{bmatrix}0.578 \\\\0.816\\end{bmatrix} \\text{ and } \\mathbf{u}_2=\\begin{bmatrix} -0.816\\\\0.578\\end{bmatrix}.\\] If we call \\(\\hat{h}\\) and \\(\\hat{w}\\) the centered height and weight, respectively, then our new variables are \\[y_1=0.578\\hat{h}+0.816\\hat{w} \\text{ and }y_2=-0.816\\hat{h}+0.578\\hat{w}.\\] For example, the \\((h,w)=(150,50)\\leadsto (\\hat{h},\\hat{w})=(-16,-20)\\) individual will have \\[y_1=0.578(-16)+0.816(-20)=-25.568\\] and \\[y_2=-0.816(-16)+0.578(-20)=1.496.\\] The eigenvalues again were \\(\\lambda_1=573.2\\) and \\(\\lambda_2=25.3\\). These correspond to the variance of the \\(y_1\\) and \\(y_2\\) variables, respectively. That means that \\[\\frac{573.2}{573.2+25.3}\\cdot 100\\%=95.77\\%\\] of the total variance comes from \\(y_1\\), with the remaining 4.23% coming from \\(y_2\\). PCA in R There are two base R functions that will run PCA on a matrix or data frame: prcomp and princomp. The output is slightly different, and princomp divides \\(\\mathbf{B}^T\\mathbf{B}\\) by \\(n\\) instead of \\(n-1\\). (This is the population covariance matrix.) This doesnt change the eigenvectors, but it does scale the variances. We will look at the output from prcomp. There are other technical differences that we wont discuss. h &lt;- c(150,154,165,183,178) w &lt;- c(50,56,64,82,98) hw &lt;- data.frame(h,w) hw.pca &lt;- prcomp(hw) hw.pca ## Standard deviations (1, .., p=2): ## [1] 23.941947 5.028239 ## ## Rotation (n x k) = (2 x 2): ## PC1 PC2 ## h 0.5782541 0.8158568 ## w 0.8158568 -0.5782541 Here we have the standard deviations of the new variables (square roots of the variances) and the eigenvectors or rotations, which is what prcomp calls the principal components. The princomp function calls them loadings. The summary function will give the proportion of the total variance for each variable. The plot function will produce a scree plot of the variances (Fig. 6.6). summary(hw.pca) ## Importance of components: ## PC1 PC2 ## Standard deviation 23.9419 5.02824 ## Proportion of Variance 0.9578 0.04224 ## Cumulative Proportion 0.9578 1.00000 plot(hw.pca) Figure 6.6: A (bar) scree plot. They can also be line plots. When we calculated the \\(y_1\\) and \\(y_2\\) values for the 150-cm,50-kg individual, we calculated the scores for that individual. For prcomp output, the scores are in the $x variable. hw.pca$x ## PC1 PC2 ## [1,] -25.569200 -1.488627 ## [2,] -18.361043 -1.694724 ## [3,] -5.473395 2.653668 ## [4,] 19.620600 6.930516 ## [5,] 29.783038 -6.400832 The covariance matrix with these new scores shows us that the new variables are uncorrelated. (The covariances are essentially zero.) cov(hw.pca$x) ## PC1 PC2 ## PC1 5.732168e+02 -1.768828e-13 ## PC2 -1.768828e-13 2.528319e+01 Biplots We can represent the results of our Principal component analysis two dimensionally in what is known as a biplot. A biplot combines the new scores for the first two principal components along with vector representations of the original variables. There are different ways to do this. biplot(hw.pca, scale = 0) Figure 6.7: A biplot with scale = 0 In Figure 6.7, the left and bottom scales give the PC1 and PC2 scores for the observations. The top and right scales give the PC1 and PC2 rotations for the original variables, represented as vectors. biplot(hw.pca, scale = 1) Figure 6.8: A biplot with scale = 1 When scale = 1 (the default), the observations and variable vectors are scaled. The main effect of the scaling is that we see how much each variable contributes to the first two principal components. In Figure 6.8, we see that the two original variables are pointing in similar directions, implying some degree of correlation. The weight contributes more, especially towards PC1. This is primarily due to the fact that weight has a higher variance than height in the data. Example 6.8 The iris data frame that comes with R has 4 measurements on 150 iris flowers, 50 each from 3 species. # The fifth column of iris contains the species names iris.pca &lt;- prcomp(iris[,-5]) iris.pca ## Standard deviations (1, .., p=4): ## [1] 2.0562689 0.4926162 0.2796596 0.1543862 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 summary(iris.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.0563 0.49262 0.2797 0.15439 ## Proportion of Variance 0.9246 0.05307 0.0171 0.00521 ## Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 The summary tells us that the first two principal components account for more than 97% of the total variance. Also see the scree plot (Fig. 6.9). # Changing to a line plot for this scree plot. plot(iris.pca, type=&quot;l&quot;) Figure 6.9: The scree plot for the iris data The ggfortify package has an autoplot function that will produce a biplot from PCA output. library(ggfortify) autoplot(iris.pca, loadings = TRUE, loadings.label = TRUE, data = iris, color=&quot;Species&quot;) Figure 6.10: A biplot using ggfortify The biplot in Figure 6.10 shows us a few things. The first two principal components do a good job of separating the three species. Petal width and petal length are strongly correlated. Sepal width has a very weak correlation with the petal measurements, at least as far as PCA1 and PCA2 are concerned. We can use the frame = TRUE setting to draw polygons around the species, again showing that there is good separation (Fig. 6.11). autoplot(iris.pca, loadings = TRUE, loadings.label = TRUE, data = iris, color=&quot;Species&quot;, frame = TRUE,) Figure 6.11: A biplot with framing polygons Scaling When we perform PCA on the mtcars data frame with the am transmission type variable removed, we get the following summary. mt.pca &lt;- prcomp(mtcars[,-9]) summary(mt.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 136.533 38.14744 3.06645 1.29508 0.90482 0.65061 0.3055 ## Proportion of Variance 0.927 0.07237 0.00047 0.00008 0.00004 0.00002 0.0000 ## Cumulative Proportion 0.927 0.99937 0.99984 0.99992 0.99997 0.99999 1.0000 ## PC8 PC9 PC10 ## Standard deviation 0.2859 0.2328 0.21 ## Proportion of Variance 0.0000 0.0000 0.00 ## Cumulative Proportion 1.0000 1.0000 1.00 Even with ten original variables, the first two principal components account for a whopping 99.9% of the total variance. The biplot (Fig. 6.12) is revealing. autoplot(mt.pca, labels = FALSE, loadings = TRUE, loadings.label = TRUE) Figure 6.12: PCA on the unscaled mtcars data frame The engine displacement (disp) and horsepower (hp) variables dominate the first two principal components. Why? Look at the variances. apply(mtcars, MARGIN = 2, FUN = var) ## mpg cyl disp hp drat wt ## 3.632410e+01 3.189516e+00 1.536080e+04 4.700867e+03 2.858814e-01 9.573790e-01 ## qsec vs am gear carb ## 3.193166e+00 2.540323e-01 2.489919e-01 5.443548e-01 2.608871e+00 If we look at all of the variables, those two have by far the highest variances, so it is not surprising that they would dominate the first two principal components. If youre going to use PCA for dimension reduction or any prediction purposes, you should scale your variables so that the ones that take on larger values dont have an outsized impact on the results. The scale = TRUE setting scales the variables by subtracting the mean and dividing by the standard deviation. When we do that, we get more useful, though less dramatic results. mt.pca2 &lt;- prcomp(mtcars[,-9], scale = TRUE) summary(mt.pca2) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.5087 1.4696 0.77481 0.51912 0.47244 0.41776 0.3507 ## Proportion of Variance 0.6293 0.2160 0.06003 0.02695 0.02232 0.01745 0.0123 ## Cumulative Proportion 0.6293 0.8453 0.90536 0.93231 0.95463 0.97208 0.9844 ## PC8 PC9 PC10 ## Standard deviation 0.27812 0.23803 0.14896 ## Proportion of Variance 0.00773 0.00567 0.00222 ## Cumulative Proportion 0.99212 0.99778 1.00000 It now takes the first 5 principal components before we get above 95% of the total variance. We left out the am variable so we could see how well we can predict that value using PCA on the other variables. mtcars2 &lt;- mtcars mtcars2$am &lt;- as.factor(mtcars2$am) autoplot(mt.pca2, loadings = TRUE, loadings.label = TRUE, data = mtcars2, color=&quot;am&quot;, frame = TRUE,) Figure 6.13: Using PCA to guess transmission type As we see in Figure 6.13, it only takes two principal components to separate the two transmission types completely. Again, it is generally a good idea to scale your variables when you perform PCA on your data so that individual variables dont have excessive influence on the results. 6.6 Exercises Diagonalize \\(\\mathbf{A}=\\begin{bmatrix}5 &amp; -6\\\\4 &amp; -5\\end{bmatrix}.\\) Diagonalize \\(\\mathbf{B}=\\begin{bmatrix}4 &amp; -1\\\\6 &amp; -1\\end{bmatrix}.\\) Use your answer to Problem 1 to find \\(\\mathbf{A}^4.\\) Use your answer to Problem 2 to find \\(\\mathbf{B}^4.\\) Diagonalize \\(\\mathbf{A}=\\begin{bmatrix}0 &amp; 2\\\\2 &amp; 0\\end{bmatrix}.\\) Verify that \\(\\mathbf{A}\\) is orthogonally diagonalizable. Diagonalize \\(\\mathbf{B}=\\begin{bmatrix}-7 &amp; 24\\\\24 &amp; 7\\end{bmatrix}.\\) Verify that \\(\\mathbf{B}\\) is orthogonally diagonalizable. Let \\(\\mathbf{A}=\\begin{bmatrix}5&amp;2\\\\2&amp;4\\end{bmatrix}.\\) Let \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Write \\(Q(\\mathbf{x})\\) as a function of \\(x_1\\) and \\(x_2.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Let \\(\\mathbf{A}=\\begin{bmatrix}3&amp;1\\\\1&amp;4\\end{bmatrix}.\\) Let \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Write \\(Q(\\mathbf{x})\\) as a function of \\(x_1\\) and \\(x_2.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Let \\(Q(x_1,x_2)=2x_1^2-3x_1x_2+4x_2^2.\\) Find a matrix \\(\\mathbf{A}\\) such that \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Classify \\(Q\\) as a quadratic form. That is, is it positive definite, negative definite, indefinite, or something else? Let \\(Q(x_1,x_2)=2x_1^2-3x_1x_2+4x_2^2.\\) Find a matrix \\(\\mathbf{A}\\) such that \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Classify \\(Q\\) as a quadratic form. That is, is it positive definite, negative definite, indefinite, or something else? Let \\(Q(x_1,x_2,x_3)=2x_1^2+3x_2^2-3x_3^2+4x_1x_2-2x_1x_3+6x_2x_3.\\) Find a matrix \\(\\mathbf{A}\\) such that \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Classify \\(Q\\) as a quadratic form. That is, is it positive definite, negative definite, indefinite, or something else? Let \\(Q(x_1,x_2,x_3)=4x_1^2-3x_2^2-5x_3^2-2x_1x_2+6x_1x_3+8x_2x_3.\\) Find a matrix \\(\\mathbf{A}\\) such that \\(Q(\\mathbf{x})=\\mathbf{x}^T\\mathbf{A}\\mathbf{x}.\\) Find the maximum and minimum values of \\(Q(\\mathbf{x})\\) on the circle \\(\\mathbf{x}^T\\mathbf{x}=1.\\) Classify \\(Q\\) as a quadratic form. That is, is it positive definite, negative definite, indefinite, or something else? Let \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 2\\\\-2 &amp; -2\\end{bmatrix}.\\) Find the singular value decomposition of \\(\\mathbf{A}\\) by hand. Let \\(\\mathbf{B}=\\begin{bmatrix}1 &amp; 3\\\\-3 &amp; 1\\end{bmatrix}.\\) Find the singular value decomposition of \\(\\mathbf{B}\\) by hand. Let \\(\\mathbf{A}=\\begin{bmatrix}2 &amp; 4 &amp; 0\\\\-4 &amp; -2 &amp; 0\\end{bmatrix}.\\) Find the singular value decomposition of \\(\\mathbf{A}\\) without using a singular value decomposition function. Let \\(\\mathbf{B}=\\begin{bmatrix}11 &amp; 10 &amp; -2\\\\-2 &amp; 5 &amp; 14\\end{bmatrix}.\\) Find the singular value decomposition of \\(\\mathbf{B}\\) without using a singular value decomposition function. The table below contains ratings of four movies from six reviewers[22]. Enter the scores in a \\(6\\times 4\\) matrix, and apply the svd function to the centered values. The columns of \\(\\mathbf{U}\\) represent the user latent factors. Which users have the most different values with respect to the first latent factor? Which users have the most different values with respect to the second latent factor? User American Beauty (1999) Apollo 13 (1995) Forrest Gump (1994) Jurassic Park (1993) 1 4.00 3.50 5.00 4.50 2 5.00 3.00 3.50 3.50 3 4.00 5.00 4.00 4.00 4 4.00 4.00 5.00 3.50 5 5.00 2.50 5.00 3.50 6 4.00 3.00 5.00 4.50 In the svd output from Problem 17, the columns of \\(\\mathbf{V}\\) represent the item or movie latent factors. For latent factor 1, which two movies have the most different values? For latent factor 2, which two movies have the most different values? The imager package for R has some image processing features and comes with some nice features. Warning: installation of the package can sometimes be an issue. One image that comes with the package is called boats. Its grayscale matrix is 256 x 384. How many numbers are in a 256 x 384 matrix? If you use the first 75 singular values to compress the image, how many numbers are involved? Use the code below to create and plot that image that uses the first 75 singular values. If you cant get imager to work on your device, the code should work at https://rdrr.io/snippets. library(imager) pic &lt;- grayscale(boats) #creates a grayscale cimg from original pic.m &lt;- as.matrix(pic) #extracts the matrix portion pic.svd &lt;- svd(pic.m) #calculates the SVD k &lt;- 75 #the number of singular values used #The next code produces the rank-k image matrix&quot; compressed &lt;- pic.svd$u[, 1:k] %*% diag(pic.svd$d[1:k]) %*% t(pic.svd$v[, 1:k]) plot(as.cimg(compressed),axes=F) # One note: the plot produced by default will be a PNG file, so # the size of the &quot;compressed&quot; image might actually be larger # than the original. Another image available through imager is called coins. You can access it by executing the command pic &lt;- load.example(\"coins\"). Modify the code in Problem 19 to complete the following. Once you have created the grayscale matrix, apply the str function to it to determine the dimensions of the matrix. How many numbers are stored in the matrix? If you use the first 75 singular values to compress the image, how many numbers are involved? Create and plot the image that uses the first 75 singular values. For the movie ratings data below, with the movies as variables, find the following. Do not scale or center the data. The covariance matrix. Use Rs eigen function to find the eigenvalues of the covariance matrix. What fraction of the total variance comes from the first principal component? Find the scores for User 1. (Try to do this without using the prcomp function.) User American Beauty (1999) Apollo 13 (1995) 1 4.00 3.50 2 5.00 3.00 3 4.00 5.00 4 4.00 4.00 5 5.00 2.50 For the movie ratings data below, with the movies as variables, find the following. Do not scale or center the data. The covariance matrix. Use Rs eigen function to find the eigenvalues and eigenvectors of the covariance matrix. What fraction of the total variance comes from the first principal component? Find the scores for User 1. (Try to do this without using the prcomp function.) User Forrest Gump (1994) Jurassic Park (1993) 1 5.00 4.50 2 3.50 3.50 3 4.00 4.00 4 5.00 3.50 5 5.00 3.50 The MASS package has the Pima.tr data frame that we used in Chapter 5. The 8th column is the type factor variable. Use the prcomp function on the data without the type variable. Make sure you scale the data. How many principal components are needed to get the cumulative variance above 95% of the total? Use the autoplot function from the ggfortify package to create a biplot with framing polygons colored by type. The state.x77 data set has data from the 50 U.S. states published by the U.S. Census Bureau in 1977. To get the data set in to data frame format, evaluate states &lt;- data.frame(state.x77). Use the prcomp function on the data. Make sure you scale the data. How many principal components are needed to get the cumulative variance above 95% of the total? Add the region variable to the data frame by evaluating states$region &lt;- state.region. Use the autoplot function from the ggfortify package to create a biplot with framing polygons colored by region. References 5. Lay DC, Lay SR, McDonald JJ (2015) Linear algebra and its applications, Pearson Education. 20. Strang G (2022) Introduction to linear algebra, 6th ed., Wellesley Cambridge Press. 21. Eckart C, Young G (1936) The approximation of one matrix by another of lower rank. Psychometrika 1: 211218. 22. Hahsler M (2022) Recommenderlab: An r framework for developing and testing recommendation algorithms. Another option is finding eigenvectors of \\(\\mathbf{A}\\mathbf{A}^T\\) corresponding to the eigenvalue 0. In fact, some sources [20] get the left singular vectors from \\(\\mathbf{A}\\mathbf{A}^T\\) and use them to find \\(r\\) right singular vectors. "],["additional-topics.html", "Chapter 7 Additional Topics 7.1 \\(k\\)-means Clustering 7.2 Collaborative Filtering 7.3 Decision Tree Classification 7.4 Support Vector Machines 7.5 Exercises", " Chapter 7 Additional Topics This chapter contains some additional topics that dont quite fit in one of the earlier chapters. 7.1 \\(k\\)-means Clustering Clustering is a machine learning method of grouping objects for classification purposes. The scatterplot in Figure 7.1 shows the petal lengths and widths of 150 irises, collected into three groups based on those measurements, by a method called k-means clustering. Figure 7.1: Iris flowers clustering based on two measurements As an example, here are eight Sandra Bullock movies from the 1990s and early 2000s, with their IMDB.com[23] ratings and domestic box office receipts in millions. Movie Rating Box Office 28 Days 6.1 37.2 A Time to Kill 7.5 108.8 Miss Congeniality 6.3 106.8 The Net 6.0 50.7 Practical Magic 6.3 46.7 Speed 7.3 121.2 Speed 2 3.9 48.6 While You Were Sleeping 6.8 81.1 We are going to try to group these movies based upon these two attributes. Figure 7.2 is a scatterplot of the data. It appears that SPEED and SPEED 2 are not going to end up in the same cluster. Figure 7.2: IMDB ratings and box office Receipts for eight Sandra Bullock films \\(k\\)-means clustering The \\(k\\)-means clustering process can work with any number of variables on a set of individuals. Each one of \\(m\\) individuals consists of an \\(n\\)-dimensional vector \\[(x_{i1},x_{i2},\\dots,x_{in}),i=1\\dots m.\\] The Bullock data has \\(m=8\\) individuals (the movies) and \\(n=2\\) variables. Our procedure will involve finding the center or mean of a collection of vectors. The mean of a set of vectors is simply another vector whose components are the individual component (variable) means. For example, if we have the vectors \\[\\mathbf{X}_1=(1,2,-2),\\mathbf{X}_2=(3,1,5),\\text{ and } \\mathbf{X}_3=(2,-1,4),\\] then \\[\\bar{\\mathbf{X}}=\\left(\\frac{1+3+2}{3},\\frac{2+1+-1}{3},\\frac{-2+5+4}{3}\\right)=\\left(2,\\frac{2}{3},\\frac{7}{3}\\right).\\] Heres the process. \\(k\\)-means clustering Randomly assign the individuals to one of \\(k\\) clusters. Calculate the center of each cluster. It will be the mean of all the individuals in that cluster. (The original centers can also be pre-defined, skipping Step 1.) Find the distance from each individual to every one of the \\(k\\) centers. Reassign each individual to the cluster with the nearest center. If the cluster assignments have not changed, the process ends. Otherwise, return to Step 2. Here is the process with the Sandra Bullock data. First, we randomly assign the initial clusters: Movie IMDB Rating Box Office Cluster 28 Days 6.1 37.2 3 A Time to Kill 7.5 108.8 1 Miss Congeniality 6.3 106.8 1 The Net 6.0 50.7 2 Practical Magic 6.3 46.7 2 Speed 7.3 121.2 1 Speed 2 3.9 48.6 3 While You Were Sleeping 6.8 81.1 2 The mean rating for Cluster 1 is \\((7.5+6.3+7.3)/3=7.033.\\) Similarly, the mean box office total for Cluster 1 is \\((108.8+106.8+121.2)/3=112.267.\\) The means for both variables for all clusters are as follows. Cluster Rating Mean Box Office Mean 1 7.033 112.267 2 6.367 59.500 3 5.000 42.900 When we show the cluster centers on the scatterplot, it looks like Figure 7.3. Figure 7.3: Cluster centers (squares) To find the distance from an individual to a center, we use the distance formula. The distance from 28 DAYS (Rating, Box Office) \\(=(6.1,37.2)\\) to Center 1 \\((7.033,112.267)\\) is \\[\\sqrt{(6.1-7.033)^2+(37.2-112.267)^2}\\approx 75.073.\\] We do this for all 8 movies and all 3 centers and get the following results. Movie Rating Box Office Old Cluster D1 D2 D3 New Cluster 28 Days 6.1 37.2 3 75.07 22.30 5.81 3 A Time to Kill 7.5 108.8 1 3.50 49.31 65.95 1 Miss Congeniality 6.3 106.8 1 5.52 47.30 63.91 1 The Net 6.0 50.7 2 61.58 8.81 7.86 3 Practical Magic 6.3 46.7 2 65.57 12.80 4.02 3 Speed 7.3 121.2 1 8.94 61.71 78.33 1 Speed 2 3.9 48.6 3 63.74 11.18 5.81 3 While You Were Sleeping 6.8 81.1 2 31.17 21.60 38.24 2 With the new assignments, we calculate the new cluster centers. Cluster Rating Mean Box Office Mean 1 7.033 112.267 2 6.800 81.100 3 5.575 45.800 The new centers appear in Figure 7.4. Figure 7.4: New cluster centers (squares) Next we calculate the new distances and make any necessary reassignments. Movie Rating Box Office Old Cluster D1 D2 D3 New Cluster 28 Days 6.1 37.2 3 75.07 43.91 8.62 3 A Time to Kill 7.5 108.8 1 3.50 27.71 63.03 1 Miss Congeniality 6.3 106.8 1 5.52 25.70 61.00 1 The Net 6.0 50.7 3 61.58 30.41 4.92 3 Practical Magic 6.3 46.7 3 65.57 34.40 1.16 3 Speed 7.3 121.2 1 8.94 40.10 75.42 1 Speed 2 3.9 48.6 3 63.74 32.63 3.26 3 While You Were Sleeping 6.8 81.1 2 31.17 0.00 35.32 2 As the cluster assignments have not changed, we are done. Some notes: With a finite number of individuals, there will be an optimal clustering, though not necessarily unique, and the k-means algorithm might not find it. As said before, different starting assignments could lead to different results. Remember to set the seed if starting with random centers in order to get reproducible results. The magnitude of the variables will affect the clustering. In the movie data, the box office numbers are much larger than the ratings numbers. A small relative difference in box office numbers leads to a greater difference in distances. Figure 7.5 shows the results when we use the kmeans function on scaled data. Figure 7.5: Reults from running the kmeans function on the scaled movie data The number of clusters obviously affects the results. Figure 7.6 shows the clustering when we use four centers instead of three. Figure 7.6: \\(k\\)-means clustering with four centers Cluster optimization When you dont know the right number of clusters, there are several techniques for finding the optimal number, including The elbow method The gap statistic The average silhouette method. The factoextra package can do all three of these calculations. The elbow method involves looking at a plot of the total within-cluster variance (or sum of squares). This number will naturally decrease as the number of clusters increases. When the plot of the variance has an elbow, that means the addition of a cluster doesnt reduce the variance by much. Setting method = \"wss\" produces the elbow plot. library(factoextra) set.seed(1234) fviz_nbclust(scale(MovieDF[, 2:3]), kmeans, k.max = 7, method = &quot;wss&quot;) Figure 7.7: The elbow method on the movie data The graph in Figure 7.7 seems to have two prominent elbows: one at \\(k=2\\) and another at \\(k=5.\\) The ambiguity in the elbow method can make it difficult to decide on the optimal number of clusters. The gap statistic was introduced by Tibshirani, Walther, and Hastie in a paper published in 2002[24]. The gap statistic basically compares the within-cluster variance for the data with \\(k\\) clusters to that for a null-reference distributionbasically a distribution with no natural clustering. The gap statistic method tries to maximize the gap statistic while trying to keep the number of clusters low. The algorithm in this case (Figure 7.8 selects \\(k=3\\) to be the optimal number. set.seed(1234) fviz_nbclust(scale(MovieDF[, 2:3]), kmeans, k.max = 7, method = &quot;gap&quot;) Figure 7.8: The elbow method on the movie data The silhouette of an observation compares how close it is to the other observations in its cluster to how close it is to the observations in the nearest neighboring cluster. Its a number in the interval \\([-1,1]\\). If the value is close to 1, its well inside its cluster. If its close to 0, its near the decision boundary. If its negative, it might be in the wrong cluster. Generally speaking, the better the clustering, the higher the average silhouette. The average silhouette method selects the value of \\(k\\) that yields the maximum average silhouette. With our data, this method selects \\(k=3\\) (Fig. 7.9). set.seed(1234) fviz_nbclust(scale(MovieDF[, 2:3]), kmeans, k.max = 7, method = &quot;silhouette&quot;) Figure 7.9: The elbow method on the movie data To summarize, different methods can lead to different answers. We need to ask ourselves what we are trying to do with our clustering, because the answer might affect the choices we make. Clustering for classification We opened this section with the iris data frame. In that case, there are three natural clusters: the three species included in the data set. How well does clustering identify the three species involved? set.seed(1234) iris.kmeans&lt;-kmeans(iris[, 1:4], centers = 3) iris$Cluster&lt;-iris.kmeans$cluster kmt&lt;-with(iris, table(Species, Cluster)) 1 2 3 setosa 50 0 0 versicolor 0 48 2 virginica 0 14 36 When we create three clusters with the full iris data, we see that Cluster 1 aligns perfectly with the setosa species. Clusters 2 and 3 are dominated by the versicolor and virginica species, respectively, but three are two versicolors in Cluster 3 and 14 virginicas in Cluster 2. 7.2 Collaborative Filtering In 2006, the (then) video delivery service Netflix released a collection of movie ratings data for a contest[25]. The goal of the contest was to create an algorithm that would predict user movie ratings based on previous customer ratings. Using old ratings to predict new ratings is an example of collaborative filtering. Here is a very small scale example. We asked six students to rate a collection of movies on a scale from 1 to 10. Here is a collection of the results from six users and nine movies. There are some missing values that represent movies that were not seen. Wed like to fill in the NA values. User1 User2 User3 User4 User5 User6 Avengers: Endgame 9 9 9 10 9 10 Captain Marvel 8 3 8 7 7 6 Frozen II NA 5 7 7 7 6 Joker 6 9 7 6 10 10 The Lion King NA 3 9 NA 7 10 Parasite 10 6 10 NA 8 10 Spider-Man: Far From Home 6 7 9 8 8 10 Star Wars IX: The Rise of Skywalker 7 6 6 7 8 9 Toy Story 4 7 3 5 9 NA 10 For instance, what kind of review might User 1 give to FROZEN II, which they hadnt seen yet? There are some averages that we could use. The grand average of all ratings for all movies seen is 7.61. The user average is the average of each users ratings. User Mean User1 7.57 User2 5.67 User3 7.78 User4 7.71 User5 8.00 User6 9.00 The item average (movie average) is the average of each movies ratings. Movie Mean Avengers: Endgame 9.33 Captain Marvel 6.50 Frozen II 6.40 Joker 8.00 The Lion King 7.25 Parasite 8.80 Spider-Man: Far From Home 8.00 Star Wars IX: The Rise of Skywalker 7.17 Toy Story 4 6.80 One estimate could take all of these into account: the fact that a movies ratings are below or above the average ratings (item bias), and how a users ratings compare to the average rating (user bias). The FROZEN II average rating is 6.40, which is below the overall average rating of 7.61. User 1s average rating is 7.57, which is 0.04 below the overall average. We could simply take the movie average of 6.40 and adjust it downward by 0.04 to get a predicted rating of 6.36. Note that we could also start at 7.57 for the Users average rating and adjust it by the FROZEN II bias of \\(6.40-7.61=-1.21\\) to get the predicted rating: \\(7.57+-1.21=6.36.\\) Collaborative filtering finds similar users or similar movies, and uses their ratings to fill in the blanks. User-User methods find the users who are most similar to the user in question, and use their ratings to estimate the missing movie rating. Item-Item methods find the items (movies) that are most similar to the movie in question, and use the users ratings on those movies to fill in the blank. Similarity measures Before we can find the similar movies or users, we need a way to measure similarity. There are a couple of measures that are commonly used. The first is cosine similarity: Definition 7.1 Let \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) be the vectors of user (or movie) ratings, ignoring the missing values. Then the cosine similarity between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is \\[\\text{CosSim}(\\mathbf{x},\\mathbf{y})=\\frac{\\mathbf{x}\\cdot \\mathbf{y}}{|\\mathbf{x}||\\mathbf{y}|}.\\] Weve seen this before. Its the cosine of the angle between the vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Since the ratings are all positive, it will be a number between 0 and 1. The closer to 1, the more similar the users (or movies). The closer to 0, the less similar. For example, we could use cosine similarity to compare Users 1 and 2: User1 User2 Avengers: Endgame 9 9 Captain Marvel 8 3 Frozen II NA 5 Joker 6 9 The Lion King NA 3 Parasite 10 6 Spider-Man: Far From Home 6 7 Star Wars IX: The Rise of Skywalker 7 6 Toy Story 4 7 3 Users 1 and 2 have 7 movies in common. The vectors are \\(\\mathbf{x}=(9,8,6,10,6,7,7)\\) and \\(\\mathbf{y}=(9,3,9,6,7,6,3).\\) Their cosine similarity is \\[\\text{CosSim}(\\mathbf{x},\\mathbf{y})=\\frac{324}{\\sqrt{415}\\sqrt{301}}=0.9167.\\] We can make a table of all the cosine similarities. User1 User2 User3 User4 User5 User6 User1 1.0000 0.9167 0.9827 0.9898 0.9695 0.9703 User2 0.9167 1.0000 0.9278 0.9225 0.9715 0.9398 User3 0.9827 0.9278 1.0000 0.9755 0.9785 0.9711 User4 0.9898 0.9225 0.9755 1.0000 0.9792 0.9805 User5 0.9695 0.9715 0.9785 0.9792 1.0000 0.9885 User6 0.9703 0.9398 0.9711 0.9805 0.9885 1.0000 Collaborative filters often use a certain number, say \\(N\\), of the users that have the highest similarity measure with the one in question. If we use the three nearest neighbors to User 1, they are User 4, User 3, and User 6, in descending order. We can average the FROZEN II ratings for Users 4, 3, and and 6 to predict User 1s rating, but we should weight them by similarity. The weighted average rating is \\[\\text{Rating}=\\frac{7\\cdot 0.9827+7\\cdot 0.9898+6\\cdot 0.9703}{0.9827+0.9898+0.9703}=6.6703.\\] In general, if we have \\(N\\) nearest neighbors with similarity scores of \\(s_1,\\dots,s_N\\) and ratings \\(r_1,\\dots,r_N\\), the weighted average rating is \\[\\text{Rating}=\\frac{\\sum_{i=1}^N r_i s_i}{\\sum_{i=1}^N s_i}.\\] Another popular measure of similarity is the Pearson correlation coefficient. The Pearson correlation coefficient is actually the cosine similarity after the vectors have been centered at their means. (As a consequence, the Pearson correlation coefficient is not defined if one of the vectors is constant.) Here are the Pearson correlations between FROZEN II and all of the other movies. Movie r Avengers: Endgame 0.102 Captain Marvel 0.959 Frozen II 1.000 Joker -0.431 The Lion King 0.647 Parasite 0.636 Spider-Man: Far From Home 0.294 Star Wars IX: The Rise of Skywalker 0.129 Toy Story 4 0.448 The three movies most positively correlated with FROZEN II are CAPTAIN MARVEL (\\(r=0.959\\)), THE LION KING (\\(r=0.647\\)), and PARASITE (\\(r=0.636\\).) We can use User 1s ratings on CAPTAIN MARVEL, THE LION KING, and PARASITE to estimate their rating for FROZEN II. The problem is that User 1 didnt see THE LION KING. In this case, we can use the item average of 7.25. With the Pearson correlations as weights, we estimate User 1s rating of FROZEN II to be \\[\\text{Rating}=\\frac{8\\cdot 0.959+7.25\\cdot 0.647+10\\cdot 0.636}{0.959+0.647+0.636}=8.3509.\\] Collaborative filtering with recommenderlab We can use the recommenderlab package to perform more complicated collaborative filtering operations. Figure 7.10 displays the ratings on a grayscale. Missing ratings are white. library(recommenderlab) # To use our data, we need the movies as the variables # and we need it in matrix form, which we then convert # to a realRatingMatrix. (The movie ratings are currently # in a data frame called &quot;moviesdf&quot;.) moviest &lt;- t(as.matrix(moviesdf)) movies.RRM &lt;- as(moviest, &quot;realRatingMatrix&quot;) image(movies.RRM) Figure 7.10: A ratings matrix plot Here, we fill in the blanks using cosine similarity and user-based collaborative filtering (UCBF). model_params &lt;- list(method = &quot;cosine&quot;, nn = 3, normalize = &quot;center&quot;) reco&lt;-Recommender(movies.RRM, method = &quot;UBCF&quot;, parameter = model_params) predictions&lt;-predict(reco, movies.RRM, type = &quot;ratings&quot;) preds&lt;-as(predictions, &quot;matrix&quot;) When we print the predictions, only the missing values appear. User1 User2 User3 User4 User5 User6 Avengers: Endgame NA NA NA NA NA NA Captain Marvel NA NA NA NA NA NA Frozen II 6.8 NA NA NA NA NA Joker NA NA NA NA NA NA The Lion King 8.8 NA NA 8.7 NA NA Parasite NA NA NA 9.5 NA NA Spider-Man: Far From Home NA NA NA NA NA NA Star Wars IX: The Rise of Skywalker NA NA NA NA NA NA Toy Story 4 NA NA NA NA 7 NA 7.3 Decision Tree Classification Decision trees can classify objects using both numerical and categorical criteria. Here is an example. Can a Titanic passengers cabin class (Pclass), sex, and embarkation location be used to predict whether or not they survived? Figure 7.11 contains a decision tree created using data from a training set. Figure 7.11: A decision tree for Titanic survival data The first node shows that 62% of the passengers perished (No being the predominant result). If the passengers sex is male, we follow the tree to the left. If that is the case, we come to an end. Eighty-one percent of the male passengers in the training set died. If the passenger sex is female, we go to the right. At that point, we have 26% of the female passengers dying and 74% surviving. We can ask another question that will better separate things. If the passenger was in Third Class, we go to the left, and if not (meaning they were in First or Second Class), we go to the right, and 95% of those survived. For Third Class passengers, we see an even split. Among those who departed from Southampton (Embarked=S), 62% died. If they left from some other port, they were more likely to survive. To see how decision trees are created, we will look at a smaller example. The data set comes from [26]. Suppose youre trying to decide whether options on a particular commodity are going to go up or down, based upon recent trends, open interest level, and trading volume. The table below shows ten different cases. Past Trend Open Interest Trading Volume Return 1 Positive Low High Up 2 Negative High Low Down 3 Positive Low High Up 4 Positive High High Up 5 Negative Low High Down 6 Positive Low Low Down 7 Negative High High Down 8 Negative Low High Down 9 Positive Low Low Down 10 Positive High High Up Wed like to come up with a decision tree to help us determine whether an options return will go up or down. To start, we find the one feature (Past Trend, Open Interest, or Trading Volume) that gives us the best split. If we choose Past Trend, the split looks like this. Past Trend Positive Negative Return Up 4 (4/6) 0 (0/4) Down 2 (2/6) 4 (4/4) Among those with a positive trend, 4 of 6 went up, while two went down. Among those with a negative trend, all four went down. The Gini impurity index measures how pure each group is. If \\(p_1\\) and \\(p_2\\) are respectively the proportion of Ups and Downs in a group, the Gini index is \\(\\text{Ginx}=2p_1p_2\\). If a group contains only one type, its index will be 0. If it is evenly split, its index will be 1/2. For those with a positive trend, the Gini index is \\(2\\cdot\\frac{4}{6}\\cdot\\frac{2}{6}=\\frac{4}{9}\\), while for those with a negative trend, it is \\(2\\cdot\\frac{0}{4}\\cdot\\frac{4}{4}=0\\). We then weight these by group size to get the weighted Gini index: \\[\\text{WeightedGinx}=\\frac{6}{10}\\cdot\\frac{4}{9}+\\frac{4}{10}\\cdot0=\\frac{4}{15}\\approx 0.27.\\] If we split by trading volume, we get the following. Volume Low High Return Up 0 (0/3) 4 (4/7) Down 3 (3/3) 3 (3/7) The weighted Gini index when we split by trading volume is \\[\\text{WeightedGinx}=\\frac{3}{10}\\cdot 2\\cdot 0\\cdot 1+\\frac{7}{10}\\cdot 2\\cdot \\frac{4}{7}\\cdot\\frac{3}{7}=\\frac{12}{35}\\approx 0.34.\\] Similarly, the index when we split by open interest is 0.47. A lower index means a purer split, so our first split is by Past Trend. All of the cases with a negative past trend are associated with returns being down. The trending up group still can be split. It turns out that splitting by Trading Volume at the next level creates a pure split, and were done. Our tree is in Figure 7.12. Figure 7.12: A decision tree on the trading data In reality, such perfect splits dont occur so quickly, and most algorithms have a maximum number of splits or minimum group size. When you split on numerical data, you order the values and make the midpoints the possible splits. For example, with this mpg/transmission type data, mpg transmission 14.7 automatic 15.2 automatic 19.2 manual 26.0 automatic 30.4 manual the possible splits for mpg would be at \\((14.7+15.2)/2=14.95,(15.2+19.2)/2=17.2\\) and so on. Decision trees with R There are a few R packages that will create classification trees. One is rpart. In this example, we create a decision tree for the species in a training sample from the iris data set using the rpart function. library(rpart) data(iris) set.seed(1234) inTrain &lt;- sample(nrow(iris), size = 0.75*nrow(iris)) iris.train &lt;- iris[inTrain,] iris.test &lt;- iris[-inTrain,] iris.tree &lt;- rpart(Species~.,data = iris.train) iris.tree ## n= 112 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 112 73 virginica (0.31250000 0.33928571 0.34821429) ## 2) Petal.Length&lt; 2.6 35 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.6 77 38 virginica (0.00000000 0.49350649 0.50649351) ## 6) Petal.Length&lt; 5 40 3 versicolor (0.00000000 0.92500000 0.07500000) * ## 7) Petal.Length&gt;=5 37 1 virginica (0.00000000 0.02702703 0.97297297) * This output can be hard to follow. There are a couple of functions that will produce nice decision trees. The rpart.plot package has its namesake function. The rattle package has fancyRpartPlot, which can be fickle, but produces nice plots. Figure 7.13 shows the results for the iris training data. Note that it creates a pretty good, but not perfect, split. library(rattle) fancyRpartPlot(iris.tree, sub = NULL) #print w/o stamp Figure 7.13: A decision tree for the iris data We can now use the decision tree to make predictions on the testing set. The function confusionMatrix comes from the caret package. library(caret) preds &lt;- predict(iris.tree, newdata = iris.test, type = &quot;class&quot;) confusionMatrix(preds, iris.test$Species)$table ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 11 3 ## virginica 0 1 8 Overall, it does a pretty good job, though, as weve seen before, there is some confusion between versicolor and virginica. Random Forests Random forest models create multiple classification trees and make classifications based on consensus picks. There are actually two random elements: Each tree in a random forest uses a random subset of the training data. Each tree in a random forest can use a random subset of the available features (variables). The number of features used can be chosen, or a package might experiment to find the best number. Here is some sample output. Note that because of the randomness involved, the seed should be set for reproducibility. library(randomForest) set.seed(1234) iris.rf &lt;- randomForest(Species~., data = iris.train) iris.rf ## ## Call: ## randomForest(formula = Species ~ ., data = iris.train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 5.36% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 35 0 0 0.00000000 ## versicolor 0 36 2 0.05263158 ## virginica 0 4 35 0.10256410 The random forest model itself is too messy to display here. When we apply it to the testing set, we get better results than the decision tree. preds.rf&lt;-predict(iris.rf, newdata = iris.test) confusionMatrix(preds.rf, iris.test$Species)$table ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 11 1 ## virginica 0 1 10 Here are some advantages and disadvantages of the random forest approach. Advantages: Reduces overfitting by decision trees. Works well on nonlinear data. Generally dont need to pre-process data. Disadvantages: It can be slow and take a lot of computational power. It cant be interpreted. 7.4 Support Vector Machines Support vector machines are common methods for classifying objects. Figure 7.14 shows a simple case. The training data consist of two types: A and B. The two groups are well separated. Figure 7.14: Nicely separated 1-dimensional data To classify new data, we pick a threshold value. When we put it at the midpoint between the two closest observations from the two groups, the margin is maximized, giving us a maximal margin classifier (Fig. 7.15). New values to the left of the threshold will be classified as type A, and those to the right will be classified as type B. Figure 7.15: A maximal margin classifier If our data have two variables, the threshold will be a line (Fig. 7.16). Figure 7.16: Two dimensional data In 3 dimensions, it will be a plane. In higher dimensions, it is called a hyperplane (of codimension 1). The optimal line is calculated using a formula involving the dot products of all the points when considered as vectors. The points on the margin are called support vectors (Fig. 7.17). Figure 7.17: Support vectors Data cant always be perfectly separated this way. In Figure 7.18 on the left, we have an outlier in the B data that is very close to the A values. Putting the threshold between these values would almost certainly lead to misclassification of future type A observations. An even more likely issue is that it could be impossible to separate the two groups by a single number or line or plane. We have to allow for possible misclassification in the training set if we want to create any type of threshold. Figure 7.18: Some issues When we allow for misclassifications, we include a cost function in our optimization problem. Some observations will be on or inside what is now a soft margin. These are all support vectors, and the resulting classifier is a soft margin or support vector classifier (Fig. 7.19). Figure 7.19: Soft margins What if the data look like Figure 7.20? Type A is concentrated in the center, while type B is on the extremes. Our support vector classifier wont do a great job on this one. Figure 7.20: One type surrounded by another Support vector machines take lower-dimensional data and transform them into higher dimensions, where they can be separated. For example, if we add a new variable to our old data by just squaring the \\(x\\) values, it will look like Figure 7.21. (Note: the line and margins were drawn in by hand, so they probably are not horizontal in reality.) Figure 7.21: Transforming into a higher dimension As mentioned before, finding the optimal threshold involves finding the dot products between all of the observations. It can be computationally expensive to transform the data and then find all of the dot products. Fortunately, for many types of transformations, we can calculate the dot products without actually having to compute the transformations. This involves kernels. If \\(\\varphi\\) is the transformation, then the corresponding kernel function \\(K\\) satisfies \\[K(\\mathbf{u},\\mathbf{v})=\\varphi(\\mathbf{u})\\cdot \\varphi(\\mathbf{v}),\\] where \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are two untransformed observations. The polynomial kernel can be used for polynomial transformations. The kernel is \\[K(\\textbf{u},\\textbf{v})=(\\textbf{u}\\cdot\\textbf{v}+c)^d,\\] where \\(d\\) is the degree of the transformation and \\(c\\) is a parameter that can be adjusted. Both \\(c\\) and \\(d\\) can be optimized using cross-validation and other techniques. For the transformation in Figure 7.21, \\(\\varphi(x)=(x,x^2)\\), the values \\(c=1/2\\) and \\(d=2\\) give the proper kernel. Here is how it works. Suppose we have \\(x_1\\) and \\(x_2\\). Then applying the transformation gives us \\((x_1,x_1^2)\\) and \\((x_2,x_2^2)\\). We actually add a third component equal to 1/2 to make the constant work out. This is because \\[(x_1,x_1^2,1/2)\\cdot(x_2,x_2^2,1/2)=x_1x_2+x_1^2x_2^2+1/4,\\] while \\[(x_1\\cdot x_2+1/2)^2=(x_1x_2)^2+2\\cdot\\frac{1}{2}x_1x_2+(\\frac{1}{2})^2=x_1^2x_2^2+x_1x_2+1/4.\\] Note that with one-dimensional data, the dot product is the regular product. Support vector machines in R There are a couple of ways to perform classification using support vector machines in R. The e1071 package has an svm function. You need to set the type of kernel used. (See ?svm for available kernels.) In this example, we again work with the iris data frame, but remove the setosa species, and use just the petal length and sepal width. library(e1071) library(caret) iris2 &lt;- subset(iris, Species != &quot;setosa&quot;) iris3 &lt;- iris2[, c(2, 3, 5)] iris3$Species &lt;- droplevels(iris3$Species) set.seed(123) inTrain &lt;- createDataPartition(iris3$Species, p = 0.75, list = F) training &lt;- iris3[inTrain,] testing &lt;- iris3[-inTrain,] iris.svm &lt;- svm(Species ~., data = training, kernel = &quot;linear&quot;) iris.svm ## ## Call: ## svm(formula = Species ~ ., data = training, kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 22 When we apply the plot function to the output, we get Figure 7.22. The support vectors are marked by an x, while the remaining points are marked by an o. The color of the symbol shows the species. plot(iris.svm, data=training, grid = 100) Figure 7.22: A plot of an SVM applied to a training set How well does it predict on the testing set? preds &lt;- predict(iris.svm, newdata = testing) confusionMatrix(preds, testing$Species)$table ## Reference ## Prediction versicolor virginica ## versicolor 10 0 ## virginica 2 12 Can we do better with a different type of kernel? The radial kernel is the default kernel for the svm function. We get a nonlinear threshold (Fig. 7.23). iris.svmr &lt;- svm(Species ~., data = training, kernel = &quot;radial&quot;) plot(iris.svmr, data = training, grid = 100) Figure 7.23: The radial kernel Are the predictions better? Not for this data. predsr &lt;- predict(iris.svmr, newdata = testing) confusionMatrix(predsr, testing$Species)$table ## Reference ## Prediction versicolor virginica ## versicolor 10 0 ## virginica 2 12 You can also use support vector machines as methods in carets train function. See the associated help page for kernel-specific methods. 7.5 Exercises The chart below shows eight \\((x,y)\\)-pairs and an initial clustering. Find the two centers based on the initial clustering. Find the distance from each \\((x,y)\\)-pair to each center. Find the new cluster assignments based on these distances. \\(x\\) \\(y\\) Initial Cluster 1 4 1 4 5 1 9 9 1 6 6 1 1 3 2 4 6 2 7 5 2 4 2 2 The chart below shows eight \\((x,y)\\)-pairs and an initial clustering. Find the two centers based on the initial clustering. Find the distance from each \\((x,y)\\)-pair to each center. Find the new cluster assignments based on these distances. \\(x\\) \\(y\\) Initial Cluster 2 3 1 6 2 1 8 1 1 4 6 1 1 3 2 6 6 2 3 5 2 4 2 2 The faithful data set from base R has information on waiting time between eruptions and eruption duration. Set the seed to be 123 and then find the optimal number of clusters using the gap statistic. Make sure you scale the data. With that optimal number of clusters, use the kmeans function to create the clustering. Set the seed again to 123 before running the function. Create a scatterplot of the data, with waiting as a function of eruption. Use the cluster to color the scatterplot. Use the $centers portion of the kmeans output to describe the different clusters. The state.x77 data set has state-by-state information taken from a 1977 U.S. Census Bureau report. Turn it into a data frame by executing states &lt;- data.frame(state.x77). Set the seed to be 1729 and then find the optimal number of clusters using the gap statistic. Make sure you scale the data. With that optimal number of clusters, use the kmeans function to create the clustering. Set the seed again to 1729 before running the function. Create a scatterplot of the data, with Area on the horizonal axis and Population on the vertical axis. Use the cluster to color the scatterplot. Use the $centers portion of the kmeans output to describe the different clusters. The table below has movie review data that we saw in the Chapter 6 Exercises. This time, some of the ratings are missing. Find the cosine similarity between Users 4 and 5 by hand. The coop package has a cosine function that calculates cosine similarities between vectors or between columns of matrices. Use the argument use = \"complete.obs\" so that it will ignore missing values. Use this function on the transpose of the ratings matrix to find all cosine similarities involving User 4. Create a weighted average of the two nearest neighbors to User 4 to estimate their rating for American Beauty. User American Beauty Apollo 13 Forrest Gump Jurassic Park 1 4.00 3.50 5.00 4.50 2 5.00 3.00 NA 3.50 3 4.00 5.00 4.00 4.00 4 NA 4.00 5.00 3.50 5 5.00 2.50 5.00 3.50 6 4.00 3.00 5.00 4.50 Again, we will use the partial ratings data from Problem 5. This time we will try to fill in User 2s rating for Forrest Gump. Calculate the cosine similarity between Forrest Gump and Jurassic Park. Calculate the all cosine similarities between movies. See Problem 5 for instructions. This time, we dont need to transpose the matrix. Create a weighted average of User 2s ratings of the two nearest neighbors to Forrest Gump to estimate their rating for it. Below is a small set of information on nine Titanic passengers. We want to create a decision tree to predict whether a passenger survived or perished. Two passenger classes appear in the data. Compute the weighted Gini index if the split is based on class. Compute the weighted Gini index if the split is based on Sex. What should the first split be? Survived Pclass Sex N 3 male Y 1 female Y 3 female N 1 female N 3 male N 3 male Y 1 male N 3 male Y 3 female Here is a different set of ten Titanic passengers. Two passenger classes appear in the data. Compute the weighted Gini index if the split is based on class. Compute the weighted Gini index if the split is based on Sex. What should the first split be? Survived Pclass Sex N 1 male Y 1 female Y 3 female N 1 male N 1 male N 1 male Y 3 male N 3 male N 3 female Y 3 female The Pima.tr data frame in the MASS package contains information related to diabetes in Native American women. Create and plot a decision tree with type as the dependent variable. Use the decision tree to predict whether or not the person with the following values has diabetes. (Not all values will be used.) glu = 150 age = 30 bmi = 40 bp = 90 ped = 0.5 The Pima.te data frame is a testing set. Use your decision tree to make predictions on the testing set. Create a confusion matrix to determine the overall accuracy of the predictions. The Pima.tr2 data frame is similar to the Pima.tr data frame, but has some missing values. Create and plot a decision tree with type as the dependent variable using the Pima.tr2 data frame. Use the decision tree to predict whether or not the person with the following values has diabetes. (Not all values will be used.) glu = 120 age = 25 bmi = 25 bp = 65 ped = 0.2 Use your decision tree to make predictions on the Pima.te testing set. Create a confusion matrix to determine the overall accuracy of the predictions. This time we will create a random forest using the Pima.tr data. Set the seed to 1234 and create the random forest, with type as the dependent variable. Use the random forest to predict the type using the Pima.te testing data. Create a confusion matrix to determine the overall accuracy of the predictions. The randomForest function doesnt work with missing data, but there is an rfImpute function that will impute missing values. Set the seed to 1234 and define Pima.imputed &lt;-rfImpute(type~., data = Pima.tr2) Set the seed to 1234 and create the random forest on the Pima.imputed data. Use the random forest to predict the type using the Pima.te testing data. Create a confusion matrix to determine the overall accuracy of the predictions. Create a support vector machine to predict transmission type am based on all of the other variables. Because the am variable is seen as numerical, you will have to use as.factor(am) in the function call. Set the kernel to linear. Use the predict function on the original data. Create a confusion matrix to determine the accuracy of the predictions. (You will have to again convert am to a factor variable.) Repeat Problem 13, but with a radial kernel. We can use support vector machines when there are more than two levels. Create a support vector machine with a linear kernel to predict the Species with the full iris data frame. Use Petal.Length and Petal.Width as the independent variables. Apply the plot function to the output. You will need to set data=iris[,3:5], so as not to confuse the function. USe the predict function on the original data. Create a confusion matrix to determine the accuracy of the predictions. Repeat Problem 15, but with a radial kernel. Repeat Problem 15, but with kernel = \"polynomial\" and the degree set to: The default, which is 3. Two (degree = 2). References 23. The internet movie database. 24. Tibshirani R, Walther G, Hastie T (2002) Estimating the Number of Clusters in a Data Set Via the Gap Statistic. Journal of the Royal Statistical Society Series B: Statistical Methodology 63: 411423. 25. Bell RM, Koren Y (2007) Lessons from the netflix prize challenge. Acm Sigkdd Explorations Newsletter 9: 7579. 26. Thakar C, Tahsildar S (2022) Gini index: Decision tree, formula, calculator, gini coefficient in machine learning. "],["MVCA.html", "A An Introduction to Multivariable Calculus A.1 Functions of several variables A.2 Partial Derivatives A.3 Directional Derivatives A.4 Optimization A.5 Exercises", " A An Introduction to Multivariable Calculus Section 5.10 on the gradient descent algorithm assumes a working knowledge of partial differentiation. This is a very brief summary of that and related topics from multivariable calculus. It does assume an working knowledge of single variable calculus. A.1 Functions of several variables Many real-world quantities depend on more than one variable. For instance, the wind chill is a function of the temperature and the wind speed. The balance on a deposit is a function of the initial deposit, the interest rate, and time. For the most part, well be focusing on functions of two variables. For example, consider \\(f(x,y)=x^2+y^2.\\) When we plug in various \\((x,y)\\)-pairs, we get different outputs. \\(f(1,2)=1^2+2^2=1+4=5\\) \\(f(-1,3)=(-1)^2+3^2=1+9=10\\) If we let \\(z=f(x,y)\\), then we can graph the set of points \\((x,y,z)\\), where \\(x\\) and \\(y\\) are in the domain of \\(f\\). These graphs are usually surfaces for nice functions. Figure A.1 shows the graph of \\(z=x^2+y^2\\) over the rectangle \\(-2\\leq x\\leq 2,-2\\leq y\\leq 2\\). f &lt;- function(x,y) x^2+y^2 x &lt;- seq(-2, 2, length = 40) y &lt;- seq(-2, 2, length = 40) z &lt;- outer(x, y, f) persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = &quot;lightblue&quot;) Figure A.1: The graph of a function of two variables Another option for visualizing functions of two variables is to plot level curves. A level curve for the function \\(f(x,y)\\) is the set of solutions to the equation \\(f(x,y)=k\\) for some constant \\(k\\). Isotherms (curves of equal temperature) and isobars (equal pressure) on a weather map are examples of level curves. Topographic maps, as in Figure A.2 show curves of equal elevation. Figure A.2: A topographic map including a volcano For example, for \\(f(x,y)=x^2+y^2\\), level curves are given by the equation \\(x^2+y^2=k.\\) For \\(k&gt;0\\), the level curve will be the circle of radius \\(\\sqrt{k}\\) centered at the origin. For \\(k=0\\), the solution to \\(x^2+y^2=0\\) is simply the point \\((0,0)\\). For \\(k&lt;0\\), the equation \\(x^2+y^2=k\\) has no solution. We can graph level curves in two dimensions. When we plot several of them on the same graph, we get what is called a contour plot. If we want, we can shade the regions between level curves. In Figure A.3 darker colors indicate lower function values. Note: mathcolors is a special color palette. contour(x,y,z, asp=1) filled.contour(x,y,z, color = mathcolors, plot.axes = { axis(1) axis(2) contour(x,y,z, add = TRUE)},asp=1) Figure A.3: Contour plots Minima and maxima You can often use contour plots to locate local minima and maxima for functions. Figure A.4 shows a contour plot for \\(f(x,y)=\\frac{x}{e^{x^2+y^2}}.\\) From the looks of it, this function appears to have a local minimum around \\((-0.7,0)\\) and a local maximum around \\((0.7,0)\\). Figure A.4: A contour plot for \\(f(x,y)=\\frac{x}{e^{x^2+y^2}}\\) Figure A.5 is the 3D graph of the same function, showing the local minimum and maximum. Figure A.5: Another look at \\(f(x,y)=\\frac{x}{e^{x^2+y^2}}\\) Figures A.6 shows the graph and contour plot for \\(f(x,y)=xy\\). The lines \\(x=0\\) and \\(y=0\\) form the level set \\(f(x,y)=0\\). Their crossing is what we call a saddle point. Figure A.6: The function \\(f(x,y)=xy\\) We will learn how to locate and classify minima and maxima soon. Limits and continuity Limits and continuity for functions of two variables are a bit more complicated than their 1-variable versions. Definition A.1 Let \\(f(x,y)\\) be a function defined on a neighborhood including the point \\((a,b)\\) but not necessarily at \\((a,b)\\) itself. We say that the limit of \\(f\\) as \\((x,y)\\) approaches \\((a,b)\\) is \\(L\\) and write \\[\\lim_{(x,y)\\to(a,b)}f(x,y)=L\\] if for \\(\\epsilon&gt;0\\) there exists a \\(\\delta&gt;0\\) such that whenever \\[\\sqrt{(x-a)^2+(y-b)^2}&lt;\\delta,|f(x,y)-L|&lt;\\epsilon.\\] Dont worry about memorizing this; we wont be using it. The idea is that for the limit to exist, the value of \\(f\\) needs to get closer and closer to \\(L\\) as the points \\((x,y)\\) get closer and closer to \\((a,b)\\). Figure A.7 shows the contour plot of a nice function around the point \\((1,1)\\). For points near \\((1,1)\\), the function values are close to the value of \\(f(1,1)\\). Figure A.7: A nice function Figure A.8 shows the contour plot for the graph of \\(h(x,y)=\\frac{xy^2}{x^2+y^4}\\) near the point \\((0,0).\\) Note how the function values are all over the place. For this function, the limit does not exist at the point \\((0,0)\\). Figure A.8: The limit does not exist Definition A.2 A function \\(f\\) is continuous at a point \\((a,b)\\) if \\[\\lim_{(x,y)\\to(a,b)}f(x,y)=f(a,b).\\] Fortunately for us, most, if not all, of the functions that we will be dealing with will be continuous on their domains. A polynomial in two variables is the sum of terms of the form \\(cx^my^n\\), where \\(c\\) is a constant and \\(m\\) and \\(n\\) are nonnegative integers. For instance, \\(f(x,y)=2x^3+4x^2y^2+xy^3+4y^4\\) and \\(g(x,y)=xy\\) are polynomials. Polynomials are continuous everywhere on \\(\\mathbb{R}^2\\). A rational function is the quotient of two polynomials, such as \\(h(x,y)=\\frac{xy^2}{x^2+y^4}\\) from before. Rational functions are continuous on their domains. This function, \\(h(x,y),\\) whose contour plot we saw in Figure A.8, is discontinuous at \\((0,0)\\), but is continuous everywhere else. A.2 Partial Derivatives Overall differentiability for functions of several variables is a complicated topic, but one fairly simple notion is that of a partial derivative. We can take a partial derivative of a function with respect to any of the variables, by pretending that the other variables are actually constant and applying the regular differentiation rules to what is left. Example A.1 Lets consider the function \\(f(x,y)=x^2+3xy^2+y^3.\\) We can take the partial derivative of \\(f\\) with respect to \\(x,\\) denoted \\(f_x,\\) by pretending that \\(y\\) is a constant: \\[\\begin{align*} f(x,y)&amp;=x^2+3x\\cdot\\underbrace{y^2}_{\\text{constant}}+\\underbrace{y^3}_{\\text{constant}}\\\\ &amp;=x^2+3y^2x+y^3. \\end{align*}\\] So \\[f_x(x,y)=2x+3y^2(1)+0=2x+3y^2.\\] To take the partial with respect to \\(y\\), we pretend that \\(x\\) is a constant: \\[f(x,y)=\\underbrace{x^2}_{\\text{constant}}+\\underbrace{3x}_{\\text{constant}}\\cdot y^2+y^3.\\] So \\[f_y(x,y)=0+3x\\cdot 2y+3y^2=6xy+3y^2.\\] Example A.2 If \\(f(x,y)=(3x+2y^2)^5\\), then, using the chain rule for both \\[f_x(x,y)=5(3x+2y^2)^4(3)=15(3x+2y^2)^4,\\] and \\[f_y(x,y)=5(3x+2y^2)^4(4y)=20y(3x+2y^2)^4.\\] The Leibniz notation for partial derivatives involves a curly \\(\\partial\\) instead of the normal \\(d\\). For instance, \\(f_x=\\frac{\\partial f}{\\partial x}\\) and \\(f_y=\\frac{\\partial f}{\\partial y}.\\) For example \\[\\frac{\\partial}{\\partial x}\\left(e^{x^2+y^2}\\right)=e^{x^2+y^2}(2x)=2xe^{x^2+y^2}.\\] Higher order partial derivatives Since partial derivatives of functions of two or more variables are also functions of two or more variables, we can take their partial derivatives, too. In Example A.1, for \\(f(x,y)=x^2+3xy^2+y^3\\), we learned that \\(f_x(x,y)=2x+3y^2\\) and \\(f_y(x,y)=6xy+3y^2\\). Then \\[f_{xx}(x,y)=\\frac{\\partial}{\\partial x}f_x(x,y)=\\frac{\\partial}{\\partial x}(2x+3y^2)=2\\] and \\[f_{yy}(x,y)=\\frac{\\partial}{\\partial y}f_y(x,y)=\\frac{\\partial}{\\partial y}(6xy+3y^2)=6x+6y.\\] We can also take mixed partial derivatives: \\[f_{xy}=\\frac{\\partial}{\\partial y}f_x(x,y)=\\frac{\\partial}{\\partial y}(2x+3y^2)=6y,\\] and \\[f_{yx}=\\frac{\\partial}{\\partial x}f_y(x,y)=\\frac{\\partial}{\\partial x}(6xy+3y^2)=6y.\\] Hey, these are the same. Is that a coincidence? No. It is not a coincidence. Theorem A.1 (Clairaut's Theorem) Suppose \\(f\\) is defined on a disk \\(D\\) that contains the point \\((a,b)\\). If the functions \\(f_{xy}\\) and \\(f_{yx}\\) are both continuous on \\(D\\) then \\(f_{xy}(a,b)=f_{yx}(a,b)\\). Clairauts theorem is stated at a point, but if the mixed partial derivatives are continuous on a whole region, then they are equal on the whole region. The Leibniz notation for higher order derivatives is \\[f_{xx}(x,y)=\\frac{\\partial}{\\partial x}\\left(\\frac{\\partial f}{\\partial x}\\right)=\\frac{\\partial^2 f}{\\partial x^2},\\] \\[f_{yy}(x,y)=\\frac{\\partial}{\\partial y}\\left(\\frac{\\partial f}{\\partial y}\\right)=\\frac{\\partial^2 f}{\\partial y^2},\\] \\[f_{xy}(x,y)=\\frac{\\partial}{\\partial y}\\left(\\frac{\\partial f}{\\partial x}\\right)=\\frac{\\partial^2 f}{\\partial y\\partial x},\\] and \\[f_{yx}(x,y)=\\frac{\\partial}{\\partial x}\\left(\\frac{\\partial f}{\\partial y}\\right)=\\frac{\\partial^2 f}{\\partial x\\partial y}.\\] In the subscript notation (such as \\(f_{xy}\\)), the order of differentiation is read from left to right, while in Leibniz notation, it is read from right to left. A.3 Directional Derivatives If \\(f\\) is a function of a single variable, its derivative at a number \\(a, f&#39;(a)\\), tells us the instantaneous rate of change of \\(f(x)\\) at \\(x=a.\\) For example, if \\(f(x)=x^2, f&#39;(x)=2x\\), and \\(f&#39;(3)=6\\), which tells us that at \\(x=3\\), for each one unit of change in the \\(x\\)-direction, \\(f(x)\\) will change by about 6 units. If \\(f\\) is a function of two variables, \\(x\\) and \\(y\\), then the partials \\(f_x\\) and \\(f_y\\) will tell us the instantaneous rate of change of \\(f\\) in the \\(x\\) and \\(y\\) directions, respectively. For example if \\(f(x,y)=x^3-3xy^2\\), then \\(f_x(x,y)=3x^2-3y^2\\) and \\(f_y(x,y)=-6xy\\). If we evaluate these partials at the point \\((2,1)\\), we get \\[f_x(2,1)=3\\cdot2^2-3\\cdot 1^2=12-3=9,\\] and \\[f_y(2,1)=-6\\cdot 2\\cdot 1=-12.\\] Figure A.9 is the contour plot for \\(f\\). (In this version, darker colors represent smaller function values.) If we go in the positive \\(x\\)-direction from the point \\((2,1)\\), the value of the function increases. If we go in the \\(y\\)-direction, it decreases. Figure A.9: Partial derivatives Since were no longer trapped on the \\(x\\)-axis, we can go in any direction we want (Fig. A.10). The rate of change in an arbitrary direction is called the directional derivative. Figure A.10: Directional derivatives Definition A.3 The directional derivative of \\(f\\) at \\((x_0,y_0)\\) in the direction of the unit vector \\(\\mathbf{u}=(u_1,u_2)\\) is \\[D_{\\mathbf{u}}f(x_0,y_0)=\\lim_{h\\to 0}\\frac{f(x_0+hu_1,y_0+hu_2)-f(x_0,y_0)}{h}\\] if this limit exists. Note: we want \\(\\mathbf{u}\\) to be a unit vector so this value tells us the approximate rate of change per unit traveled in the direction of \\(\\mathbf{u}.\\) We fortunately dont have to use limits to calculate directional derivatives, thanks to this theorem. Theorem A.2 If \\(f\\) is a differentiable function of \\(x\\) and \\(y\\), then \\(f\\) has a directional derivative in the direction of any unit vector \\(\\mathbf{u}=(u_1,u_2)\\) and \\[D_{\\mathbf{u}}f(x,y)=f_x(x,y)u_1+f_y(x,y)u_2.\\] Example A.3 Let \\(f(x,y)=x^3-3xy^2\\). Then \\(f_x(x,y)=3x^2-3y^2\\) and \\(f_y(x,y)=-6xy\\). Lets find the directional derivative in the direction of \\(\\mathbf{u}=(\\frac{4}{5},-\\frac{3}{5})\\). \\[\\begin{align*} D_{\\mathbf{u}}f(2,1)&amp;=((3)(2^2)-(3)(1^2))\\frac{4}{5}+-6(2)(1)(-\\frac{3}{5})\\\\ &amp;=9\\cdot\\frac{4}{5}+12\\cdot\\frac{3}{5}=\\frac{36}{5}+\\frac{36}{5}\\\\ &amp;=\\frac{72}{5}. \\end{align*}\\] If \\(\\mathbf{v}=(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}})\\), then \\[D_{\\mathbf{v}}f(2,1)=9\\cdot\\frac{1}{\\sqrt{2}}-12\\frac{1}{\\sqrt{2}}=-\\frac{3}{\\sqrt{2}}.\\] The gradient vector Note that for a unit vector \\(\\mathbf{u}=(u_1,u_2)\\), the directional derivative can be written as a dot product: \\[D_{\\mathbf{u}}f(x,y)=f_x(x,y)u_1+f_y(x,y)u_2=\\underbrace{(f_x(x,y),f_y(x,y))}_{\\nabla f(x,y)}\\cdot (u_1,u_2).\\] We give the vector of the partial derivatives a name: the gradient of \\(f\\), denoted \\(\\nabla f\\). Using this notation, we can write the directional derivative as \\[D_{\\mathbf{u}}f(x,y)=\\nabla f(x,y)\\cdot \\mathbf{u}.\\] If \\(f=f(x,y,z)\\), then \\(\\nabla f=(f_x,f_y,f_z)\\), and so on. Example A.4 Let \\(f(x,y)=x^2+y^2\\). Then \\(\\nabla f=(2x,2y),\\) and if \\(\\mathbf{u}=(\\frac{1}{2},\\frac{\\sqrt{3}}{2})\\), then \\[D_{\\mathbf{u}}f(-1,3)=(2(-1),2(3))\\cdot(\\frac{1}{2},\\frac{\\sqrt{3}}{2})=(-2)\\cdot\\frac{1}{2}+6\\cdot \\frac{\\sqrt{3}}{2}=-1+3\\sqrt{3}.\\] Note that \\[D_{\\mathbf{u}}f(x,y)=\\nabla f(x,y)\\cdot \\mathbf{u}=|\\nabla f(x,y)||\\mathbf{u}|\\cos \\theta,\\] where \\(\\theta\\) is the angle between \\(\\nabla f(x,y)\\) and \\(\\mathbf{u}.\\) Since \\(\\mathbf{u}\\) is a unit vector \\(|\\mathbf{u}|=1\\), so \\(D_{\\mathbf{u}}f(x,y)=|\\nabla f(x,y)|\\cos \\theta.\\) How big can this directional derivative be? Its greatest when \\(\\cos \\theta=1\\), or when \\(\\theta=0\\). That is, the directional derivative is greatest in the direction of the gradient vector! (Its at its minimum when \\(\\mathbf{u}\\) is pointing in the opposite direction. We use this fact in Section 5.10 when we implement the gradient descent algorithm.) In other words, the gradient of \\(f\\) points in the direction of maximum increase of \\(f\\), and that maximum value is \\(|\\nabla f|\\). The gradient can be evaluated at several points, and the (scaled) vectors can be displayed on a graph. In the example below, the gradient field is superimposed on the contour plot. Note how the vectors are perpendicular to the level curves and are pointing uphill (from darker to lighter). Figure A.11: The gradient vector points uphill A.4 Optimization An important task in multivariable calculus is to optimize functions of several variables. In the one variable case, we look for critical points: values where the derivative equals zero (or doesnt exist). In the two-variable case, if a function \\(f\\) has a minimum or maximum at a point, it must be a zero for both partial derivatives, \\(f_x\\) and \\(f_y\\), or a place where at least one partial isnt defined. (Well not worry about these.) This means that \\(f\\) will have a minimum or a maximum at a point where \\(\\nabla f\\) is the zero vector. These points are called critical points. Example A.5 Let \\(f(x,y)=x^2+y^2-6x+4y+13\\). Find the critical points of \\(f\\). We have \\(f_x(x,y)=2x-6\\) and \\(f_y=2y+4.\\) We set both of these equal to zero and solve for \\(x\\) and \\(y\\). \\[\\begin{align*} 2x-6&amp;=0\\\\ 2y+4&amp;=0. \\end{align*}\\] This system of equations is simple: \\(x=3\\) and \\(y=-2\\). Our critical point is \\((3,-2).\\) Example A.6 Find the critical points of \\(f(x,y)=x^2+xy+y^2+y\\). Our partial derivatives are \\(f_x=2x+y\\) and \\(f_y=x+2y+1.\\) Our system of equations is \\[\\begin{align*} 2x+y&amp;=0\\\\ x+2y&amp;=-1. \\end{align*}\\] The solution in this case is \\((x,y)=(\\frac13,-\\frac23).\\) Example A.7 Let \\(f(x,y)=4+x^3+y^3-3xy\\). Then \\(f_x=3x^2-3y\\) and \\(f_y=3y^2-3x.\\) This ones a bit more complicated because of the squared terms involved, but if we set \\(f_x=3x^2-3y=0\\), we get \\(3y=3x^2\\) or \\(y=x^2\\). For the second equation, we have \\(f_y=3y^2-3x=0,\\) and using \\(y=x^2\\), we get \\[\\begin{align*} 3(x^2)^2-3x&amp;=0\\\\ x^4-x&amp;=0\\\\ x(x^3-1)&amp;=0. \\end{align*}\\] Since \\(x^3-1=0\\) means \\(x^3=1\\) or \\(x=1\\), the two possible \\(x\\) values are \\(x=0\\) and \\(x=1\\). If \\(x=0,y=0^2=0\\), and if \\(x=1,y=1^2=1\\). Our two critical points are \\((0,0)\\) and \\((1,1)\\). How do we classify them? Classifying critical points Figure A.12 shows the contour plots for our three examples, with the critical points added. Figure A.12: Classifying critical points In the first two graphs, the critical points correspond to minima. In the last graph, one critical point corresponds to a minimum, and the other to a saddle point. These are two of the main possibilities for critical points. Much like the one-variable version, the two-variable version of the second derivative test can be used to classify critical points. Theorem A.3 (The Second Derivative Test) Suppose the second partial derivatives of \\(f\\) are continuous on a disk with center \\((a,b)\\) and suppose that \\(f_x(a,b)=0\\) and \\(f_y(a,b)=0\\). Let \\[D=f_{xx}(a,b)f_{yy}(a,b)-[f_{xy}(a,b)]^2.\\] If \\(D&gt;0\\) and \\(f_{xx}(a,b)&gt;0\\), then \\(f\\) has a local minimum at \\((a,b)\\). If \\(D&gt;0\\) and \\(f_{xx}(a,b)&lt;0\\), then \\(f\\) has a local maximum at \\((a,b)\\). If \\(D&lt;0\\), then \\(f\\) has a saddle point at \\((a,b)\\). If \\(D=0\\), there is no conclusion. Note that \\[D=f_{xx}(a,b)f_{yy}(a,b)-[f_{xy}(a,b)]^2\\] is the following determinant \\[\\begin{vmatrix}f_{xx}(a,b) &amp; f_{xy}(a,b)\\\\f_{yx}(a,b) &amp; f_{yy}(a,b)\\end{vmatrix}.\\] Since the two mixed partials are continuous in order for the theorem to work, they are equal, and their product can be written as \\([f_{xy}(a,b)]^2\\). The matrix involved is called the Hessian of \\(f\\). Lets return to our examples and use the second derivative test. For \\(f(x,y)=x^2+y^2-6x+4y+13\\) (Example A.5), we had \\(f_x=2x-6, f_y=2y+4\\), and a critical point of \\((3,-2).\\) The second-order partials are \\(f_{xx}=2, f_{yy}=2,\\) and \\(f_{xy}=f_{yx}=0.\\) This makes \\(D=2\\cdot 2-0^2=4&gt;0\\). With \\(f_{xx}(3,-2)=2&gt;0\\), we have a local minimum at \\((3,-2)\\) of \\(f(3,-2)=3^2+(-2)^2-6(3)+4(-2)+13=0\\). For \\(f(x,y)=x^2+xy+y^2+y\\) (Example A.6), we had \\(f_x(x,y)=2x+y, f_y(x,y)=x+2y+1\\), and a critical point of \\((1/3,-2/3)\\). The second-order partials are \\(f_{xx}(x,y)=2, f_{yy}(x,y)=2,\\) and \\(f_{xy}(x,y)=1.\\) This makes \\(D=2\\cdot 2-1^2=3&gt;0\\). With \\(f_{xx}(1/3,-2/3)=2&gt;0\\), we again have a local minimum. For \\(f(x,y)=4+x^3+y^3-3xy\\) (Example A.7), we had \\(f_x(x,y)=3x^2-3y,f_y(x,y)=3y^2-3x\\), and critical points of \\((0,0)\\) and \\((1,1)\\). The second-order partials are \\(f_{xx}(x,y)=6x, f_{yy}(x,y)=6y\\), and \\(f_{xy}(x,y)=-3.\\) This gives us \\[D=D(x,y)=(6x)(6y)-(-3)^2=36xy-9.\\] With \\(D(0,0)=-9&lt;0\\), we have a saddle point at \\((0,0)\\). With \\(D(1,1)=36-9=27&gt;0\\) and \\(f_{xx}(1,1)=6(1)&gt;0\\), we have a local minimum at \\((1,1)\\). A.5 Exercises Let \\(f(x,y)=3x^2+4xy+5y.\\) Evaluate the following. \\(f(-1,3)\\) \\(f(2,4)\\) Let \\(g(x,y)=x^4+2x^2y-3xy^2+y^3.\\) Evaluate the following. \\(g(2,3)\\) \\(g(1,-1)\\) Let \\(f(x,y)=3x^2+4xy+5y.\\) Find the following. \\(f_x(x,y)\\) \\(f_y(x,y)\\) \\(f_{xx}(x,y)\\) \\(f_{yy}(x,y)\\) \\(f_{xy}(x,y)\\) Let \\(g(x,y)=x^4+2x^2y-3xy^2+y^3.\\) Find the following. \\(g_x(x,y)\\) \\(g_y(x,y)\\) \\(g_{xx}(x,y)\\) \\(g_{yy}(x,y)\\) \\(g_{xy}(x,y)\\) Let \\(f(x,y)=e^x\\ln y.\\) Find the following. \\(f_x(x,y)\\) \\(f_y(x,y)\\) Let \\(g(x,y)= e^{2x}+\\ln y^3.\\) Find the Following. \\(g_x(x,y)\\) \\(g_y(x,y)\\) Let \\(f(x,y)=(2x+3y)^4.\\) Find the following. \\(f_x(x,y)\\) \\(f_y(x,y)\\) Let \\(g(x,y)=(3x^2+3y^2)^5.\\) Find the following. \\(g_x(x,y)\\) \\(g_y(x,y)\\) Figure A.13 shows the contour plot for a function that has a local minimum or maximum at the point \\((3,4)\\) Is it a minimum or a maximum? Explain. Figure A.13: Contour plot for Problem 9 Figure A.14 shows the contour plot for a function that has a local minimum or maximum at the point \\((2,-2)\\) Is it a minimum or a maximum? Explain. Figure A.14: Contour plot for Problem 10 Let \\(f(x,y)=3x^3-2x^2y+4xy^2+y^3-2.\\) Find \\(\\nabla f(x,y).\\) Find \\(D_{\\mathbf{u}}f(3,1),\\) where \\(\\mathbf{u}=(3/5,4/5).\\) Let \\(g(x,y)=x^4+2x^2y-3xy^2+y^2.\\) Find \\(\\nabla g(x,y).\\) Find \\(D_{\\mathbf{u}}g(1,2),\\) where \\(\\mathbf{u}=(1/\\sqrt{2},-1/\\sqrt{2}).\\) Let \\(f(x,y)=x^2+2xy+2y^2+x\\). Find and classify any critical points for \\(f\\). Let \\(g(x,y)=2x^2+y^2-6x+2y+1.\\) Find and classify any critical points for \\(g.\\) Let \\(f(x,y)=x^4+y^2+2xy.\\) Find and classify any critical points for \\(f.\\) Let \\(g(x,y)=x^3-y^3-3x^2-3y^2-9x.\\) Find and classify any critical points for \\(g.\\) "],["the-igraph-and-ggally-packages.html", "B The iGraph and GGally Packages", " B The iGraph and GGally Packages Here is some additional information on the use of the iGraph and GGally packages for creating and performing computations on graphs. We can create a graph in several ways. Perhaps the simplest is by giving it a vector of vertex numbers. By default, igraph will pair up the numbers to represent an edge. Another default assumption is that the first number in each pair is the starting vertex of a directed edge. library(igraph) edges &lt;- c(1,2, 2,3, 4,3, 2,4) g1 &lt;- graph(edges) g1 ## IGRAPH d018c35 D--- 4 4 -- ## + edges from d018c35: ## [1] 1-&gt;2 2-&gt;3 4-&gt;3 2-&gt;4 The plot function will usually create a decent plot for an uncomplicated graph, though the default vertex size is somewhat small (Fig. B.1). There can be some randomness in the placement of the vertices, so you might wish to set the seed before plotting the graph. set.seed(1729) plot(g1, vertex.size = 30) Figure B.1: A graph To create an undirected graph, just have directed = FALSE as a graph setting. g1u &lt;- graph(edges, directed = FALSE) g1u ## IGRAPH d0304c2 U--- 4 4 -- ## + edges from d0304c2: ## [1] 1--2 2--3 3--4 2--4 You can add an edge or edges using the add.edges function (Fig. B.2). set.seed(1729) g2u &lt;- add.edges(g1u,c(1,1, 4,2)) plot(g2u, vertex.size = 30) Figure B.2: The original graph with more edges, including a loop You can find an adjacency matrix from a graph or create a graph from an adjacency matrix. By default, the output or input is a sparse matrix, with only the nonzero terms stored. as_adjacency_matrix(g1) ## 4 x 4 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 . . ## [2,] . . 1 1 ## [3,] . . . . ## [4,] . . 1 . as_adjacency_matrix(g1, sparse = FALSE) ## [,1] [,2] [,3] [,4] ## [1,] 0 1 0 0 ## [2,] 0 0 1 1 ## [3,] 0 0 0 0 ## [4,] 0 0 1 0 as_adjacency_matrix(g1u, sparse = FALSE) ## [,1] [,2] [,3] [,4] ## [1,] 0 1 0 0 ## [2,] 1 0 1 1 ## [3,] 0 1 0 1 ## [4,] 0 1 1 0 set.seed(1234) # We create a random 5x5 matrix with zeros and ones. M &lt;- matrix(sample(0:1, 25, replace = TRUE), nrow = 5) M ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 1 0 1 1 1 ## [3,] 1 0 1 0 1 ## [4,] 1 0 0 1 1 ## [5,] 0 1 1 1 1 gM &lt;- graph_from_adjacency_matrix(M) gM ## IGRAPH d04db13 D--- 5 19 -- ## + edges from d04db13: ## [1] 1-&gt;1 1-&gt;2 1-&gt;3 1-&gt;4 1-&gt;5 2-&gt;1 2-&gt;3 2-&gt;4 2-&gt;5 3-&gt;1 3-&gt;3 3-&gt;5 4-&gt;1 4-&gt;4 4-&gt;5 ## [16] 5-&gt;2 5-&gt;3 5-&gt;4 5-&gt;5 plot(gM, vertex.size = 30, vertex.color = &quot;lightblue&quot;) Figure B.3: A graph from a random matrix You can create a graph using named vertices instead of numbers (Fig. B.4) edgesn &lt;- c(&quot;Al&quot;, &quot;Bob&quot;, &quot;Al&quot;, &quot;Cam&quot;, &quot;Bob&quot;, &quot;Cam&quot;) gn &lt;- graph(edgesn) gn ## IGRAPH d05deed DN-- 3 3 -- ## + attr: name (v/c) ## + edges from d05deed (vertex names): ## [1] Al -&gt;Bob Al -&gt;Cam Bob-&gt;Cam plot(gn, vertex.size = 0) Figure B.4: A graph with named labels If youre entering the graph by hand, its probably easier to do the naming later. You can either feed the plot function a vector of names, or you can change them using the V function. Here, well change the names in the first graph. V(g1)$name &lt;- c(&quot;Bears&quot;, &quot;Lions&quot;, &quot;Packers&quot;, &quot;Vikings&quot;) g1 ## IGRAPH d018c35 DN-- 4 4 -- ## + attr: name (v/c) ## + edges from d018c35 (vertex names): ## [1] Bears -&gt;Lions Lions -&gt;Packers Vikings-&gt;Packers Lions -&gt;Vikings There are some ways to set the layout. For example, the following code will arrange the vertices of our random graph in a circle (Fig. B.5). Since there are five vertices, it looks like a pentagon. coords &lt;- layout_in_circle(gM) plot(gM, vertex.size = 30, vertex.color = &quot;lightblue&quot;, layout = coords) Figure B.5: A circular layout GGally The GGally package is an extension to ggplot2 that includes many features. One of them is a ggnet2 function that will create nice graphs, as in Figure B.6. This graph uses the randomly generated adjacency matrix from above. library(GGally) set.seed(22) ggnet2(M, arrow.size = 12, color = &quot;orangered&quot;, arrow.gap = 0.025, label = LETTERS[1:5]) Figure B.6: A graph using ggnet2 We can use ggnet2 with an igraph graph, though it requires the intergraph package. Figure B.7 shows the result. library(intergraph) set.seed(222) ggnet2(gn, arrow.size = 12, color = &quot;lightpink&quot;, arrow.gap = 0.045, size=15, label = V(gn)$name) Figure B.7: Another ggnet2 graph "],["packages.html", "C Packages", " C Packages Here is a list of the packages used in this book. Note: this list is generated automatically and may not include all dependencies. Also, some packages were used strictly for formatting purposes, and may not be needed for any of the displayed sample code. Package Loaded Version arules 1.7-7 bitops 1.0-7 caret 6.0-94 dplyr 1.1.4 e1071 1.7-14 expm 0.999-9 factoextra 1.0.7 GGally 2.2.1 ggfortify 0.4.17 ggplot2 3.5.1 ggrepel 0.9.5 igraph 2.0.3 intergraph 2.0-4 kableExtra 1.4.0 lattice 0.22-6 leaps 3.1 magick 2.8.3 Matrix 1.7-0 pracma 2.4.4 proxy 0.4-27 randomForest 4.7-1.1 rattle 5.5.1 readr 2.1.5 recommenderlab 1.0.6 rpart 4.1.23 tibble 3.2.1 titanic 0.1.0 This book was assembled with Bookdown version 0.39 using R version 4.4.0 (2024-04-24 ucrt). "],["references.html", "References", " References 1. Navidi W (2015) Statistics for engineers and scientists, 4th ed., McGraw-Hill Education. 2. Lial ML, Hungerford TW, Holcomb JP, et al. (2018) Mathematics with applications in the management, natural, and social sciences, 12th ed., Pearson. 3. Edwards CH, Penney DE, Calvis DT (2017) Differential equations and linear algebra, 4th ed., Pearson. 4. Neuhauser C (2004) Calculus for biology and medicine, 2nd ed., Pearson/Prentice Hall. 5. Lay DC, Lay SR, McDonald JJ (2015) Linear algebra and its applications, Pearson Education. 6. Desmos matrix calculator. 7. Callaghan T, Mucha PJ, Porter MA (2004) The bowl championship series: A mathematical review. Notices Amer Math Soc 51: 887893. 8. Colley WN (2002) The Colley matrix method. 9. Olofsson P, Andersson M (2012) Probability, statistics, and stochastic processes, 2nd ed., John Wiley &amp; Sons, Inc. 10. Leslie PH (1945) The use of matrices in certain population mathematics. Biometrika 33: 183212. 11. Langville AN, Meyer CD (2012) Whos #1? The science of rating and ranking, Princeton University Press. 12. Meyer CD (2023) Matrix analysis and applied linear algebra, 2nd ed., Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA. 13. Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems 33: 107117. 14. Balreira EC, Miceli BK, Tegtmeyer T (2014) An oracle method to predict NFL games. Journal of Quantitative Analysis in Sports 10: 183196. 15. Massey K (1997) Statistical models applied to the rating of sports teams. 16. Smith B (2015) Calculating original gravity for beer recipe design. 17. Bobbitt Z (2021) How to interpret diagnostic plots in R. 18. Akaike H (1974) A new look at the statistical model identification. IEEE Trans Automatic Control AC-19: 716723. 19. Schwarz G (1978) Estimating the dimension of a model. Ann Statist 6: 461464. 20. Strang G (2022) Introduction to linear algebra, 6th ed., Wellesley Cambridge Press. 21. Eckart C, Young G (1936) The approximation of one matrix by another of lower rank. Psychometrika 1: 211218. 22. Hahsler M (2022) Recommenderlab: An r framework for developing and testing recommendation algorithms. 23. The internet movie database. 24. Tibshirani R, Walther G, Hastie T (2002) Estimating the Number of Clusters in a Data Set Via the Gap Statistic. Journal of the Royal Statistical Society Series B: Statistical Methodology 63: 411423. 25. Bell RM, Koren Y (2007) Lessons from the netflix prize challenge. Acm Sigkdd Explorations Newsletter 9: 7579. 26. Thakar C, Tahsildar S (2022) Gini index: Decision tree, formula, calculator, gini coefficient in machine learning. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
